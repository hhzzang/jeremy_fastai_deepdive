{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"12c_ulmfit.ipynb","provenance":[],"collapsed_sections":["Wo7wXjYhuAwU","Zq3RyamouAwZ"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fyEOdAiZuAwD"},"source":["# ULMFit"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vz7EuwM_uAwG","executionInfo":{"status":"ok","timestamp":1630339136988,"user_tz":-540,"elapsed":26277,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c4efe6d4-8cb7-4f03-aa19-b8796e80bea1"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","%cd gdrive/MyDrive/Ml/nbs/dl2/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Ml/nbs/dl2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zB6YqnJmyvp4","executionInfo":{"status":"ok","timestamp":1630339286907,"user_tz":-540,"elapsed":415,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c1c495a6-c8e2-492b-bd6a-45378cad4881"},"source":["%%writefile setup.sh\n","\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Overwriting setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHHhwXCTyzxu","executionInfo":{"status":"ok","timestamp":1630339530895,"user_tz":-540,"elapsed":241328,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"72e7760e-c4c5-49a9-975f-36d0c0be73c6"},"source":["!sh setup.sh"],"execution_count":6,"outputs":[{"output_type":"stream","text":["fatal: destination path 'apex' already exists and is not an empty directory.\n","/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n","  cmdoptions.check_install_build_global(options)\n","Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n","Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/include/python3.7/UNKNOWN\n","sysconfig: /usr/include/python3.7m/UNKNOWN\n","Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/bin\n","sysconfig: /usr/bin\n","Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local\n","sysconfig: /usr\n","Additional context:\n","user = False\n","home = None\n","root = None\n","prefix = None\n","Non-user install because site-packages writeable\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-094mexqc\n","Created temporary directory: /tmp/pip-req-tracker-_v5lybmw\n","Initialized build tracking at /tmp/pip-req-tracker-_v5lybmw\n","Created build tracker: /tmp/pip-req-tracker-_v5lybmw\n","Entered build tracker: /tmp/pip-req-tracker-_v5lybmw\n","Created temporary directory: /tmp/pip-install-3n4420hu\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-xc5h_f5_\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","  Added file:///content/gdrive/MyDrive/Ml/nbs/dl2/apex to build tracker '/tmp/pip-req-tracker-_v5lybmw'\n","    Running setup.py (path:/tmp/pip-req-build-xc5h_f5_/setup.py) egg_info for package from file:///content/gdrive/MyDrive/Ml/nbs/dl2/apex\n","    Created temporary directory: /tmp/pip-pip-egg-info-875fuhgu\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.9.0+cu102\n","\n","\n","    running egg_info\n","    creating /tmp/pip-pip-egg-info-875fuhgu/apex.egg-info\n","    writing /tmp/pip-pip-egg-info-875fuhgu/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-pip-egg-info-875fuhgu/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-pip-egg-info-875fuhgu/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-pip-egg-info-875fuhgu/apex.egg-info/SOURCES.txt'\n","    adding license file 'LICENSE'\n","    writing manifest file '/tmp/pip-pip-egg-info-875fuhgu/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-xc5h_f5_/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-xc5h_f5_ has version 0.1, which satisfies requirement apex==0.1 from file:///content/gdrive/MyDrive/Ml/nbs/dl2/apex\n","  Removed apex==0.1 from file:///content/gdrive/MyDrive/Ml/nbs/dl2/apex from build tracker '/tmp/pip-req-tracker-_v5lybmw'\n","Created temporary directory: /tmp/pip-unpack-0xlsnes4\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/lib/python3.7/dist-packages\n","  sysconfig: /usr/lib/python3.7/site-packages\n","  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/lib/python3.7/dist-packages\n","  sysconfig: /usr/lib/python3.7/site-packages\n","  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/include/python3.7/apex\n","  sysconfig: /usr/include/python3.7m/apex\n","  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/bin\n","  sysconfig: /usr/bin\n","  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local\n","  sysconfig: /usr\n","  Additional context:\n","  user = False\n","  home = None\n","  root = None\n","  prefix = None\n","  Created temporary directory: /tmp/pip-record-gm9387ic\n","    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-gm9387ic/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex\n","\n","\n","    torch.__version__  = 1.9.0+cu102\n","\n","\n","    /tmp/pip-req-build-xc5h_f5_/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2020 NVIDIA Corporation\n","    Built on Wed_Jul_22_19:09:09_PDT_2020\n","    Cuda compilation tools, release 11.0, V11.0.221\n","    Build cuda_11.0_bu.TC445_37.28845127_0\n","    from /usr/local/cuda/bin\n","\n","    Traceback (most recent call last):\n","      File \"<string>\", line 1, in <module>\n","      File \"/tmp/pip-req-build-xc5h_f5_/setup.py\", line 171, in <module>\n","        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n","      File \"/tmp/pip-req-build-xc5h_f5_/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n","        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n","    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.2.\n","    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n","\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-gm9387ic/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex Check the logs for full command output.\u001b[0m\n","Exception information:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 825, in install\n","    req_description=str(self.req),\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/install/legacy.py\", line 81, in install\n","    raise LegacyInstallFailure\n","pip._internal.operations.install.legacy.LegacyInstallFailure\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n","    status = self.run(options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 402, in run\n","    pycompile=options.compile,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 85, in install_given_reqs\n","    pycompile=pycompile,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 829, in install\n","    six.reraise(*exc.parent)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/six.py\", line 703, in reraise\n","    raise value\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/install/legacy.py\", line 71, in install\n","    cwd=unpacked_source_directory,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 278, in runner\n","    spinner=spinner,\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 244, in call_subprocess\n","    raise InstallationSubprocessError(proc.returncode, command_desc)\n","pip._internal.exceptions.InstallationSubprocessError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-xc5h_f5_/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-gm9387ic/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex Check the logs for full command output.\n","Removed build tracker: '/tmp/pip-req-tracker-_v5lybmw'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"id":"k9y8wKJsuAwI","executionInfo":{"status":"error","timestamp":1630339530895,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"cd9365a3-4df2-401e-9a75-3b4372f8f5bb"},"source":["#export\n","from exp.nb_12a import *"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-df36ff150c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_12a\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/gdrive/MyDrive/Ml/nbs/dl2/exp/nb_12a.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# file to edit: dev_nb/12a_awd_lstm.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/Ml/nbs/dl2/exp/nb_12.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# file to edit: dev_nb/12_text.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_11a\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/Ml/nbs/dl2/exp/nb_11a.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# file to edit: dev_nb/11a_transfer_learning.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mp_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/Ml/nbs/dl2/exp/nb_11.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# file to edit: dev_nb/11_train_imagenette.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_10c\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/Ml/nbs/dl2/exp/nb_10c.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_10b\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mapex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_master\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_master\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apex.fp16_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"5I4wlUh1uAwI"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"H264dzufuAwJ"},"source":["We load the data from 12a, instructions to create that file are there if you don't have it yet so go ahead and see."]},{"cell_type":"markdown","metadata":{"id":"kx5rhN2WuAwJ"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7459)"]},{"cell_type":"code","metadata":{"id":"uJGmUDituAwJ"},"source":["path = datasets.untar_data(datasets.URLs.IMDB)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ti1TyDbEuAwK"},"source":["ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNuIycskuAwL"},"source":["bs,bptt = 128,70\n","data = lm_databunchify(ll, bs, bptt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsNbcRQLuAwM"},"source":["vocab = ll.train.proc_x[1].vocab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWK0M8KBuAwM"},"source":["## Finetuning the LM"]},{"cell_type":"markdown","metadata":{"id":"zhUrm-I1uAwM"},"source":["Before tackling the classification task, we have to finetune our language model to the IMDB corpus."]},{"cell_type":"markdown","metadata":{"id":"Nx_q7g3puAwN"},"source":["We have pretrained a small model on [wikitext 103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) that you can download by uncommenting the following cell."]},{"cell_type":"code","metadata":{"id":"wKcxd62tuAwN"},"source":["# ! wget http://files.fast.ai/models/wt103_tiny.tgz -P {path}\n","# ! tar xf {path}/wt103_tiny.tgz -C {path}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wtw-3cXouAwN"},"source":["dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\n","tok_pad = vocab.index(PAD)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MvEU_QjuAwN"},"source":["emb_sz, nh, nl = 300, 300, 2\n","model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CrZhIHCRuAwO"},"source":["old_wgts  = torch.load(path/'pretrained'/'pretrained.pth')\n","old_vocab = pickle.load(open(path/'pretrained'/'vocab.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAXZ3IPRuAwO"},"source":["In our current vocabulary, it is very unlikely that the ids correspond to what is in the vocabulary used to train the pretrain model. The tokens are sorted by frequency (apart from the special tokens that are all first) so that order is specific to the corpus used. For instance, the word 'house' has different ids in the our current vocab and the pretrained one."]},{"cell_type":"code","metadata":{"id":"0-i6otUIuAwO"},"source":["idx_house_new, idx_house_old = vocab.index('house'),old_vocab.index('house')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HrH3wkPHuAwO"},"source":["We somehow need to match our pretrained weights to the new vocabulary. This is done on the embeddings and the decoder (since the weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order.\n","\n","It may also happen that we have words that aren't in the pretrained vocab, in this case, we put the mean of the pretrained embedding weights/decoder bias."]},{"cell_type":"code","metadata":{"id":"wFbzT8BfuAwO"},"source":["house_wgt  = old_wgts['0.emb.weight'][idx_house_old]\n","house_bias = old_wgts['1.decoder.bias'][idx_house_old] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qe3nLJp4uAwP"},"source":["def match_embeds(old_wgts, old_vocab, new_vocab):\n","    wgts = old_wgts['0.emb.weight']\n","    bias = old_wgts['1.decoder.bias']\n","    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()\n","    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))\n","    new_bias = bias.new_zeros(len(new_vocab))\n","    otoi = {v:k for k,v in enumerate(old_vocab)}\n","    for i,w in enumerate(new_vocab): \n","        if w in otoi:\n","            idx = otoi[w]\n","            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]\n","        else: new_wgts[i],new_bias[i] = wgts_m,bias_m\n","    old_wgts['0.emb.weight']    = new_wgts\n","    old_wgts['0.emb_dp.emb.weight'] = new_wgts\n","    old_wgts['1.decoder.weight']    = new_wgts\n","    old_wgts['1.decoder.bias']      = new_bias\n","    return old_wgts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzHii_lOuAwP"},"source":["wgts = match_embeds(old_wgts, old_vocab, vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAWLJov0uAwP"},"source":["Now let's check that the word \"*house*\" was properly converted."]},{"cell_type":"code","metadata":{"id":"W9RRhZPYuAwP"},"source":["test_near(wgts['0.emb.weight'][idx_house_new],house_wgt)\n","test_near(wgts['1.decoder.bias'][idx_house_new],house_bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXNjQdXJuAwP"},"source":["We can load the pretrained weights in our model before beginning training."]},{"cell_type":"code","metadata":{"id":"28itrzXEuAwQ"},"source":["model.load_state_dict(wgts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Qf1g1TquAwQ"},"source":["If we want to apply discriminative learning rates, we need to split our model in different layer groups. Let's have a look at our model."]},{"cell_type":"code","metadata":{"id":"aVU3ji7buAwQ","outputId":"267c4a59-263f-443e-9d10-b5df3151dce7"},"source":["model"],"execution_count":null,"outputs":[{"data":{"text/plain":["SequentialRNN(\n","  (0): AWD_LSTM(\n","    (emb): Embedding(60003, 300, padding_idx=2)\n","    (emb_dp): EmbeddingDropout(\n","      (emb): Embedding(60003, 300, padding_idx=2)\n","    )\n","    (rnns): ModuleList(\n","      (0): WeightDropout(\n","        (module): LSTM(300, 300, batch_first=True)\n","      )\n","      (1): WeightDropout(\n","        (module): LSTM(300, 300, batch_first=True)\n","      )\n","    )\n","    (input_dp): RNNDropout()\n","    (hidden_dps): ModuleList(\n","      (0): RNNDropout()\n","      (1): RNNDropout()\n","    )\n","  )\n","  (1): LinearDecoder(\n","    (output_dp): RNNDropout()\n","    (decoder): Linear(in_features=300, out_features=60003, bias=True)\n","  )\n",")"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"iLNdJJ2SuAwQ"},"source":["Then we split by doing two groups for each rnn/corresponding dropout, then one last group that contains the embeddings/decoder. This is the one that needs to be trained the most as we may have new embeddings vectors."]},{"cell_type":"code","metadata":{"id":"8EZVgxjouAwR"},"source":["def lm_splitter(m):\n","    groups = []\n","    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n","    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]\n","    return [list(o.parameters()) for o in groups]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou4qeA02uAwR"},"source":["First we train with the RNNs freezed."]},{"cell_type":"code","metadata":{"id":"RW0vx_9YuAwR"},"source":["for rnn in model[0].rnns:\n","    for p in rnn.parameters(): p.requires_grad_(False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0H-q_5huAwR"},"source":["cbs = [partial(AvgStatsCallback,accuracy_flat),\n","       CudaCallback, Recorder,\n","       partial(GradientClipping, clip=0.1),\n","       partial(RNNTrainer, α=2., β=1.),\n","       ProgressCallback]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8tWA4UsuAwR"},"source":["learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(),\n","                cb_funcs=cbs, splitter=lm_splitter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYgT6xDwuAwR"},"source":["lr = 2e-2\n","cbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRsu-_n5uAwS"},"source":["learn.fit(1, cbs=cbsched)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5YsaiAuuAwS"},"source":["Then the whole model with discriminative learning rates."]},{"cell_type":"code","metadata":{"id":"jqEBcBAbuAwS"},"source":["for rnn in model[0].rnns:\n","    for p in rnn.parameters(): p.requires_grad_(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlRA03iLuAwS"},"source":["lr = 2e-3\n","cbsched = sched_1cycle([lr/2., lr/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmfLUJYsuAwS"},"source":["learn.fit(10, cbs=cbsched)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGnJaHQquAwS"},"source":["We only need to save the encoder (first part of the model) for the classification, as well as the vocabulary used (we will need to use the same in the classification task)."]},{"cell_type":"code","metadata":{"id":"bhKZny30uAwS"},"source":["torch.save(learn.model[0].state_dict(), path/'finetuned_enc.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3fmSJgDuAwS"},"source":["pickle.dump(vocab, open(path/'vocab_lm.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAqgeAk1uAwT"},"source":["torch.save(learn.model.state_dict(), path/'finetuned.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBFFN7SfuAwT"},"source":["## Classifier"]},{"cell_type":"markdown","metadata":{"id":"QFoP41cHuAwT"},"source":["We have to process the data again otherwise pickle will complain. We also have to use the same vocab as the language model."]},{"cell_type":"markdown","metadata":{"id":"hW7h3AuhuAwT"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7554)"]},{"cell_type":"code","metadata":{"id":"Dz4xu4MtuAwT"},"source":["vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))\n","proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqwEOVZ3uAwT"},"source":["il = TextList.from_files(path, include=['train', 'test'])\n","sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n","ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56TJEqOcuAwT"},"source":["pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgWxRYicuAwT"},"source":["ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))\n","vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAkN3rJXuAwT"},"source":["bs,bptt = 64,70\n","data = clas_databunchify(ll, bs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wo7wXjYhuAwU"},"source":["### Ignore padding"]},{"cell_type":"markdown","metadata":{"id":"er5dOB0NuAwU"},"source":["We will those two utility functions from PyTorch to ignore the padding in the inputs."]},{"cell_type":"code","metadata":{"id":"xBOycQTDuAwU"},"source":["#export\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huFoVhULuAwU"},"source":["Let's see how this works: first we grab a batch of the training set."]},{"cell_type":"code","metadata":{"id":"AGZ0045BuAwU"},"source":["x,y = next(iter(data.train_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKGzziJJuAwU"},"source":["x.size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hv-rQkfmuAwU"},"source":["We need to pass to the utility functions the lengths of our sentences because it's applied after the embedding, so we can't see the padding anymore."]},{"cell_type":"code","metadata":{"id":"Rs-4b0gYuAwU"},"source":["lengths = x.size(1) - (x == 1).sum(1)\n","lengths[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U28OG28MuAwU"},"source":["tst_emb = nn.Embedding(len(vocab), 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wqeeqr9uAwU"},"source":["tst_emb(x).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwCL9VIauAwU"},"source":["128*70"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2N7liRhuAwU"},"source":["We create a `PackedSequence` object that contains all of our unpadded sequences"]},{"cell_type":"code","metadata":{"id":"60J2Ke9yuAwV"},"source":["packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z02xdslHuAwV"},"source":["packed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdZ58oB8uAwV"},"source":["packed.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9zaR7cTuAwV"},"source":["len(packed.batch_sizes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BrP7QKzruAwV"},"source":["8960//70"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoS0rqIQuAwV"},"source":["This object can be passed to any RNN directly while retaining the speed of CuDNN."]},{"cell_type":"code","metadata":{"id":"-zcWy6pGuAwV"},"source":["tst = nn.LSTM(300, 300, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJUyFyLMuAwV"},"source":["y,h = tst(packed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okSYVsuCuAwV"},"source":["Then we can unpad it with the following function for other modules:"]},{"cell_type":"code","metadata":{"id":"gUATHJK1uAwV"},"source":["unpack = pad_packed_sequence(y, batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uK-lzwtLuAwV"},"source":["unpack[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXqkaGPQuAwV"},"source":["unpack[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TSwJiLTzuAwW"},"source":["We need to change our model a little bit to use this."]},{"cell_type":"code","metadata":{"id":"i6kg69dhuAwW"},"source":["#export\n","class AWD_LSTM1(nn.Module):\n","    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n","    initrange=0.1\n","\n","    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n","                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n","        super().__init__()\n","        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token\n","        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n","        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n","        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n","                             batch_first=True) for l in range(n_layers)]\n","        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n","        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n","        self.input_dp = RNNDropout(input_p)\n","        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n","\n","    def forward(self, input):\n","        bs,sl = input.size()\n","        mask = (input == self.pad_token)\n","        lengths = sl - mask.long().sum(1)\n","        n_empty = (lengths == 0).sum()\n","        if n_empty > 0:\n","            input = input[:-n_empty]\n","            lengths = lengths[:-n_empty]\n","            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]\n","        raw_output = self.input_dp(self.emb_dp(input))\n","        new_hidden,raw_outputs,outputs = [],[],[]\n","        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n","            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)\n","            raw_output, new_h = rnn(raw_output, self.hidden[l])\n","            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]\n","            raw_outputs.append(raw_output)\n","            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n","            outputs.append(raw_output)\n","            new_hidden.append(new_h)\n","        self.hidden = to_detach(new_hidden)\n","        return raw_outputs, outputs, mask\n","\n","    def _one_hidden(self, l):\n","        \"Return one hidden state.\"\n","        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n","        return next(self.parameters()).new(1, self.bs, nh).zero_()\n","\n","    def reset(self):\n","        \"Reset the hidden states.\"\n","        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q525Km1GuAwW"},"source":["### Concat pooling"]},{"cell_type":"markdown","metadata":{"id":"YVOOnQ7YuAwW"},"source":["We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last element/average/maximum."]},{"cell_type":"markdown","metadata":{"id":"0-ck2hemuAwX"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7604)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cFXmGPLvmeY","executionInfo":{"status":"ok","timestamp":1630338925690,"user_tz":-540,"elapsed":381,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"27ad66e9-6ab5-4ac9-e580-c14530f01410"},"source":["import torch\n","mask = torch.arange(300).reshape(2,3,50).view(2,3,50).bernoulli_(0.05).bool()\n","torch.arange(300).reshape(2,3,50)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False],\n","         [ True, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","           True, False, False,  True, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False],\n","         [False, False, False,  True, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False,  True, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False,  True, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False]],\n","\n","        [[False, False, False, False, False, False, False, False, False, False,\n","          False,  True, False, False, False, False, False, False, False,  True,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False,  True, False, False, False, False, False, False, False],\n","         [False, False, False,  True,  True,  True, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False,  True, False, False],\n","         [False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","          False, False, False, False, False, False, False, False, False, False,\n","           True, False, False, False, False, False, False, False,  True, False]]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"12TFsDfpuAwX","executionInfo":{"status":"error","timestamp":1630338117771,"user_tz":-540,"elapsed":295,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"7842cd1b-0e03-43f5-aeb6-1ad7d90febac"},"source":["class Pooling(nn.Module):\n","    def forward(self, input):\n","        raw_outputs,outputs,mask = input\n","        output = outputs[-1]\n","        lengths = output.size(1) - mask.long().sum(dim=1) # padding 뺀 최대길이\n","        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n","        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n","        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n","        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n","        return output,x"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-580c775c3b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"DT9o5V9DuAwX","executionInfo":{"status":"aborted","timestamp":1630338117766,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["emb_sz, nh, nl = 300, 300, 2\n","tok_pad = vocab.index(PAD)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COZ5gEGAuAwX","executionInfo":{"status":"aborted","timestamp":1630338117767,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad)\n","pool = Pooling()\n","enc.bs = bs\n","enc.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoYaDL79uAwX","executionInfo":{"status":"aborted","timestamp":1630338117768,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["x,y = next(iter(data.train_dl))\n","output,c = pool(enc(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMj4NxymuAwX"},"source":["We can check we have padding with 1s at the end of each text (except the first which is the longest)."]},{"cell_type":"code","metadata":{"id":"mXDl2EtQuAwX","executionInfo":{"status":"aborted","timestamp":1630338117768,"user_tz":-540,"elapsed":10,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRZRY2W3uAwX"},"source":["PyTorch puts 0s everywhere we had padding in the `output` when unpacking."]},{"cell_type":"code","metadata":{"id":"sa6d92n2uAwX","executionInfo":{"status":"aborted","timestamp":1630338117769,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["test_near((output.sum(dim=2) == 0).float(), (x==tok_pad).float())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNAJYQLNuAwX"},"source":["So the last hidden state isn't the last element of `output`. Let's check we got everything right. "]},{"cell_type":"code","metadata":{"id":"_l5FDVUYuAwX","executionInfo":{"status":"aborted","timestamp":1630338117769,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["for i in range(bs):\n","    length = x.size(1) - (x[i]==1).long().sum()\n","    out_unpad = output[i,:length]\n","    test_near(out_unpad[-1], c[i,:300])\n","    test_near(out_unpad.max(0)[0], c[i,300:600])\n","    test_near(out_unpad.mean(0), c[i,600:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VLBn3q24uAwY"},"source":["Our pooling layer properly ignored the padding, so now let's group it with a classifier."]},{"cell_type":"code","metadata":{"id":"3Dy-9DIcuAwY","executionInfo":{"status":"aborted","timestamp":1630338117769,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n","    layers = [nn.BatchNorm1d(n_in)] if bn else []\n","    if p != 0: layers.append(nn.Dropout(p))\n","    layers.append(nn.Linear(n_in, n_out))\n","    if actn is not None: layers.append(actn)\n","    return layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZPA5xWPuAwY","executionInfo":{"status":"aborted","timestamp":1630338117770,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["class PoolingLinearClassifier(nn.Module):\n","    \"Create a linear classifier with pooling.\"\n","\n","    def __init__(self, layers, drops):\n","        super().__init__()\n","        mod_layers = []\n","        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n","        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n","            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n","        self.layers = nn.Sequential(*mod_layers)\n","\n","    def forward(self, input):\n","        raw_outputs,outputs,mask = input\n","        output = outputs[-1]\n","        lengths = output.size(1) - mask.long().sum(dim=1)\n","        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) # bs,sequence,embedding\n","        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n","        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n","        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n","        x = self.layers(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LmCv8FWiuAwY"},"source":["Then we just have to feed our texts to those two blocks, (but we can't give them all at once to the AWD_LSTM or we might get OOM error: we'll go for chunks of bptt length to regularly detach the history of our hidden states.)"]},{"cell_type":"code","metadata":{"id":"nsXDXSzjuAwY","executionInfo":{"status":"aborted","timestamp":1630338117770,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def pad_tensor(t, bs, val=0.):\n","    if t.size(0) < bs:\n","        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])\n","    return t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsr1wkciuAwY","executionInfo":{"status":"aborted","timestamp":1630338117770,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["class SentenceEncoder(nn.Module):\n","    def __init__(self, module, bptt, pad_idx=1):\n","        super().__init__()\n","        self.bptt,self.module,self.pad_idx = bptt,module,pad_idx\n","\n","    def concat(self, arrs, bs):\n","        return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]\n","    \n","    def forward(self, input):\n","        bs,sl = input.size()\n","        self.module.bs = bs\n","        self.module.reset()\n","        raw_outputs,outputs,masks = [],[],[]\n","        for i in range(0, sl, self.bptt):\n","            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])\n","            masks.append(pad_tensor(m, bs, 1))\n","            raw_outputs.append(r)\n","            outputs.append(o)\n","        return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDLzXshtuAwY","executionInfo":{"status":"aborted","timestamp":1630338117771,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, \n","                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):\n","    \"To create a full AWD-LSTM\"\n","    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n","                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n","    enc = SentenceEncoder(rnn_enc, bptt)\n","    if layers is None: layers = [50]\n","    if drops is None:  drops = [0.1] * len(layers)\n","    layers = [3 * emb_sz] + layers + [n_out] \n","    drops = [output_p] + drops\n","    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2Mfkw5cuAwZ","executionInfo":{"status":"aborted","timestamp":1630338117771,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["emb_sz, nh, nl = 300, 300, 2\n","dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n","model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zq3RyamouAwZ"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"AK4Zzm_fuAwZ"},"source":["We load our pretrained encoder and freeze it."]},{"cell_type":"markdown","metadata":{"id":"69nQcaqvuAwZ"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7684)"]},{"cell_type":"code","metadata":{"id":"2yIafmThuAwZ"},"source":["def class_splitter(m):\n","    enc = m[0].module\n","    groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)]\n","    for i in range(len(enc.rnns)): groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i]))\n","    groups.append(m[1])\n","    return [list(o.parameters()) for o in groups]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWool-GEuAwZ"},"source":["for p in model[0].parameters(): p.requires_grad_(False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE0zeCr_uAwZ"},"source":["cbs = [partial(AvgStatsCallback,accuracy),\n","       CudaCallback, Recorder,\n","       partial(GradientClipping, clip=0.1),\n","       ProgressCallback]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD4JTfHcuAwZ"},"source":["model[0].module.load_state_dict(torch.load(path/'finetuned_enc.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLItsAn3uAwZ"},"source":["learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhvEQqtSuAwZ"},"source":["lr = 1e-2\n","cbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYLjz08BuAwa"},"source":["learn.fit(1, cbs=cbsched)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2W50tcqauAwa"},"source":["for p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cj_-5wEFuAwa"},"source":["lr = 5e-3\n","cbsched = sched_1cycle([lr/2., lr/2., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJzK6TenuAwa"},"source":["learn.fit(1, cbs=cbsched)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMcYms7YuAwa"},"source":["for p in model[0].parameters(): p.requires_grad_(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6-1EFOduAwa"},"source":["lr = 1e-3\n","cbsched = sched_1cycle([lr/8., lr/4., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Qm5FGIJuAwa"},"source":["learn.fit(2, cbs=cbsched)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GIy4qLXuAwa"},"source":["x,y = next(iter(data.valid_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C62sdOJhuAwa"},"source":["Predicting on the padded batch or on the individual unpadded samples give the same results."]},{"cell_type":"code","metadata":{"id":"w4LjBu2auAwa"},"source":["pred_batch = learn.model.eval()(x.cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBD1INt2uAwa"},"source":["pred_ind = []\n","for inp in x:\n","    length = x.size(1) - (inp == 1).long().sum()\n","    inp = inp[:length]\n","    pred_ind.append(learn.model.eval()(inp[None].cuda()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNKvRDqBuAwb"},"source":["assert near(pred_batch, torch.cat(pred_ind))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYw3QQacuAwb"},"source":[""],"execution_count":null,"outputs":[]}]}