{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"02_fully_connected_annotated_by_moonjung.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"b6hNunCF5-Xt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80d4a907-3033-4f3b-cc4d-d53939b02b92"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vgTY2qeJ93x2"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMSTsOm36Ake","outputId":"50734226-dffc-470d-913a-254b69998032"},"source":["  from google.colab import drive\n","  drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vzh-eI8g5-Xy"},"source":["## The forward and backward passes"]},{"cell_type":"code","metadata":{"id":"z8zzwU5P6QPd"},"source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOHZ_3Ls7MSg","outputId":"05412e39-a4a5-4a61-a94a-e4f93fc79934"},"source":["sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2',\n"," '/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2',\n"," '/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2',\n"," '/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2',\n"," '/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Mm9wPIu45-Xz"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=4960)"]},{"cell_type":"code","metadata":{"id":"z7T-i77P5-Xz"},"source":["#export\n","from exp.nb_01 import *\n","\n","\n","\n","def get_data():\n","    import os\n","    import torchvision.datasets as datasets\n","    \n","    datasets.MNIST.resources = [\n","        ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'),\n","        ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'),\n","        ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'),\n","        ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c')\n","    ]\n","    \n","    root = '../data'\n","    if not os.path.exists(root):\n","        os.mkdir(root)\n","    train_set = datasets.MNIST(root=root, train=True, download=True)\n","    test_set = datasets.MNIST(root=root, train=False, download=True)\n","    X_train, X_valid = train_set.data.split([50000, 10000])\n","    Y_train, Y_valid = train_set.targets.split([50000, 10000])\n","    \n","    return (X_train.view(50000, -1) / 256.0), Y_train.float(), (X_valid.view(10000, -1))/ 256.0, Y_valid.float()\n","\n","#x_train,y_train,x_valid,y_valid = get_data()\n","\n","\n","#def get_data():\n","#    path = datasets.download_data(MNIST_URL, ext='.gz')\n","#    with gzip.open(path, 'rb') as f:\n","#        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n","#    return map(tensor, (x_train,y_train,x_valid,y_valid))\n","\n","def normalize(x, m, s): return (x-m)/s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYm5W0QcGiQZ"},"source":["In the following, I will add mathematical description for the python statements that define the model and the gradients of the loss function with respect to the parameters (weights) of the neural net studied in this notebook. \n","\n","Please review what you studied in linear algebra or college math. This mathematical description of the neural net is an application of your basic knowledge of linear algebra.\n","Without the basic knowledge of linear algebra, that is, the\n","language of vectors, matrices, transformations, it is not possible to follow this course successfully. You need to spend time to review or study it. I provided good references in the cybercampus site. \n","\n","Note that sometimes, I will use variables different from the original notebook, for clarification and for the match with the mathematical description of the network. \n","\n","It is really important to be able to understand the mathematical description of the network for \"machine learning from foundations\". Much of this mathematical description will be the subject matter of the mid-term exam."]},{"cell_type":"code","metadata":{"id":"NfmPbWZZ5-X0"},"source":["X_train,Y_train,X_valid,Y_valid = get_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik0tdaD_8-Ij","outputId":"9008a671-9581-40fd-ebde-5fbf65f90d1e"},"source":["len(X_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"8uUZyVF_wGvn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ca75c3a-0fc1-4bb5-bf9c-a1f9c488185e"},"source":["Y_valid.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10000])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"3vNHIGAOH0VS"},"source":["X_train is a set of input vectors to the neural net, represented as a matrix: \n","\n","X_train =  \\\\\n","$X = \\begin{bmatrix}\n","  x^{(1)}  \\\\ \\vdots \\\\ x^{(m)} \n","   \\end{bmatrix}\n"," =  \n","\\begin{bmatrix} \n"," x_{1}^{(1)} & \\cdots  & x_{n^{[0]}}^{(1)} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," x_{1}^{(m)} & \\cdots &  x_{n^{[0]}}^{(m)}\n","  \\end{bmatrix}\n","  $\n","\n"," Here, the index in the parenthesis superfix 1 in $x^{(1)}$ means that input vector $x^{(1)}$ is from  from data sample 1. Each data sample is represented as a row vector $x^{(s)}$.  The suffix 1 in $x_{1}^{(1)}$ means that $x_{1}^{(1)}$  is the first element of vector $x^{(1)}$. $m$ is the number of samples in the dataset:  $m=50,000$. $n^{[0]}$ is the size of the input vector on the 0th layer (the input layer):  $n^{[0]}=784$. In the notebook, $n^{[0]}$ is written as $n0$. \n","\n"," Note that \n"," $\\begin{bmatrix}\n","  x^{(1)}  \\\\ \\vdots \\\\ x^{(m)} \n","   \\end{bmatrix}\n"," $ is often written as \n","\n","  $\\begin{bmatrix}\n","  x^{(1)} & \\cdots & x^{(m)} \n","   \\end{bmatrix}^{T}$ to save space.\n","\n","  In summary, note that the input to the neural net for training is represented as a matrix x_train. In this representation, the rows of the matrix x_train refer to samples. Some software and papers represent X so that the columns of x_train refer to samples as follows:\n","\n","  X_train \\\\\n","$X = \\begin{bmatrix}\n","  x^{(1)}   \\cdots  x^{(m)} \n","   \\end{bmatrix}\n"," = \n","\\begin{bmatrix} \n"," x_{1}^{(1)} & \\cdots  & x_{1}^{(m)} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," x_{n^{[0]}}^{(1)} & \\cdots &  x_{n^{[0]}}^{(m)}\n","  \\end{bmatrix}\n","  $"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XGPCf8T9UjJ","outputId":"c2e1505b-d6e3-4a1a-da8a-0e04654e865c"},"source":["X_train[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([784])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"VbILlqfJG7Er"},"source":[" The number of nodes in  the input layer =  the size of the 0th layer =  the size of the input vector, x,  = 784 = 28 * 28 = x_train[0].shape.  In the mathematical description of the network, I will use $n^{[0]}$ to refer to the size of the 0th layer: $n^{[0]}$ = 784."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaZzISo_5-X1","outputId":"853d6473-793b-4bf4-9a42-4857fb5c4c3c"},"source":["train_mean,train_std = X_train.mean(),X_train.std()\n","train_mean,train_std"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1304), tensor(0.3073))"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"I4Gu7BF3Chu8"},"source":["In this notebook, we construct a fully connected (FC) neural net with two layers, one hidden layer and the output layer. The input layer, considered the 0th layer, is not counted in the number of layers. \n","\n","The number of nodes in the hidden layer = the size of the hidden layer = 50. The number of nodes in  the input layer =  the size of the 0th layer =  the size of the input vector, x,  = 784 = 28 * 28. The number of nodes in the output layer = the size of the output layer = 1. In the mathematical description of the network, I will use $n^{[0]}$, $n^{[1]}$, $n^{[2]}$ to refer to the size of the 0th layer, the 1st layer, and the second layer, respectively. Hence we have:\n","\n","$n^{[0]}$ = 784, = $n^{[1]}$ = 50, $n^{[2]}$ =1.\n","\n","In the notebook, $n^{[0]}$, $n^{[1]}$, $n^{[2]}$ will be written as n0, n1, n2, respectively. "]},{"cell_type":"code","metadata":{"id":"S3nmN8UL5-X3"},"source":["X_train = normalize(X_train, train_mean, train_std)\n","# NB: Use training, not validation mean for validation set\n","X_valid = normalize(X_valid, train_mean, train_std)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCCqEllW5-X3","outputId":"ba0af0b4-5fe9-4ee8-f0a5-6d7ab3e6d1ba"},"source":["train_mean,train_std = X_train.mean(),X_train.std()\n","train_mean,train_std"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(3.8966e-08), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"6xBjabxA5-X4"},"source":["#export\n","def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vS8_j4lZ5-X4"},"source":["test_near_zero(X_train.mean())\n","test_near_zero(1-X_train.std())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-awerTWNc63"},"source":["m is the number of samples in the dataset. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_Dz6Z6YOQF9","outputId":"9ccf657c-95a7-41be-d1e0-954f39636505"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000, 784])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzgYF69T5-X5","outputId":"d66ce9f6-8b0b-4429-bfeb-6d9dc884f81a"},"source":["m, n0 = X_train.shape\n","c = Y_train.max()+1\n","m,n0,c\n","Y_train.max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9.)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"vagz-GZe5-X5"},"source":["## Foundations version"]},{"cell_type":"markdown","metadata":{"id":"pfeJZz_45-X5"},"source":["### Basic architecture"]},{"cell_type":"markdown","metadata":{"id":"NnJVOx2vBrpR"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"4kqnGdFs5-X6"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=5128)"]},{"cell_type":"markdown","metadata":{"id":"_wo4BvAdB57G"},"source":[""]},{"cell_type":"code","metadata":{"id":"WzFgW_Wi5-X6"},"source":["# num hidden\n","n1 = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_NV4eLQ5-X6"},"source":["[Tinker practice](https://course.fast.ai/videos/?lesson=8&t=5255)"]},{"cell_type":"markdown","metadata":{"id":"W968ePvjRSoQ"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"sYJAHMZ5aciZ"},"source":[" \n","The definition of the first layer:\n","\n","The weighted sum:\n","\n","Note that matrices are written in capital letters.\n","\n","$Z^{[1]}\n","=  X W^{[1]} + b^{[1]}$\n","\n","$ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}, s=1,m = 50000$\n","\n","\n","\n"," $ z^{[1](s)}\n"," = \\begin{bmatrix} z_{1}^{[1](1)} & \\cdots  & z_{n^{[1]}}^{[1](1)}\\end{bmatrix}\n","= \\begin{bmatrix} x^{(s)} w_{\\bullet  1}^{[1]} & \\cdots &  x^{(s)} w_{\\bullet  n^{[1]}}^{[1]} \\end{bmatrix} \n","+ \\begin{bmatrix} b_{1}^{[1]} & \\cdots & b_{n^{[1]} }^{[1]}\\end{bmatrix} \\\\\n","$\n","\n"," $ z^{[1](s)}\n"," = \\begin{bmatrix} z_{1}^{[1](1)} & \\cdots  & z_{n^{[1]}}^{[1](1)}\\end{bmatrix}\n","= \\begin{bmatrix} \n","x_{j}^{(s)} w_{j1}^{[1]} & \\cdots &  x_j^{(s)} w_{j n^{[1]}}^{[1]} \n","\\end{bmatrix} \n","+ \\begin{bmatrix} b_{1}^{[1]} & \\cdots & b_{n^{[1]} }^{[1]}\\end{bmatrix} \\\\\n","$\n","\n","Note that $w_{\\bullet  1}^{[1]}$ refers to the column $1$ of matrix $W^{[1]}$.\n","Note that $x_{j}^{(s)} w_{j1}^{[1]}$ is the Einstein summation for weighted sum: \n","\n","$ x_{j}^{(s)} w_{j1}^{[1]} = \\sum_{j=1}^{n^{[0]}}x_{j}^{(s)} w_{j1}^{[1]} \\\\\n","$  \n","\n","Note that because the input vector $x^{(s)}$ is a row vector, the output of the neuron on the first layer $z^{[1](s)}$  is also a row vector.\n","\n","\n"," $\\begin{bmatrix} \n"," z^{[1](1)}  \\\\ \n"," \\vdots     \\\\ \n"," z^{[1](m)}  \n"," \\end{bmatrix}\n"," $  =\n","$\\begin{bmatrix} \n"," z_{1}^{[1](1)} & \\cdots  & z_{n^{[1]}}^{[1](1)} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," z_{1}^{[1](m)} & \\cdots &  z_{n^{[1]}}^{[1](m)} \n"," \\end{bmatrix}\n"," $ \\\\\n","\n"," \n"," = \n","$\\begin{bmatrix} \n"," x_{1}^{(1)} & \\cdots  & x_{n^{[0]}}^{(1)} \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," x_{1}^{(m)} & \\cdots &  x_{n^{[0]}}^{(m)} \n"," \\end{bmatrix}\n"," $\n"," $ \n","\\begin{bmatrix} \n"," w_{11}^{[2]} & \\cdots  & w_{1n^{[1]}}^{[2]} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," w_{n^{[0]}1}^{[2]} & \\cdots &  w_{n^{[0]}n^{[1]}}^{[2]} \\end{bmatrix} \n"," $ \\\\\n","\n"," +\n"," $ \n","  \\begin{bmatrix} \n","   b_{1}^{[1]}  & \\cdots &  b_{n^{[1]}}^{[1]} \n","  \\end{bmatrix} \n"," $ \n","\n","=\n","$\\begin{bmatrix} \n"," x^{(1)} \\cdot  w_{\\bullet 1}^{[2]} & \\cdots  & x^{(1)} \\cdot w_{\\bullet n^{[1]}}^{[2]} \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," x^{(m)} \\cdot  w_{\\bullet 1}^{[2]}  & \\cdots & \n"," x^{(m)} \\cdot w_{\\bullet n^{[1]}}^{[2]} \n"," \\end{bmatrix}\n"," $\n"," \\\\\n","\n"," +\n"," $ \n","  \\begin{bmatrix} \n","   b_{1}^{[1]}  & \\cdots &  b_{n^{[1]}}^{[1]}  \\\\\n","   \\vdots  &  \\cdots  & \\vdots \\\\\n","    b_{1}^{[1]}  & \\cdots &  b_{n^{[1]}}^{[1]}\n","  \\end{bmatrix} \n"," $ \n","\n","Note again that $s$ refers to each data sample vector. $n^{[0]}$, $n^{[1]}$ refers to the number of nodes in the 0th layer (input layer) and the 1st layer. \n","\n","Note that the last line of the above formula is obtained by the broadcasting of row vector \n","$\\begin{bmatrix}b_{1}^{[1]}  & \\cdots &  b_{n^{[1]}}^{[1]}\\end{bmatrix}$.\n","\n","The activation:\n","\n","$ \n"," A^{[1]}\n"," =  g( Z^{[1]}) = g(X W^{[1]} + b^{[1]}) $\n","\n","  $ \n"," \\begin{bmatrix} \n"," a^{[1](1)} \\\\ \n"," \\vdots  \\\\ \n"," a^{[1](m)}  \n"," \\end{bmatrix}\n","=\\begin{bmatrix} \n"," a_{1}^{[1](1)} & \\cdots  & a_{n^{[1]}}^{[1](1)} \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," a_{1}^{[1](m)} & \\cdots &  a_{n^{[1]}}^{[1](m)} \n"," \\end{bmatrix}\n","=\\begin{bmatrix} \n","g(z_{1}^{[1](1)}) & \\cdots  & g(z_{n^{[1]}}^{[1](1)}) \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," g(z_{1}^{[1](m)}) & \\cdots &  g(z_{n^{[1]}}^{[1](m)})  \\end{bmatrix} \n"," =\\begin{bmatrix} \n","g(z^{[1](1)})  \\\\ \n"," \\vdots     \\\\ \n"," g(z^{[1](m)})  \\end{bmatrix} \n","$\n","\n","In our network, $g = relu$. The activations $A^{[1]}$ of the 1st layer will be used as the input to the second layer.\n","\n","\n","Here,  the index in the square superfix 2 in $z^{[1](1)}$ means that the variable $z^{[1](1)}$ lives on the 1st  layer of the network. \n"," The index in the parenthesis superfix 1 in $z^{[1](1)}$ means that the variable $z^{[1](1)}$ is computed from data sample 1.  The suffix 1 in \n","  $z_{1}^{[1](1)}$ means that $z_{1}^{[1](1)}$  is the first element of vector $z^{[1](1)}$.  $n^{[i]}$ is the number of the nodes (units or neurons) on the $i$-th layer. The 0th layer of the network refers to the input layer. \n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"77kObYlhC3U1"},"source":["The definition of the 2nd layer.\n","\n"," \n","\n","$ Z^{[2]}\n","=  A^{[1]} W^{[2]} + b^{[2]}$\n","\n","$ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}, s=1,m = 50000$\n","\n","\n","$\\begin{bmatrix} \n"," z_{1}^{[2](1)} & \\cdots  & z_{n^{[2]}}^{[2](1)} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," z_{1}^{[2](m)} & \\cdots &  z_{n^{[2]}}^{[2](m)} \n"," \\end{bmatrix}\n"," $ \\\\\n","\n"," = \n","$\\begin{bmatrix} \n"," a_{1}^{(1)} & \\cdots  & a_{n^{[1]}}^{(1)} \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," a_{1}^{(m)} & \\cdots &  a_{n^{[1]}}^{(m)} \n"," \\end{bmatrix}\n"," $\n"," $ \n","\\begin{bmatrix} \n"," w_{11}^{[2]} & \\cdots  & w_{1n^{[2]}}^{[2]} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," w_{n^{[1]}1}^{[2]} & \\cdots &  w_{n^{[1]}n^{[2]}}^{[2]} \\end{bmatrix} \n"," $ \\\\\n","\n"," +\n"," $ \n","  \\begin{bmatrix} \n","   b_{1}^{[2]}  & \\cdots &  b_{n^{[2]}}^{[2]} \n","  \\end{bmatrix} \n"," $ \n","\n","\n","\n","Note that  the activations $A^{[1]}$ of the 1st layer are used as the input to weight matrix $W^{[2]}$. \n"," Here, the index in the square superfix 2 in $z^{[2](1)}$ means that the variable $z^{[2](1)}$ lives on the 2nd layer of the network. \n"," The index in the parenthesis superfix 1 in $z^{[2](1)}$ means that the variable $z^{[2](1)}$ is computed from data sample 1.  The suffix 1 in $z_{1}^{[2](1)}$ means that $z_{1}^{[2](1)}$  is the first element of vector $z^{[2](1)}$. \n"," \n"," \n"," \n","The size of the second layer (the output layer), $n^{[2]}$ is more than one, in general. But in this neural net, there is only one node on the second layer, hence only one element in the output vector $z^{[2](i)}$ on the 2nd layer.  \n","So we have:\n","\n","\n","$\\begin{bmatrix} \n"," z_{1}^{[2](1)} \\\\ \n"," \\vdots      \\\\ \n"," z_{1}^{[2](m)}  \n"," \\end{bmatrix}\n"," $ \\\\\n"," = \n","$\\begin{bmatrix} \n"," a_{1}^{[1](1)} & \\cdots  & a_{n^{[1]}}^{[1](1)} \\\\ \n"," \\vdots       & \\cdots & \\vdots \\\\ \n"," a_{1}^{(m)} & \\cdots &  a_{n^{[1]}}^{[1](m)} \n"," \\end{bmatrix}\n"," $\n"," $ \n","\\begin{bmatrix} \n"," w_{11}^{[2]}  \\\\ \n"," \\vdots    \\\\ \n"," w_{n^{[1]}1}^{[2]} \n","  \\end{bmatrix} \n"," $ \\\\\n"," +\n"," $ \n","  \\begin{bmatrix} \n","   b_{1}^{[2]}   \n","  \\end{bmatrix} \n"," $ \\\\\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"WObdO6AgT19v"},"source":["In the above, the description of the 1st layer\n","\n","$ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}$ \n","\n","assumes that input vector $x^{(s)}$ is written a row vector.\n","In this case, the vectors in all the layers are considered as row vectors. \n","\n","Other people often write input vector $x^{(s)}$ as a column vector as follows:\n","\n","\n","  x_train \\\\\n","$= \\begin{bmatrix}\n","  x^{(1)}   \\cdots  x^{(m)} \n","   \\end{bmatrix}\n"," = \n","\\begin{bmatrix} \n"," x_{1}^{(1)} & \\cdots  & x_{1}^{(m)} \\\\ \n"," \\vdots          & \\cdots & \\vdots \\\\ \n"," x_{n^{[0]}}^{(1)} & \\cdots &  x_{n^{[0]}}^{(m)}\n","  \\end{bmatrix}\n","  $\n","\n","  In this case, the description of the 1st layer is written as:\n","\n","  $ z^{[1](s)}\n","=  W^{[1]} x^{(s)}  + b^{[1]}$ \n","\n","Here the output vector $z^{[1](s)}$  is a column vector. Also, the vectors on the other layers are considered as column vectors. If you want to convert this column vector notation to our row vector notation, transpose the equation as follows:\n","\n","$ {z^{[1](s)}}^{T}\n","=  ( W^{[1]} x^{(s)} ) ^{T} + {b^{[1]}}^{T}\n","= {x^{(s)}}^{T}{W^{[1]}}^{T}   + { b^{[1]}}^{T}\n","$ \n","\n","In the row vector notation, the weight matrix W comes next to the row vector. In the column vector notation, the weight matrix comes before the column vector. The column vector notation is the standard in mathematics and other machine learning API's. \n","We use the row vector notattion because the fastai code uses the row vector notation."]},{"cell_type":"markdown","metadata":{"id":"F6_hSGCpa2ep"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"86MGtdAGg_M3"},"source":["To train the neural net means to determine the values of the weights $W^{[1]}, W^{[2]}$ and biases $b^{[1]}, b^{[2]}$, so that the loss function L ($W^{[1]}, W^{[2]}$,  $b^{[1]}, b^{[2]}$) is minimized. To do so, we need to initialize them to random numbers chosen appropriately, as follows:\n","\n","Note that in the noteboo, $W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}, A ^{[1]} $ are written as W1, W2, b1, b2, A1."]},{"cell_type":"code","metadata":{"id":"8Xzlvhy25-X7"},"source":["# simplified kaiming He  init\n","W1 = torch.randn(n0,n1)/math.sqrt(n0) # n0 is the size of the input vector x, n1 is the number of nodes in the 1st layer\n","b1 = torch.zeros(n1)\n","W2 = torch.randn(n1,1)/math.sqrt(n1) # 1 is the number of nodes in the second layer (the output layer)\n","b2 = torch.zeros(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-jtQUbI5-X8","colab":{"base_uri":"https://localhost:8080/","height":250},"outputId":"b7877e3a-8b9e-4a56-8a6e-24d4d8be3409"},"source":["test_near_zero(W1.mean())\n","test_near_zero(W1.std()-1/math.sqrt(m))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-07902b9a5759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-4e308fe2567d>\u001b[0m in \u001b[0;36mtest_near_zero\u001b[0;34m(a, tol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Near zero: {a}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: Near zero: 0.031315840780735016"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBjo8r7O5-X9","outputId":"29c95de3-e079-4e8b-cdbe-f5fbd30e717d"},"source":["# This should be ~ (0,1) (mean,std)...\n","X_valid.mean(),X_valid.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-0.0059), tensor(0.9924))"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"c8dIv9H2k3Iy"},"source":["Function lin() is used to compute\n","\n"," $ Z^{[2]}\n","=  A^{[1]} W^{[2]} + b^{[2]}$ and $ Z^{[1]}\n","=  X W^{[1]} + b^{[1]}$ "]},{"cell_type":"code","metadata":{"id":"6T6Hp_oM5-X-"},"source":["def lin(X, W, b): return X@W + b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-faJxX55-X_"},"source":["Z1 = lin(X_valid, W1, b1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4LwFw7dY3eF","outputId":"6b09bcd1-f9d3-443e-933b-51b86d16884f"},"source":["X_valid.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10000, 784])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uj2llJ7b5-X_","outputId":"1dfb02a9-6253-48ad-c69d-602c325d6228"},"source":["#...so should this, because we used kaiming init, which is designed to do this\n","Z1.mean(),Z1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-0.0940), tensor(0.9507))"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"8yANZFMz5-YB"},"source":["def relu(Z): return Z.clamp_min(0.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fk5_q7ld5-YC"},"source":["Z1 = lin(X_valid, W1, b1)\n","A1 = relu( Z1 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iv7GQ0c45-YC","outputId":"55a9305e-4629-4162-9297-fddeca7f17e1"},"source":["#...actually it really should be this!\n","A1.mean(),A1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.3322), tensor(0.5370))"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"-tGq6Sta5-YD"},"source":["From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n","\n","$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n","\n","This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!)"]},{"cell_type":"markdown","metadata":{"id":"5pqJ9i9m5-YD"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=5128)"]},{"cell_type":"code","metadata":{"id":"09FOdeFH5-YD"},"source":["# kaiming init / he init for relu\n","W1 = torch.randn(n0,n1)*math.sqrt(2/n0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ePZ4wD15-YE","outputId":"44debba2-7719-4bb7-bc36-4ce4ad7e95fa"},"source":["W1.mean(),W1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0003), tensor(0.0506))"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJdmqNag5-YE","outputId":"6da9dc91-64f7-4a82-aef1-309d29d28752"},"source":["Z1 = lin(X_valid, W1, b1)\n","A1 = relu( Z1)\n","A1.mean(),A1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.5202), tensor(0.7672))"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"r8O1NjrC5-YF"},"source":["#export\n","from torch.nn import init"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjXu4Gq05-YF"},"source":["W1 = torch.zeros(n0,n1)\n","init.kaiming_normal_(W1, mode='fan_out')\n","A1 = relu(lin(X_valid, W1, b1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8Thp0YVo3as","colab":{"base_uri":"https://localhost:8080/"},"outputId":"855a65f4-06be-471c-de36-1ae1a4e2cab6"},"source":["\n","W1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([784, 50])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"mhCoytJqpK5i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"681a45ec-fe33-4e6a-d6a6-51b511898d50"},"source":["W1.t().shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50, 784])"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"5jETx2oR5-YF"},"source":["init.kaiming_normal_??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJDqDh6l5-YF","outputId":"20440b84-5300-4eb5-8e6c-0c1e1dc9b943"},"source":["W1.mean(),W1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-0.0004), tensor(0.0513))"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqoOHNXn5-YG","outputId":"93a8e468-e943-4d9b-af4f-cfb41710156c"},"source":["A1.mean(),A1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1692), tensor(0.2476))"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wv7oXXVO5-YG","outputId":"8571560d-b44e-4aa6-80af-9aab121ecf3d"},"source":["W1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([784, 50])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"J0Tg2Uq15-YG"},"source":["import torch.nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzfYmlPYpfx_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"88849bec-0c6f-4559-8021-3c25f805ed00"},"source":["X_valid[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([784])"]},"metadata":{"tags":[]},"execution_count":137}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRgE733i5-YH","outputId":"ba179788-395a-4cdc-a210-48730411400c"},"source":["torch.nn.Linear(n0,n1).weight.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50, 784])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"m9_aH0It5-YH"},"source":["torch.nn.Linear.forward??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjHq6PoH5-YH"},"source":["torch.nn.functional.linear??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VsxXtMp5-YI"},"source":["torch.nn.Conv2d??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EDFhDvI-5-YI"},"source":["torch.nn.modules.conv._ConvNd.reset_parameters??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDZaD2dA5-YI"},"source":["# what if...?\n","def relu(Z): return Z.clamp_min(0.) - 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jNw_L-B5-YI","outputId":"acaef2cb-05c2-47a3-d2e1-139d71d963bb"},"source":["# kaiming init / he init for relu\n","W1 = torch.randn(n0,n1)*math.sqrt(2./n0 )\n","A1 = relu(lin(X_valid, W1, b1))\n","A1.mean(),A1.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0013), tensor(0.7653))"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"7IJ5e9X-xT2a"},"source":["function model(X) computes the output of the neural net Z2 such that\n","\n"," $ Z^{[2]}\n","=  A^{[1]} W^{[2]} + b^{[2]}$\n","\n","$ Z^{[1]}\n","=  X W^{[1]} + b^{[1]}$ "]},{"cell_type":"code","metadata":{"id":"NhvenibH5-YJ"},"source":["def model(X):\n","    Z1 = lin(X, W1, b1)\n","    #print (\"Z1=\", Z1)\n","    A1= relu(Z1)\n","    #print (\"A1 = \", A1)\n","    Z2= lin(A1, W2, b2)\n","    #print (\"Z2 = \", Z2)\n","    return Z2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DU5VFUTBG8uU"},"source":[" Z2=model(X_valid[:2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1c1X6Wu5-YJ"},"source":["%timeit -n 10 _=model(X_valid[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9SoPoSl5-YJ"},"source":["assert model(X_valid).shape==torch.Size([X_valid.shape[0],1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlyflLbC5-YK"},"source":["### Loss function: MSE"]},{"cell_type":"markdown","metadata":{"id":"QMKbmGY_5-YK"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=6372)"]},{"cell_type":"markdown","metadata":{"id":"0BQsZ2xW5-YK"},"source":["We need `squeeze()` to get rid of that trailing (,1), in order to use `mse`. (Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"]},{"cell_type":"markdown","metadata":{"id":"2SoqHuMr9Xgq"},"source":["\n"," "]},{"cell_type":"markdown","metadata":{"id":"bOEHIG5gLe7D"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"FcQxM1bBK4SZ"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"bRf_q9JzNngt"},"source":["The loss function $L$:\n","\n","\n"," $L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \n","$ \\\\\n","\n"," $ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}$\n","\n","$ a^{[1](s)} = g( z^{[1](s)} )$\n","\n"," $ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}$\n","\n","\n","Here, $Y\\_train$\n","$ \n","= \\begin{bmatrix}\n"," y^{(1)} \\\\\n","  \\vdots \\\\\n","   y^{(m)}\n","\\end{bmatrix}\n","$\n","\n","Each $y{(s)}$ is a row vector in general. But in this neural net, it is a scalar. But the form of the equation does not change nor the Python code for it. \n","\n","Now to train the neural net, we need to compute the following gradient matrices of $L$ with respec to parameters $W^{[1]}, b^{[1]}, W^{[2]},  b^{[2]}$:\n","\n"," $\\frac{\\partial L}{\\partial W^{[2]}} = \\left[  \\frac{\\partial L}{\\partial w_{ij}^{[2]}} \\right]$ \\\\\n"," $\\frac{\\partial L}{\\partial b^{[2]}} = \\left[ \\frac{\\partial L}{\\partial b_{i}^{[2]}} \\right]$ \\\\\n"," $\\frac{\\partial L}{\\partial W^{[1]}} = \\left[   \\frac{\\partial L}{\\partial w_{ij}^{[1]}} \\right]$ \n"," \\\\\n"," $\\frac{\\partial L}{\\partial b^{[1]}} = \\left[ \\frac{\\partial L}{\\partial b_{i}^{[1]}} \\right]$\n"," \n","\n","During the training of the network,\n","the gradients are used to update the values of the parameters as follows:\n","\n","$\n","W^{[1]} = W^{[1]} + \\eta (-\\frac{\\partial L}{\\partial W^{[1]}} ) \\\\\n","b^{[1]} = b^{[1]} + \\eta (-\\frac{\\partial L}{\\partial b^{[1]}} ) \\\\\n","W^{[2]} = W^{[2]} + \\eta (-\\frac{\\partial L}{\\partial W^{[2]}} ) \\\\\n","b^{[2]} = b^{[2]} + \\eta (-\\frac{\\partial L}{\\partial b^{[2]}} ) \\\\\n","$"]},{"cell_type":"markdown","metadata":{"id":"OnRnvwOFMd5O"},"source":["\n","\n"," \n"," $L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \\\\\n","$ \n","\n"," $ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}$\n","\n","$\\frac{\\partial L}{\\partial W^{[2]}}\n"," = \\left[    \\frac{\\partial L}{ \\partial w_{ij}^{[2]} } \\right]$\n","\n","$L$ is a function of $z^{[s](2)}$ which is a function of $w_{ij}^{[2]}$:\n","\n","$  \\frac{\\partial L}{ \\partial w_{ij}^{[2]} }\\\\\n"," = \\frac{1}{m}\\sum_{s=1}^{m}\n"," \\frac{\\partial L} {\\partial z^{[2](s)}}\n"," \\frac{\\partial z^{[2](s)} }{\\partial w_{ij}^{[2]} }\n"," $\n","\n","The gradient of $L$ with respect to vector $z^{[2](s)}$,\n"," $\\frac{\\partial L}{\\partial z^{[2](s)} }$, is a row vector:\n","\n"," $\n","   \\frac{\\partial L}{\\partial z^{[2](s)} } \\\\\n","   = \\begin{bmatrix}\n","    \\frac{\\partial L }{ \\partial z_{1}^{[2](s)} }\n","       \\cdots \n","        \\frac{\\partial L }{ \\partial {z_{n^{[2]}}}^{[2](s)} }\n","    \\end{bmatrix} \n","   =   2*(z^{[2](s)} - y^{(s)})  \n"," $ \n","\n","The gradient of vector $z^{[2](s)}$  relative to scalar $w_{ij}^{[2]}$, $\\frac{\\partial z^{[2](s)} }{\\partial w_{ij}^{[2]} }$,  is a column vector.\n","\n","\n","\n","We have:\n","\n","\n","$\\frac{\\partial L}{ \\partial w_{ij}^{[2]} }\n","=\\frac{\\partial L} {\\partial z^{[2](s)}}\\frac{\\partial z^{[2](s)} }{\\partial w_{ij}^{[2]} }\n","=  \\sum_{k=1}^{n^{[2]}} \\frac{\\partial L}{\\partial z_{k}^{[2](s)} } \\frac{\\partial  z_{k}^{[2](s)}  }{\\partial w_{ij}^{[2]} } \\\\\n","= \n","\\sum_{k=1}^{n^{[2]}} \\frac{\\partial L}{\\partial z_{k}^{[2](s)} } \\frac{ \\partial   \\sum_{l=1}^{n^{[1]}} a_{l} ^{[1](s)} w_{lk}^{[2]}  }{\\partial w_{ij}^{[2]} }\\\\\n","= \\sum_{k=1}^{n^{[2]}} \\frac{\\partial L}{\\partial z_{k}^{[2](s)} } \\frac{   \\sum_{l=1}^{n^{[1]}} a_{l} ^{[1](s)} \\partial w_{lk}^{[2]}  }{\\partial w_{ij}^{[2]} }\n","=  \\frac{\\partial L}{\\partial z_{j}^{[2](s)} } \\frac{    a_{i} ^{[1](s)} \\partial w_{ij}^{[2]}  }{\\partial w_{ij}^{[2]} }\n","=  \\frac{\\partial L}{\\partial z_{j}^{[2](s)} } a_{i}^{[1](s)} ( scalar)\n","$ \n","\n","Finally we have\n","\n","$\\frac{\\partial L}{\\partial W^{[2]}}\n"," = \\left[    \\frac{\\partial L}{ \\partial w_{ij}^{[2]} } \\right] \n"," =\\left[ \\frac{1}{m}\\sum_{s=1}^{m}  \n","  \\frac{\\partial L}{\\partial z_{j}^{[2](s)} } a_{i}^{[1](s)} \n","   \\right] \\\\\n","   =\\left[ \\frac{1}{m}\\sum_{s=1}^{m} a_{i}^{[1](s)}  \n","  \\frac{\\partial L}{\\partial z_{j}^{[2](s)} } \n","   \\right] \\\\\n","   = \\frac{1}{m}\\sum_{s=1}^{m} {a^{[1](s)}}^{T}  \n","  \\frac{\\partial L}{\\partial z^{[2](s)} } \n","  $\n"]},{"cell_type":"markdown","metadata":{"id":"A4zu7A1nWnji"},"source":["$\\frac{\\partial L}{\\partial b^{[2]}}\n","  = \\frac{\\partial L}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} \\\\\n","  =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[2](s)}}\n","   \\frac{\\partial z^{[2](s)}}{\\partial b^{[2]}}\n","   =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[2](s)}}\n","   $\n"," \n"," \n"]},{"cell_type":"markdown","metadata":{"id":"6z60uMeDv5GR"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"34KVl_8bbr8k"},"source":["\n"," $L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \n","$ \n","\n","$\\frac{\\partial L}{\\partial W^{[1]}}\n"," = \\left[    \\frac{\\partial L}{ \\partial w_{ij}^{[1]} } \\right]$\n","\n","$  \\frac{\\partial L}{ \\partial w_{ij}^{[1]} }\n"," = \\frac{1}{m}\\sum_{s=1}^{m}\n"," \\frac{\\partial L} {\\partial z^{[1](s)}}\n"," \\frac{\\partial z^{[1](s)} }{\\partial w_{ij}^{[1]} } \\\\\n"," $\n","\n","\n","\n","$\\frac{\\partial L} {\\partial z^{[1](s)}}\n"," = \\frac{\\partial L}{\\partial a^{[1](s)}}\n"," \\frac{\\partial a^{[1](s)}} {\\partial z^{[1](s)}}\n"," = \\frac{\\partial L} {\\partial z^{[2](s)}}\n"," \\frac{\\partial z^{[2](s)}} {\\partial a^{[1](s)}}\n"," \\frac{\\partial a^{[1](s)}} {\\partial z^{[1](s)}}\n","$\n","\n","\n","$\n","   \\frac{\\partial L}{\\partial z^{[2](s)} } =2*(z^{[2](s)} - y^{(s)})\n"," $ \n","\n"," $ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}$\n","\n","$ a^{[1](s)} = g( z^{[1](s)} )$\n","\n"," $ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}$\n","\n","$\n"," \\frac{\\partial z^{[2](s)}} {\\partial a^{[1](s)}}\n"," =  {W^{[2]}}^{T}\n"," $\n","\n","$ \\frac{\\partial a^{[1](s)}}{\\partial z^{[1](s)}}\n"," =\\frac{\\partial relu(z^{[1](s)} ) } {\\partial z^{[1](s)}} \n"," = diag( sgn ( relu'(z^{[1](s)} ) ) )\n"," $\n","\n","Here, \n","\n","$ sgn( x) = 1$  if $x > 0$, $0$ otherwise. \n","\n","So we have\n","\n","$\\frac{\\partial L} {\\partial z^{[1](s)}}\n"," =\\frac{\\partial L}{\\partial a^{[1](s)}}\n"," \\frac{\\partial a^{[1](s)}} {\\partial z^{[1](s)}}\n"," = \n"," (\\frac{\\partial L} {\\partial z^{[2](s)}}\n"," \\frac{\\partial z^{[2](s)}} {\\partial a^{[1](s)}} )\n"," \\frac{\\partial a^{[1](s)}} {\\partial z^{[1](s)}} \\\\\n"," = ( 2*(z^{[2](s)} - y^{(s)})  {W^{[2]}}^{T} ) \\odot \n"," sgn(  relu'( z^{[1](s)} ) )  \n","$\n","\n","Now\n","\n","$ \\frac{\\partial L}{ \\partial w_{ij}^{[1]} }$\n"," =\n","$\\frac{\\partial L} {\\partial z^{[1](s)}}\\frac{\\partial z^{[1](s)} }{\\partial w_{ij}^{[1]} }\n","=  \\sum_{k=1}^{n^{[1]}} \\frac{\\partial L}{\\partial z_{k}^{[1](s)} } \\frac{\\partial  z_{k}^{[1](s)}  }{\\partial w_{ij}^{[1]} } \\\\\n","= \n","\\sum_{k=1}^{n^{[1]}} \\frac{\\partial L^{(s)}}{\\partial z_{k}^{[1](s)} } \\frac{ \\partial   \\sum_{l=1}^{n^{[1]}} x_{l} ^{(s)} w_{lk}^{[1]}  }{\\partial w_{ij}^{[1]} }\n","= \\sum_{k=1}^{n^{[1]}} \\frac{\\partial L}{\\partial z_{k}^{[1](s)} } \\frac{   \\sum_{l=1}^{n^{[1]}} x_{l} ^{(s)} \\partial w_{lk}^{[1]}  }{\\partial w_{ij}^{[1]} } \\\\\n","=  \\frac{\\partial L}{\\partial z_{j}^{[1](s)} } \\frac{    x_{i} ^{(s)} \\partial w_{ij}^{[1]}  }{\\partial w_{ij}^{[1]} }\n","=  \\frac{\\partial L}{\\partial z_{j}^{[1](s)} } x_{i}^{(s)}\n","$\n","\n","Finally\n","\n","$\\frac{\\partial L}{\\partial W^{[1]}}\n"," = \\left[    \\frac{\\partial L}{ \\partial w_{ij}^{[1]} } \\right] \n","  = \\left[ \\frac{1}{m}\\sum_{s=1}^{m}   \n","  \\frac{\\partial L}{\\partial z_{j}^{[1](s)} } x_{i}^{(s)}\n"," \\right] \\\\\n","  = \\left[ \\frac{1}{m}\\sum_{s=1}^{m}   \n"," x_{i}^{(s)} \\frac{\\partial L}{\\partial z_{j}^{[1](s)} }\n"," \\right] \\\\\n","= \\frac{1}{m}\\sum_{s=1}^{m}  x^{(s)^{T}} \\frac{\\partial L}{\\partial z^{[1](s)} }\n","$\n"]},{"cell_type":"markdown","metadata":{"id":"6g3pESYSbrU0"},"source":["$\\frac{\\partial L}{\\partial b^{[1]}}\n","  = \\frac{1}{m}\\frac{\\partial L}{\\partial b^{[1]}}  \n","  =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L} {\\partial z^{[1](s)}}\n","   \\frac{\\partial z^{[1](s)}}{\\partial b^{[1]}} \\\\\n","  =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L} {\\partial z^{[2](s)}}\n","    \\frac{\\partial z^{[2](s)}} {\\partial a^{[1](s)}}    \n","    \\frac{\\partial a^{[1](s)}} {\\partial z^{[1](s)}} I \\\\\n","    =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L} {\\partial z^{[2](s)}}\n","    {W^{[2]}}^{T}  \n","    diag( \\frac{\\partial relu} {\\partial z^{[1](s)}} )\\\\\n","     =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L} {\\partial z^{[2](s)}}\n","    {W^{[2]}}^{T} \\odot  \\frac{\\partial relu} {\\partial z^{[1](s)}} \n","   $\n","     "]},{"cell_type":"markdown","metadata":{"id":"MvvFiiM6H3DI"},"source":["\n"," "]},{"cell_type":"markdown","metadata":{"id":"jjkKEwU1Hsci"},"source":["\n"," "]},{"cell_type":"markdown","metadata":{"id":"fQGc9Mb0Hyij"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"lX8wDzNtt7vB"},"source":[" mse() function computes and returns the value of the loss function L:\n"," \n"," $L (Z2, Y)  = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \\\\\n","$ \n"]},{"cell_type":"code","metadata":{"id":"eOU_T3Lc5-YL"},"source":["#export\n","def mse(Z2, Y_train): return (Z2.squeeze(-1) - Y_train).pow(2).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkGRdw8y5-YL"},"source":["Y_train,Y_valid = Y_train.float(),Y_valid.float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hakvToYZ5-YL"},"source":["Z2 = model(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjWh0D-pdEfR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV1dQvJm5-YM"},"source":["loss = mse(Z2, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9G-jJnb45-YM"},"source":["### Gradients and backward pass"]},{"cell_type":"markdown","metadata":{"id":"XpgdxO7D5-YM"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=6493)"]},{"cell_type":"markdown","metadata":{"id":"okIIqki9zN0B"},"source":["$L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)})$\n","\n"," Compute the gradient of $L$ with respect to $Z^{[2]}$:\n","\n","$ \\frac{ \\partial L }{ \\partial Z^{[2]} } \n","= \\begin{bmatrix}\n"," \\frac{ \\partial L }{ \\partial z^{[2](1)} } \\\\\n"," \\vdots \\\\\n"," \\frac{ \\partial L }{ \\partial z^{[2](m)} }\n","\\end{bmatrix}\n","$\n","\n"," $\n","   \\frac{\\partial L}{\\partial z^{[2](s)} } =2*(z^{[2](s)} - y^{(s)}) / m\n"," $ "]},{"cell_type":"code","metadata":{"id":"eQVon0G75-YN"},"source":["def mse_grad(Z2, Y_train): \n","    # compute the grad of loss L with respect to Z2 and store it in Z2.g\n","   \n","\n","    #print (inp.g.shape)\n","    # tensor x_train is given a new field that contains its gradient relative y_hat\n","    m = Z2.shape[0] # the number of rows in Z2\n","    Z2.g = 2. * (Z2.squeeze() - Y_train).unsqueeze(-1) / m\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w049dquY8d3_"},"source":["$ \\frac{\\partial a^{[1](s)}}{\\partial z^{[1](s)}}\n"," =\\frac{\\partial relu(z^{[1](s)} ) } {\\partial z^{[1](s)}} \n"," = diag( sgn ( relu'(z^{[1](s)} ) ) )\n"," $\n","\n","Here, \n","\n","$ sgn( x) = 1$  if $x > 0$, $0$ otherwise.\n","\n","Represent the diagonal matrix \n","$diag( sgn ( relu'(z^{[1](s)} ) ) )$ as a vector $sgn ( relu'(z^{[1](s)} ) )$.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"L6uuYz5k5-YP"},"source":["def relu_grad(Z1, A1):\n","    # grad of the activation, relu,  with respect to Z1, and store it in Z1.g\n","    Z1.g = (Z1>0).float() * A1.g"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJnS7K7RAbny"},"source":["\n"," $L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \n","$ \\\\\n","\n"," $ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}$\n","\n","$ a^{[1](s)} = g( z^{[1](s)} )$\n","\n"," $ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}$\n","\n","Function lin_grad2(A1, Z2, W2, b2) computes three gradients at the second layer as follows:\n","\n","(1) $\n","\\frac{\\partial L}{\\partial a^{[1](s)}}\n","=(\\frac{\\partial L} {\\partial z^{[2](s)}}\n"," \\frac{\\partial z^{[2](s)}} {\\partial a^{[1](s)}} )\n","= ( 2*(z^{[2](s)} - y^{(s)})  {W^{[2]}}^{T} )$\n","\n","It will be used to compute the gradient for the layer 1. \n","\n","(2)\n","$\\frac{\\partial L}{\\partial W^{[2]}}\n"," = \\left[    \\frac{\\partial L^{(s)}}{ \\partial w_{ij}^{[2]} } \\right] \n","= \\left[ \\frac{1}{m}\\sum_{s=1}^{m} a_{i}^{[1](s)} \\frac{\\partial L^{(s)}}{\\partial z_{j}^{[2](s)} } \\right] \\\\\n","= \\frac{1}{m}\\sum_{s=1}^{m} \\left[  a_{i}^{[1](s)} \\frac{\\partial L^{(s)}}{\\partial z_{j}^{[2](s)} } \\right]\n","= \\sum_{s=1}^{m}  a^{[1](s)^{T}} \\frac{1}{m}\\frac{\\partial L^{(s)}}{\\partial z^{[2](s)} }$\n","\n","(3)\n","$\\frac{\\partial L}{\\partial b^{[2]}}\n","  = \\frac{\\partial L}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} \\\\\n","  =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[2](s)}}\n","   \\frac{\\partial z^{[2](s)}}{\\partial b^{[2]}}\n","   =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[2](s)}}\n","   $"]},{"cell_type":"code","metadata":{"id":"omBAYDnh5-YP"},"source":["def lin_grad2(A1, Z2, W2, b2):\n","    # grad of L   with respect to A1\n","    A1.g = Z2.g @ W2.t()\n","     # grad of L   with respect to W2\n","    W2.g = (A1.unsqueeze(-1) * Z2.g.unsqueeze(1)).sum(0)\n","    # grad of L   with respect to b2\n","    b2.g = Z2.g.sum(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxmxJK7zVWJg"},"source":["\n"," $L = \\frac{1}{m}\\sum_{s=1}^{m}(z^{[2](s)} - y^{(s)}) \n"," \\cdot(z^{[2](s)} - y^{(s)}) \n","$ \\\\\n","\n"," $ z^{[2](s)}\n","=  a^{[1](s)} W^{[2]} + b^{[2]}$\n","\n","$ a^{[1](s)} = g( z^{[1](s)} )$\n","\n"," $ z^{[1](s)}\n","=  x^{(s)} W^{[1]} + b^{[1]}$\n","\n","Function lin_grad1(X, Z1, W1, b1) computes three gradients at the 1st layer as follows:\n","\n","(1) $\n","\\frac{\\partial L}{\\partial x^{(s)}}\n","=\\frac{\\partial L} {\\partial z^{[1](s)}}\n"," \\frac{\\partial z^{[1](s)}} {\\partial x^{(s)}} \n"," = ( 2*(z^{[1](s)} - y^{(s)})  {W^{[1]}}^{T} $\n","\n","\n","\n","(2)\n","$\\frac{\\partial L}{\\partial W^{[1]}}\n"," = \\left[    \\frac{\\partial L^{(s)}}{ \\partial w_{ij}^{[1]} } \\right] \n","= \\left[ \\frac{1}{m}\\sum_{s=1}^{m} a_{i}^{[1](s)} \\frac{\\partial L^{(s)}}{\\partial z_{j}^{[1](s)} } \\right] \\\\\n","= \\frac{1}{m}\\sum_{s=1}^{m} \\left[  a_{i}^{[1](s)} \\frac{\\partial L^{(s)}}{\\partial z_{j}^{[1](s)} } \\right]\n","= \\sum_{s=1}^{m}  a^{[1](s)^{T}} \\frac{1}{m}\\frac{\\partial L^{(s)}}{\\partial z^{[1](s)} }$\n","\n","(3)\n","$\\frac{\\partial L}{\\partial b^{[1]}}\n","  = \\frac{\\partial L}{\\partial Z^{[1]}} \\frac{\\partial Z^{[2]}}{\\partial b^{[1]}} \\\\\n","  =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[1](s)}}\n","   \\frac{\\partial z^{[1](s)}}{\\partial b^{[1]}}\n","   =\\frac{1}{m}\\sum_{s=1}^{m} \n","   \\frac{\\partial L^{(s)}} {\\partial z^{[1](s)}}\n","   $"]},{"cell_type":"code","metadata":{"id":"nhil8iIu_3GK"},"source":["def lin_grad1(X, Z1, W1, b1):\n","    # grad of L with respect to X\n","    X.g = Z1.g @ W1.t()\n","     # grad of L   with respect to W1\n","    W1.g = (X.unsqueeze(-1) * Z1.g.unsqueeze(1)).sum(0)\n","    # grad of L   with respect to b1\n","    b1.g = Z1.g.sum(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QubdDuqC5-YP"},"source":["def forward_and_backward(X_train, Y_train): \n","    # forward pass:\n","    Z1 = X_train @ W1 + b1 # Z1 is a matrix\n","    A1 = relu(Z1)          # A1 is a matrix\n","    Z2 = A1 @ W2 + b2      # Z2 is a matrix\n","\n"," \n","    # we don't actually need the loss in backward!\n","    loss = mse(Z2, Y_train) \n","\n","    # backward pass:\n","    # compute the gradient of the loss L with respect to Z2 and store it in Z2.g\n","    mse_grad(Z2, Y_train) \n","\n","    # compute the gradient of Z2 with respect to A1, and store it in A1.g\n","    lin_grad2(A1, Z2, W2, b2) # xf. Z2 = A1 @ W2 + b2\n","    relu_grad(Z1, A1)        # cf. A1 = relu(A1)\n","    lin_grad1(X_train, Z1, W1, b1) #cf. Z1 = X_train @ W1 + b1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEh55i9J5-YQ"},"source":["forward_and_backward(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9vfp9Gd5-YQ"},"source":["# Save for testing against later\n","W1g = W1.g.clone()\n","W2g = W2.g.clone()\n","b1g = b1.g.clone()\n","b2g = b2.g.clone()\n","ig  = X_train.g.clone()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7axjYSJd5-YQ"},"source":["We cheat a little bit and use PyTorch autograd to check our results."]},{"cell_type":"code","metadata":{"id":"rflckM445-YR"},"source":["xt2 = X_train.clone().requires_grad_(True)\n","W12 = W1.clone().requires_grad_(True)\n","W22 = W2.clone().requires_grad_(True)\n","b12 = b1.clone().requires_grad_(True)\n","b22 = b2.clone().requires_grad_(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkMyi0E75-YS"},"source":["def forward(X_train, Y_train):\n","    # forward pass:\n","    Z1 = X_train  @ W12 + b12\n","    A1 = relu(Z1)\n","    Z2 = A1 @ W22 + b22\n","    # we don't actually need the loss in backward!\n","    return mse(Z2, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sW2yU7c5-YS"},"source":["loss = forward(xt2, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdIyWWhL5-YT"},"source":["loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32fqWKXq5-YT"},"source":["test_near(W22.grad, W2g)\n","test_near(b22.grad, b2g)\n","test_near(W12.grad, W1g)\n","test_near(b12.grad, b1g)\n","test_near(xt2.grad, ig )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"60sY6g9A5-YT"},"source":["## Refactor model"]},{"cell_type":"markdown","metadata":{"id":"QqwIwiob5-YU"},"source":["### Layers as classes"]},{"cell_type":"markdown","metadata":{"id":"ujwhx_1x5-YU"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=7112)"]},{"cell_type":"code","metadata":{"id":"5MozUwDh5-YU"},"source":["class Relu():\n","    def __call__(self, inp):\n","        self.inp = inp\n","        self.out = inp.clamp_min(0.)-0.5\n","        return self.out\n","    \n","    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzMZHFNE5-YV"},"source":["class Lin():\n","    def __init__(self, w, b): self.w,self.b = w,b\n","        \n","    def __call__(self, inp):\n","        self.inp = inp\n","        self.out = inp@self.w + self.b\n","        return self.out\n","    \n","    def backward(self):\n","        self.inp.g = self.out.g @ self.w.t()\n","        # Creating a giant outer product, just to sum it, is inefficient!\n","        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n","        self.b.g = self.out.g.sum(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dxirmfwb5-YW"},"source":["class Mse():\n","    def __call__(self, inp, targ):\n","        self.inp = inp\n","        self.targ = targ\n","        self.out = (inp.squeeze() - targ).pow(2).mean()\n","        return self.out\n","    \n","    def backward(self):\n","        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1guaMoQ5-YW"},"source":["class Model():\n","    def __init__(self, w1, b1, w2, b2):\n","        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n","        self.loss = Mse()\n","        \n","    def __call__(self, x, targ):\n","        for l in self.layers: x = l(x)\n","        return self.loss(x, targ)\n","    \n","    def backward(self):\n","        self.loss.backward()\n","        for l in reversed(self.layers): l.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MorGG1Qu5-YX"},"source":["W1.g,b1.g,W2.g,b2.g = [None]*4\n","model = Model(W1, b1, W2, b2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Ol-P22k5-YX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fe3fd35-00ba-44e9-a689-20792162c43c"},"source":["%time loss = model(X_train, Y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 78.2 ms, sys: 0 ns, total: 78.2 ms\n","Wall time: 78.4 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NSbKFUYs5-YY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"348f3666-a782-4a2f-e1e2-c3ae43795d67"},"source":["%time model.backward()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2.85 s, sys: 3.19 s, total: 6.04 s\n","Wall time: 6.06 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V7BfDx_v5-YY"},"source":["test_near(W2g, W2.g)\n","test_near(b2g, b2.g)\n","test_near(W1g, W1.g)\n","test_near(b1g, b1.g)\n","test_near(ig, X_train.g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmCmOpUo5-YY"},"source":["### Module.forward()"]},{"cell_type":"code","metadata":{"id":"IcdPHsTz5-YY"},"source":["class Module():\n","    def __call__(self, *args):\n","        self.args = args\n","        self.out = self.forward(*args)\n","        return self.out\n","    \n","    def forward(self): raise Exception('not implemented')\n","    def backward(self): self.bwd(self.out, *self.args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1yFJbHU5-YZ"},"source":["class Relu(Module):\n","    def forward(self, inp): return inp.clamp_min(0.)-0.5\n","    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XcYzhPO5-Ya"},"source":["class Lin(Module):\n","    def __init__(self, w, b): self.w,self.b = w,b\n","        \n","    def forward(self, inp): return inp@self.w + self.b\n","    \n","    def bwd(self, out, inp):\n","        inp.g = out.g @ self.w.t()\n","        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n","        self.b.g = out.g.sum(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16qFhJ2q5-Ya"},"source":["class Mse(Module):\n","    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n","    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLFSVnB35-Yb"},"source":["class Model():\n","    def __init__(self):\n","        self.layers = [Lin(W1,b1), Relu(), Lin(W2,b2)]\n","        self.loss = Mse()\n","        \n","    def __call__(self, x, targ):\n","        for l in self.layers: x = l(x)\n","        return self.loss(x, targ)\n","    \n","    def backward(self):\n","        self.loss.backward()\n","        for l in reversed(self.layers): l.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ut8ItyX5-Yb"},"source":["W1.g,b1.g,W2.g,b2.g = [None]*4\n","model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm9UYns25-Yc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f38d7ba2-ffe7-4793-c7ca-d22968e6dea8"},"source":["%time loss = model(X_train, Y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 74.2 ms, sys: 4.71 ms, total: 78.9 ms\n","Wall time: 79 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cJg2fbgu5-Yd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71d16e93-160c-4b84-ffa1-4a9c376d1934"},"source":["%time model.backward()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 159 ms, sys: 8.04 ms, total: 168 ms\n","Wall time: 172 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MMVSU5vX5-Yf"},"source":["test_near(W2g, W2.g)\n","test_near(b2g, b2.g)\n","test_near(W1g, W1.g)\n","test_near(b1g, b1.g)\n","test_near(ig, X_train.g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7CoEs8B5-Yf"},"source":["### Without einsum"]},{"cell_type":"markdown","metadata":{"id":"9Jkbry0n5-Yg"},"source":["[Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=7484)"]},{"cell_type":"code","metadata":{"id":"BRMk9VkP5-Yg"},"source":["class Lin(Module):\n","    def __init__(self, w, b): self.w,self.b = w,b\n","        \n","    def forward(self, inp): return inp@self.w + self.b\n","    \n","    def bwd(self, out, inp):\n","        inp.g = out.g @ self.w.t()\n","        self.w.g = inp.t() @ out.g\n","        self.b.g = out.g.sum(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYeUhw3_5-Yg"},"source":["W1.g,b1.g,W2.g,b2.g = [None]*4\n","model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSLm1_Zp5-Yh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96050216-e497-4fb5-a23a-87bdb3905b1c"},"source":["%time loss = model(X_train, Y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 80.1 ms, sys: 0 ns, total: 80.1 ms\n","Wall time: 80.1 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b1awZuZK5-Yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e63df6de-668d-4be7-b974-0d2d49e1de2a"},"source":["%time model.backward()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 152 ms, sys: 616 µs, total: 152 ms\n","Wall time: 153 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wil2jhwW5-Yj"},"source":["test_near(W2g, W2.g)\n","test_near(b2g, b2.g)\n","test_near(W1g, W1.g)\n","test_near(b1g, b1.g)\n","test_near(ig, X_train.g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjABxzup5-Yj"},"source":["### nn.Linear and nn.Module"]},{"cell_type":"code","metadata":{"id":"7aBlWkwq5-Yj"},"source":["#export\n","from torch import nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulk-yUVr5-Yj"},"source":["class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n","        self.loss = mse\n","        \n","    def __call__(self, x, targ):\n","        for l in self.layers: x = l(x)\n","        return self.loss(x.squeeze(), targ)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sc8BGGYn5-Yk"},"source":["model = Model(n0, n1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yk8W6T6I5-Yk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48c71eec-cac9-45fb-e38f-e3f2153b2f9b"},"source":["%time loss = model(X_train, Y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 82.5 ms, sys: 1.24 ms, total: 83.8 ms\n","Wall time: 86 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EABCQagY5-Yk","outputId":"f0137715-96df-421e-e29d-f701af6fd440"},"source":["%time loss.backward()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 135 ms, sys: 78.1 ms, total: 213 ms\n","Wall time: 71.1 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GDFieAnU5-Yl"},"source":["## Export"]},{"cell_type":"code","metadata":{"id":"lghBTExoWqDK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b40e465d-50ce-4d7f-8b01-d6648065d464"},"source":["!pip install fire"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting fire\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n","\r\u001b[K     |███▊                            | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20kB 26.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40kB 24.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61kB 26.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71kB 19.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81kB 21.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire) (1.15.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=b882ad09b52e4210ba8c88603897014764c94db1e5e1c185a0d7a15fe19e2935\n","  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n","Successfully built fire\n","Installing collected packages: fire\n","Successfully installed fire-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zE5lLcnmV8UB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9137a4e-79bc-4aae-95fb-3fef2f012771"},"source":["%cd /content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/ColabNotebooks/fastaiPart2/course-v3/nbs/dl2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QN1RejcK5-Yl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d15e8c89-95d9-4610-85ba-d4a2db0c3d53"},"source":["#!./notebook2script.py 02_fully_connected.ipynb\n","!python notebook2script.py 02_fully_connected.ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Converted 02_fully_connected.ipynb to exp/nb_02.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Bdw2pYJ5-Yl"},"source":[""],"execution_count":null,"outputs":[]}]}