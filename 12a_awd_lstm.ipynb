{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"12a_awd_lstm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KjUrHImB3wdl"},"source":["# AWD-LSTM"]},{"cell_type":"code","metadata":{"id":"c9p9Fb3r3wdn"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wv8Gkto-TwSV","executionInfo":{"elapsed":228,"status":"ok","timestamp":1622530930951,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"},"user_tz":-540},"outputId":"fbc7cd6b-38c9-41f0-8660-10df7c3ecfd1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzmVwSGeVpHV","executionInfo":{"elapsed":262,"status":"ok","timestamp":1622530933326,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"},"user_tz":-540},"outputId":"504b9ca3-e9ad-4647-8874-43b0ccda7e1d"},"source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/Ml/nbs/dl2')\n","sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/gdrive/MyDrive/Ml/nbs/dl2']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmnC9zlcVD3Q","executionInfo":{"elapsed":2447,"status":"ok","timestamp":1622530938695,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"},"user_tz":-540},"outputId":"0d3ac194-2a97-4c85-930d-e8b3a0b59f6f"},"source":["!git clone https://github.com/NVIDIA/apex\n","%cd apex\n","!python setup.py install"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'apex' already exists and is not an empty directory.\n","/content/apex\n","\n","\n","torch.__version__  = 1.8.1+cu101\n","\n","\n","setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","  warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","running install\n","running bdist_egg\n","running egg_info\n","writing apex.egg-info/PKG-INFO\n","writing dependency_links to apex.egg-info/dependency_links.txt\n","writing top-level names to apex.egg-info/top_level.txt\n","adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n","writing manifest file 'apex.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/apex\n","creating build/bdist.linux-x86_64/egg/apex/contrib\n","creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n","copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n","copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n","creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n","creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n","copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n","copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n","copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n","creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n","copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n","copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n","creating build/bdist.linux-x86_64/egg/apex/contrib/fmha\n","copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n","copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n","creating build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n","copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n","copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n","copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n","creating build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n","copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n","copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n","copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n","creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n","creating build/bdist.linux-x86_64/egg/apex/contrib/transducer\n","copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n","copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n","creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n","copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n","copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n","copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n","copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n","creating build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n","creating build/bdist.linux-x86_64/egg/apex/amp/lists\n","copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n","copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n","copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n","copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n","copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n","copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n","creating build/bdist.linux-x86_64/egg/apex/mlp\n","copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n","copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n","creating build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n","creating build/bdist.linux-x86_64/egg/apex/RNN\n","copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n","copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n","copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n","copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n","creating build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n","copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n","creating build/bdist.linux-x86_64/egg/apex/normalization\n","copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n","copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n","creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n","copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n","copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n","creating build/bdist.linux-x86_64/egg/apex/pyprof\n","creating build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n","copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n","copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n","creating build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n","creating build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n","copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof\n","creating build/bdist.linux-x86_64/egg/apex/reparameterization\n","copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n","copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n","copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/fmha.py to fmha.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/test.py to test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/transducer.py to transducer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/loss.py to loss.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/dropout.py to dropout.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/softmax.py to softmax.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/usage.py to usage.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/output.py to output.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/conv.py to conv.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__main__.py to __main__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/base.py to base.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/utility.py to utility.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/embedding.py to embedding.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/prof.py to prof.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pointwise.py to pointwise.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/reduction.py to reduction.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/activation.py to activation.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pooling.py to pooling.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/normalization.py to normalization.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/misc.py to misc.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/linear.py to linear.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/randomSample.py to randomSample.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/data.py to data.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/convert.py to convert.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/blas.py to blas.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/optim.py to optim.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/db.py to db.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/kernel.py to kernel.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/nvvp.py to nvvp.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__main__.py to __main__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/parse.py to parse.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/reparameterization.py to reparameterization.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/weight_norm.py to weight_norm.cpython-37.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","apex.pyprof.nvtx.__pycache__.nvmarker.cpython-37: module references __file__\n","apex.pyprof.nvtx.__pycache__.nvmarker.cpython-37: module references __path__\n","creating 'dist/apex-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing apex-0.1-py3.7.egg\n","removing '/usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg' (and everything under it)\n","creating /usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg\n","Extracting apex-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n","apex 0.1 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg\n","Processing dependencies for apex==0.1\n","Finished processing dependencies for apex==0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bnpj8IXyUHEk"},"source":["# %cd ../\n","# %cd gdrive/MyDrive/Ml/nbs/dl2/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3qtIEI23wdo"},"source":["#export\n","\n","import sys\n","\n","# /content/gdrive/MyDrive/Ml/nbs/dl2\n","# %cd gdrive/MyDrive/Ml/nbs/dl2/\n","from exp.nb_12 import *\n","\n","\n","\n","# rnn 은 doc길이만큼 layer늘리기는 그레디언트 문제 때문에 불가능\n","# 따라서 여러개의 stacked layer로 사용"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uul8s1Qc3wdo"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"L5gAdRxt3wdo"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6317)"]},{"cell_type":"code","metadata":{"id":"Wsshdpys3wdo"},"source":["path = datasets.untar_data(datasets.URLs.IMDB)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rod7t9sH3wdp"},"source":["We have to preprocess the data again to pickle it because if we try to load the previous `SplitLabeledData` with pickle, it will complain some of the functions aren't in main."]},{"cell_type":"code","metadata":{"id":"6QjsXj6h3wdp"},"source":["il = TextList.from_files(path, include=['train', 'test', 'unsup'])\n","sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hY8SR4Jf3wdp"},"source":["proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":37},"id":"ReHhOd913wdq","outputId":"29f78476-96b8-4266-d7e3-8a709b78e8bd"},"source":["ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='46' class='' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [46/46 28:56<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [5/5 03:12<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"YVULlFvg3wdq"},"source":["pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n","pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AA4VcvW3wdq"},"source":["ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n","vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhtYUen13wdq"},"source":["bs,bptt = 64,70\n","data = lm_databunchify(ll, bs, bptt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdioyTZ43wdq"},"source":["## AWD-LSTM"]},{"cell_type":"markdown","metadata":{"id":"K0Nx4ITh3wdr"},"source":["Before explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on [this website](http://joshvarty.github.io/VisualizingRNNs/)."]},{"cell_type":"markdown","metadata":{"id":"Qz1gmH1L3wdr"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6330)"]},{"cell_type":"markdown","metadata":{"id":"S7-lljTY3wdr"},"source":["### LSTM from scratch"]},{"cell_type":"markdown","metadata":{"id":"xkJ1YczM3wdr"},"source":["We need to implement those equations (where $\\sigma$ stands for sigmoid):\n","\n","![LSTM cell and equations](images/lstm.jpg)\n","(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)\n","\n","If we want to take advantage of our GPU, it's better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use `nn.Linear` to do it.\n","\n","We need two linear layers: one for the input and one for the hidden state."]},{"cell_type":"code","metadata":{"id":"rIRPOlJ13wdr"},"source":["class LSTMCell(nn.Module):\n","    def __init__(self, ni, nh):\n","        super().__init__()\n","        self.ih = nn.Linear(ni,4*nh)\n","        self.hh = nn.Linear(nh,4*nh)\n","\n","    def forward(self, input, state):\n","        h,c = state\n","        #One big multiplication for all the gates is better than 4 smaller ones\n","        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) # input과 hidden을 더하고 4개로 나눈다.\n","        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) \n","        cellgate = gates[3].tanh()\n","\n","        c = (forgetgate*c) + (ingate*cellgate)\n","        h = outgate * c.tanh()\n","        return h, (h,c)\n","\n","# it is the way of forgetting some unnecessary info"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxJ98emh3wds"},"source":["Then an LSTM layer just applies the cell on all the time steps in order."]},{"cell_type":"code","metadata":{"id":"NyI7E27_3wds"},"source":["class LSTMLayer(nn.Module):\n","    def __init__(self, cell, *cell_args):\n","        super().__init__()\n","        self.cell = cell(*cell_args)\n","\n","    def forward(self, input, state):\n","        inputs = input.unbind(1) # dim1차원 중심으로 해제\n","        outputs = []\n","        for i in range(len(inputs)):\n","            out, state = self.cell(inputs[i], state)\n","            outputs += [out]\n","        return torch.stack(outputs, dim=1), state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYOqaE1G3wds"},"source":["Now let's try it out and see how fast we are. We only measure the forward pass."]},{"cell_type":"code","metadata":{"id":"ZPwG7KdT3wds"},"source":["lstm = LSTMLayer(LSTMCell, 300, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vItXc6x3wds"},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(64, 300),torch.zeros(64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0ec-4jb3wds"},"source":["CPU"]},{"cell_type":"code","metadata":{"id":"E6aLBAXP3wds"},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fxWsz323wdt"},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQ1U2fEs3wdt"},"source":["def time_fn(f):\n","    f()\n","    torch.cuda.synchronize() # dont keep going in my Python word until my cuda word has finished -> execute faster"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFPiFziL3wdt"},"source":["CUDA"]},{"cell_type":"code","metadata":{"id":"wSKU9aut3wdt"},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Bs4duIe3wdt"},"source":["%timeit -n 10 time_fn(f) \n","# 막상 해보면 큰차이가 안남. 왜냐하면 gpu와 cpu를 왔다갔다 해야하기 때문임\n","# 그래서 torch built-in version을 이용해서 c++로 자동변환해서 돌리도록 하면, 속도 무지 빠름"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqsbJUnU3wdt"},"source":["### Builtin version"]},{"cell_type":"markdown","metadata":{"id":"j0iVlm7L3wdt"},"source":["Let's compare with PyTorch!"]},{"cell_type":"markdown","metadata":{"id":"nWPEVkVY3wdt"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6718)"]},{"cell_type":"code","metadata":{"id":"c1aCz0ic3wdv"},"source":["lstm = nn.LSTM(300, 300, 1, batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDkUTC843wdv"},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VuL3AbF3wdv"},"source":["CPU"]},{"cell_type":"code","metadata":{"id":"_EJEbYJh3wdw"},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNcDlPHT3wdw"},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBwe3sUz3wdw"},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5yVkIZPl3wdw"},"source":["GPU"]},{"cell_type":"code","metadata":{"id":"V1flBYng3wdw"},"source":["%timeit -n 10 time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jbXN6g_f3wdw"},"source":["So our version is running at almost the same speed on the CPU. However, on the GPU, PyTorch uses CuDNN behind the scenes that optimizes greatly the for loop."]},{"cell_type":"markdown","metadata":{"id":"s2yRw2Gt3wdw"},"source":["### Jit version"]},{"cell_type":"markdown","metadata":{"id":"jkukHTh53wdw"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6744)"]},{"cell_type":"code","metadata":{"id":"UnbU3pJ03wdx"},"source":["import torch.jit as jit # 코드를 C++로 바꾸는 라이브러리\n","from torch import Tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KsyGdFu93wdx"},"source":["We have to write everything from scratch to be a bit faster, so we don't use the linear layers here."]},{"cell_type":"code","metadata":{"id":"4CWWQ_603wdx"},"source":["class LSTMCell(jit.ScriptModule):\n","    def __init__(self, ni, nh):\n","        super().__init__()\n","        self.ni = ni\n","        self.nh = nh\n","        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n","        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n","        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n","        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n","\n","# 하지만 python 스러운 것을 넣어버리면, c++가 인식을 못한다.\n","    @jit.script_method\n","    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n","        hx, cx = state\n","        gates = (input @ self.w_ih.t() + self.bias_ih +\n","                 hx @ self.w_hh.t() + self.bias_hh)\n","        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n","\n","        ingate = torch.sigmoid(ingate)\n","        forgetgate = torch.sigmoid(forgetgate)\n","        cellgate = torch.tanh(cellgate)\n","        outgate = torch.sigmoid(outgate)\n","\n","        cy = (forgetgate * cx) + (ingate * cellgate)\n","        hy = outgate * torch.tanh(cy)\n","\n","        return hy, (hy, cy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4IrDoUY3wdx"},"source":["class LSTMLayer(jit.ScriptModule):\n","    def __init__(self, cell, *cell_args):\n","        super().__init__()\n","        self.cell = cell(*cell_args)\n","\n","    @jit.script_method\n","    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n","        inputs = input.unbind(1)\n","        outputs = []\n","        for i in range(len(inputs)):\n","            out, state = self.cell(inputs[i], state)\n","            outputs += [out]\n","        return torch.stack(outputs, dim=1), state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9F8SbKau3wdx"},"source":["lstm = LSTMLayer(LSTMCell, 300, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usAG3OJo3wdx"},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(64, 300),torch.zeros(64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-t8-rVDz3wdy"},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuxtznyp3wdy"},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMIwmz2D3wdy"},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwy9lIfj3wdy"},"source":["%timeit -n 10 time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YqAy2dgL3wdy"},"source":["With jit, we almost get to the CuDNN speed!"]},{"cell_type":"markdown","metadata":{"id":"SrvX2Fq03wdy"},"source":["### Dropout"]},{"cell_type":"markdown","metadata":{"id":"Cc2DTQ_33wdy"},"source":["We want to use the AWD-LSTM from [Stephen Merity et al.](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)` (think of what happens to the activations if you want to figure out why!)\n","\n","We usually apply dropout by drawing a mask that tells us which elements to nullify or not:"]},{"cell_type":"markdown","metadata":{"id":"1OYA4miF3wdz"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7003)"]},{"cell_type":"code","metadata":{"id":"JYcznqCH3wdz"},"source":["#export\n","def dropout_mask(x, sz, p):\n","    return x.new(*sz).bernoulli_(1-p).div_(1-p) # 똑같은 타입의 새로운 tensor만들어서 bernouli로 마스킹\n","# dropout을 할 때 특정 번째 sequence를 제거하려면,  sequence에서 그 뒤도 전부 제거해야함\n","# 왜냐하면 sequence를 부신다는 것은 그 뒤의 것은 의미가 없다는 뜻이기 때문"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlA9b_oWmOwr","executionInfo":{"status":"ok","timestamp":1630336122356,"user_tz":-540,"elapsed":291,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"7f96792f-9255-4f8a-c657-0d34fc26b8f3"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2., 2., 2., 2., 2., 2., 0., 2., 2., 0., 0., 0., 2., 0., 2., 2., 0., 2.,\n","         0., 0.],\n","        [2., 2., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 2., 2., 0., 2.,\n","         2., 0.],\n","        [0., 0., 2., 0., 0., 2., 0., 2., 0., 2., 2., 2., 2., 0., 0., 0., 0., 0.,\n","         0., 0.],\n","        [0., 0., 2., 0., 0., 0., 2., 2., 2., 2., 0., 2., 0., 2., 2., 0., 2., 0.,\n","         2., 2.],\n","        [0., 0., 2., 0., 0., 2., 0., 0., 2., 2., 0., 2., 0., 0., 2., 2., 0., 0.,\n","         0., 2.],\n","        [2., 0., 2., 0., 0., 2., 0., 2., 2., 0., 2., 0., 0., 0., 2., 2., 2., 0.,\n","         2., 2.],\n","        [2., 0., 0., 2., 0., 0., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 2., 0.,\n","         2., 2.],\n","        [2., 0., 2., 0., 2., 2., 0., 2., 0., 0., 0., 0., 0., 2., 2., 0., 0., 2.,\n","         0., 0.],\n","        [0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 2., 2., 0., 2., 2.,\n","         0., 2.],\n","        [0., 0., 0., 2., 2., 0., 2., 0., 0., 2., 0., 2., 2., 2., 2., 2., 2., 0.,\n","         0., 0.]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"CjwFr2mp3wdz"},"source":["x = torch.randn(10,10)\n","mask = dropout_mask(x, (10,10), 0.5); mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M94dYPaA3wdz"},"source":["Once with have a dropout mask `mask`, applying the dropout to `x` is simply done by `x = x * mask`. We create our own dropout mask and don't rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension."]},{"cell_type":"code","metadata":{"id":"C6K1kuV63wdz"},"source":["(x*mask).std(),x.std()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wyY_T3Ha3wdz"},"source":["Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size.  Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension."]},{"cell_type":"code","metadata":{"id":"qOSGq0yU3wdz"},"source":["#export\n","class RNNDropout(nn.Module):\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        self.p=p\n","\n","    def forward(self, x):\n","        if not self.training or self.p == 0.: return x\n","        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n","        return x * m\n","        # sequence 중 한 개를 dropout하면 연달아 hidden이 broken되므로, 필요없음. 따라서 sequence의 똑같은 위치에 있는 모든 것을 dropout하도록\n","        # 브로드 캐스팅하기 위해 1을 추가"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3uR0s9N3wd0"},"source":["dp = RNNDropout(0.3)\n","tst_input = torch.randn(3,3,7)\n","tst_input, dp(tst_input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COwfZkOB3wd0"},"source":["WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass."]},{"cell_type":"code","metadata":{"id":"OCZ6ib5r3wd0"},"source":["#export\n","import warnings\n","\n","WEIGHT_HH = 'weight_hh_l0'\n","# dropout the weight and activation at the same time\n","class WeightDropout(nn.Module):\n","    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n","        super().__init__()\n","        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n","        for layer in self.layer_names:\n","            #Makes a copy of the weights of the selected layers.\n","            w = getattr(self.module, layer)\n","            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n","            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n","\n","    def _setweights(self):\n","        for layer in self.layer_names:\n","            raw_w = getattr(self, f'{layer}_raw')\n","            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n","    # apply drop out to actual weight\n","    def forward(self, *args):\n","        self._setweights()\n","        with warnings.catch_warnings():\n","            #To avoid the warning that comes because the weights aren't flattened.\n","            warnings.simplefilter(\"ignore\")\n","            return self.module.forward(*args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2PPnA4G3wd1"},"source":["Let's try it!"]},{"cell_type":"code","metadata":{"id":"J6OU1cPL3wd1"},"source":["module = nn.LSTM(5, 2)\n","dp_module = WeightDropout(module, 0.4)\n","getattr(dp_module.module, WEIGHT_HH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jyu4FfG93wd1"},"source":["It's at the beginning of a forward pass that the dropout is applied to the weights."]},{"cell_type":"code","metadata":{"id":"MS4Qq91t3wd1"},"source":["tst_input = torch.randn(4,20,5)\n","h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n","x,h = dp_module(tst_input,h)\n","getattr(dp_module.module, WEIGHT_HH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOEofEtJ3wd1"},"source":["EmbeddingDropout applies dropout to full rows of the embedding matrix."]},{"cell_type":"code","metadata":{"id":"x5kf5z4G3wd1"},"source":["#export\n","# drop entire embedding vector if one of them drop out.\n","class EmbeddingDropout(nn.Module):\n","    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n","    def __init__(self, emb, embed_p):\n","        super().__init__()\n","        self.emb,self.embed_p = emb,embed_p\n","        self.pad_idx = self.emb.padding_idx\n","        if self.pad_idx is None: self.pad_idx = -1\n","\n","    def forward(self, words, scale=None):\n","        if self.training and self.embed_p != 0:\n","            size = (self.emb.weight.size(0),1)\n","            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n","            masked_embed = self.emb.weight * mask\n","        else: masked_embed = self.emb.weight\n","        if scale: masked_embed.mul_(scale)\n","        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n","                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-OaO0ri3wd1"},"source":["enc = nn.Embedding(100, 7, padding_idx=1)\n","enc_dp = EmbeddingDropout(enc, 0.5)\n","tst_input = torch.randint(0,100,(8,))\n","enc_dp(tst_input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzBOtHU53wd2"},"source":["### Main model"]},{"cell_type":"markdown","metadata":{"id":"LZ3VsF0p3wd2"},"source":["The main model is a regular LSTM with several layers, but using all those kinds of dropouts."]},{"cell_type":"code","metadata":{"id":"pemJcTyB3wd2"},"source":["#export\n","def to_detach(h):\n","    \"Detaches `h` from its history.\"\n","    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71Y5fbHM3wd2"},"source":["#export\n","class AWD_LSTM(nn.Module):\n","    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n","    initrange=0.1\n","\n","    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n","                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n","        super().__init__()\n","        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n","        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n","        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n","        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n","                             batch_first=True) for l in range(n_layers)]\n","        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n","        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n","        self.input_dp = RNNDropout(input_p)\n","        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n","\n","    def forward(self, input):\n","        bs,sl = input.size()\n","        if bs!=self.bs:\n","            self.bs=bs\n","            self.reset()\n","        raw_output = self.input_dp(self.emb_dp(input))\n","        new_hidden,raw_outputs,outputs = [],[],[]\n","        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n","            raw_output, new_h = rnn(raw_output, self.hidden[l])\n","            new_hidden.append(new_h)\n","            raw_outputs.append(raw_output)\n","            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n","            outputs.append(raw_output) \n","        self.hidden = to_detach(new_hidden)\n","        return raw_outputs, outputs\n","\n","    def _one_hidden(self, l):\n","        \"Return one hidden state.\"\n","        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n","        return next(self.parameters()).new(1, self.bs, nh).zero_()\n","\n","    def reset(self):\n","        \"Reset the hidden states.\"\n","        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rqbCgeA3wd2"},"source":["On top of this, we will apply a linear decoder. It's often best to use the same matrix as the one for the embeddings in the weights of the decoder."]},{"cell_type":"code","metadata":{"id":"QoEEjFOJ3wd3"},"source":["#export\n","class LinearDecoder(nn.Module):\n","    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n","        super().__init__()\n","        self.output_dp = RNNDropout(output_p)\n","        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n","        if bias: self.decoder.bias.data.zero_()\n","        if tie_encoder: self.decoder.weight = tie_encoder.weight\n","        else: init.kaiming_uniform_(self.decoder.weight)\n","\n","    def forward(self, input):\n","        raw_outputs, outputs = input\n","        output = self.output_dp(outputs[-1]).contiguous()\n","        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n","        return decoded, raw_outputs, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfXhJWBt3wd3"},"source":["#export\n","class SequentialRNN(nn.Sequential):\n","    \"A sequential module that passes the reset call to its children.\"\n","    def reset(self):\n","        for c in self.children():\n","            if hasattr(c, 'reset'): c.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hjs1f3103wd3"},"source":["And now we stack all of this together!"]},{"cell_type":"code","metadata":{"id":"Ozdn03YY3wd3"},"source":["#export\n","def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n","                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n","    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n","                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n","    enc = rnn_enc.emb if tie_weights else None\n","    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1kHBhjz3wd3"},"source":["tok_pad = vocab.index(PAD)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7v0G8LA3wd3"},"source":["Now we can test this all works without throwing a bug."]},{"cell_type":"code","metadata":{"id":"LTxz3-QX3wd3"},"source":["tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)\n","tst_model = tst_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOgRIsrN3wd3"},"source":["x,y = next(iter(data.train_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfbjsxAk3wd4"},"source":["z = tst_model(x.cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMeQxunS3wd4"},"source":["We return three things to help with regularization: the true output (probabilities for each word), but also the activations of the encoder, with or without dropouts."]},{"cell_type":"code","metadata":{"id":"7UadZEfm3wd4"},"source":["len(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u1IFFna83wd4"},"source":["decoded, raw_outputs, outputs = z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVlH_i0H3wd4"},"source":["The decoded tensor is flattened to `bs * seq_len` by `len(vocab)`:"]},{"cell_type":"code","metadata":{"id":"eiKa7mhZ3wd4"},"source":["decoded.size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phyY9vwO3wd4"},"source":["`raw_outputs` and `outputs` each contain the results of the intermediary layers:"]},{"cell_type":"code","metadata":{"id":"P15g0zj83wd4"},"source":["len(raw_outputs),len(outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6GQFo1O3wd5"},"source":["[o.size() for o in raw_outputs], [o.size() for o in outputs]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNeDeCQ13wd5"},"source":["### Callbacks to train the model"]},{"cell_type":"markdown","metadata":{"id":"Fh1gf7YR3wd5"},"source":["We need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a higher learning rate by putting a maximum value on the norm of the gradients."]},{"cell_type":"code","metadata":{"id":"h5ltjJPp3wd5"},"source":["#export\n","## gradient가 일정치를 넘으면 특정값으로 고정시켜라\n","class GradientClipping(Callback):\n","    def __init__(self, clip=None): self.clip = clip\n","    def after_backward(self):\n","        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEwu1H3k3wd5"},"source":["Then we add an `RNNTrainer` that will do four things:\n","- change the output to make it contain only the `decoded` tensor (for the loss function) and store the `raw_outputs` and `outputs`\n","- apply Activation Regularization (AR): we add to the loss an L2 penalty on the last activations of the AWD LSTM (with dropout applied)\n","- apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on the difference between two consecutive (in terms of words) raw outputs\n","- trigger the shuffle of the LMDataset at the beginning of each epoch"]},{"cell_type":"code","metadata":{"id":"yHotI-xQ3wd5"},"source":["#export\n","class RNNTrainer(Callback):\n","    def __init__(self, α, β): self.α,self.β = α,β\n","    \n","    def after_pred(self):\n","        #Save the extra outputs for later and only returns the true output.\n","        self.raw_out,self.out = self.pred[1],self.pred[2]\n","        self.run.pred = self.pred[0]\n","    \n","    def after_loss(self):\n","        #AR and TAR\n","        if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean() #l2 penaly on activation -> regularization\n","        if self.β != 0.:\n","            h = self.raw_out[-1]\n","            if h.size(1)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean() # trace \"the activation change\" through time step to time step -> try not to change massively during time step to time step\n","                \n","    def begin_epoch(self):\n","        #Shuffle the texts at the beginning of the epoch\n","        if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1WZ7unf3wd5"},"source":["Lastly we write a flattened version of the cross entropy loss and the accuracy metric."]},{"cell_type":"code","metadata":{"id":"ugq1sKGd3wd5"},"source":["#export\n","def cross_entropy_flat(input, target):\n","    bs,sl = target.size()\n","    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n","\n","def accuracy_flat(input, target):\n","    bs,sl = target.size()\n","    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhocoIMn3wd6"},"source":["emb_sz, nh, nl = 300, 300, 2\n","model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, \n","                           embed_p=0.1, hidden_p=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWN_F9A73wd6"},"source":["cbs = [partial(AvgStatsCallback,accuracy_flat),\n","       CudaCallback, Recorder,\n","       partial(GradientClipping, clip=0.1),\n","       partial(RNNTrainer, α=2., β=1.),\n","       ProgressCallback]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozWBObeG3wd6"},"source":["learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4-4Cl3U3wd6"},"source":["learn.fit(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2FXI9pU-iM7"},"source":["# 12b는 큰모델 학습 과정\n","# 12c는 작은 모델 학습 과정\n","    # -> 12c는 imdb와 wiki103에서 겹치는 단어들 처리해 줌"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qGMvI5d3wd6"},"source":["## Export"]},{"cell_type":"code","metadata":{"id":"uFNzHBi33wd6","outputId":"c8cd37af-b436-43ea-bee1-1357b1944933"},"source":["!python notebook2script.py 12a_awd_lstm.ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Converted 12a_awd_lstm.ipynb to exp/nb_12a.py\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uQWBeUBF3wd7"},"source":[""],"execution_count":null,"outputs":[]}]}