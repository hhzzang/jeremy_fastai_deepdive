{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"20140269midterm.ipynb","provenance":[],"collapsed_sections":["DfNuX2v1xp8B","PJzROcD-xp8r","diwEDyzJxp8v","yk8EQfB6xp8y","1o20CTI0xp82","1PWzGE8Rxp9A","nz8ts0aZxp9A","0fgmSsJdxp9D","-9uDi1fcxp9H","BkOcYVz5xp9U","IZdHSkBbxp9X","I-fmPWSBxp9b"],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1f93fc4090e24535aabcfcf811edaf33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_01962601ff474f048ca49f04bd83fcc5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_da28b50e036b499d979098a6a96479e5","IPY_MODEL_9bb9c431587f43049acd4477336c28b6"]}},"01962601ff474f048ca49f04bd83fcc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da28b50e036b499d979098a6a96479e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e92b775c679a44f28720225e0e541c7c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7067b8ceb2db4faaa56a38aae42bfbf0"}},"9bb9c431587f43049acd4477336c28b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5e616380c61546619e09ace36b13afa9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [05:29&lt;00:00, 30103.38it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ef95eb021bf4d159ac0c0a38fcc741d"}},"e92b775c679a44f28720225e0e541c7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7067b8ceb2db4faaa56a38aae42bfbf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e616380c61546619e09ace36b13afa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3ef95eb021bf4d159ac0c0a38fcc741d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ffc50301974f488982e45880fabb0cf3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9445eaaff7244e3ca7d49096eb130aa6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e73bdc13f9964680900fba0022984e18","IPY_MODEL_ed1f35274e5a4ef89f6778964a364f0b"]}},"9445eaaff7244e3ca7d49096eb130aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e73bdc13f9964680900fba0022984e18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af1eccf3bbf44556af610afe9a794764","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31ec0925b71f4fba89ae325b7b1618a0"}},"ed1f35274e5a4ef89f6778964a364f0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b940c1c95f224bf7b43becd5bb6eaff7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 238898.08it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6116562d097c4d759f3d9be9227d3023"}},"af1eccf3bbf44556af610afe9a794764":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"31ec0925b71f4fba89ae325b7b1618a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b940c1c95f224bf7b43becd5bb6eaff7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6116562d097c4d759f3d9be9227d3023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91412f3eecc342dc976f72945e8a9e26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fb43606d83cb4c4f80fc123af952e829","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_70c7febedf4a48179b0c240063c7c511","IPY_MODEL_e924ed657e554e1daeb903c6e080b106"]}},"fb43606d83cb4c4f80fc123af952e829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70c7febedf4a48179b0c240063c7c511":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9f6e41504fea45fdacf8f516f1a60edf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea56384379714951bb7572c8c146fd6c"}},"e924ed657e554e1daeb903c6e080b106":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7bb5d40793d14cd586ac559cd9b0d33b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:01&lt;00:00, 1067365.79it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78d3fc6f57be4cef9134c1c37411dd78"}},"9f6e41504fea45fdacf8f516f1a60edf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ea56384379714951bb7572c8c146fd6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7bb5d40793d14cd586ac559cd9b0d33b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"78d3fc6f57be4cef9134c1c37411dd78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99179c3c4c244e9cb78af5f7c312ee9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a836a5c9b12c42bda03183cf61d7a327","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bca5635e06384f01bab4260b7f24b167","IPY_MODEL_e191e92ce85743da9fd763031a6247d1"]}},"a836a5c9b12c42bda03183cf61d7a327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bca5635e06384f01bab4260b7f24b167":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f7d0e11d8194fe181cfb1aa2a4534a9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_264e0f3b207a41a79d2606c359760eb0"}},"e191e92ce85743da9fd763031a6247d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4eb260228813460daf4e052879eabea4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:01&lt;00:00, 3901.59it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ca430866570421cb08381e32154ca31"}},"0f7d0e11d8194fe181cfb1aa2a4534a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"264e0f3b207a41a79d2606c359760eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4eb260228813460daf4e052879eabea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3ca430866570421cb08381e32154ca31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"GJ-wL42Lxp7g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091723726,"user_tz":-540,"elapsed":557,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e1163cab-d9b9-4f5c-8e9c-1f14d2425652"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AO3i1A7iL--k"},"source":["\n","#from google.colab import drive\n","#drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdH_c8nnqn-c","executionInfo":{"status":"ok","timestamp":1619071637500,"user_tz":-540,"elapsed":583,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"d1cc45d2-b3e5-4c32-f0d2-207d19d08b49"},"source":["#Q1. Execute the following statement. What is displayed? What does it mean?\n","!pwd\n","# we can get the directory where we are working on.\n","# if run the code, we will get /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jhAroxykqtRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091825165,"user_tz":-540,"elapsed":842,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"31f188ab-5d13-48bc-82bb-56a2da93ed69"},"source":["#Q2: Execute the following statement. What happens?  Examine the left column of your colab page to see what happens.\n","!git clone https://github.com/fastai/course-v3.git\n","# we can get the whole files at github with url. Then new folder that is course-v3 appeared in our directory \n","# It is child directory of our current directory which could be known by !pwd."],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'course-v3' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I86djNvcxp70","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1619091782698,"user_tz":-540,"elapsed":771,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c5505df3-c89c-427c-8c27-b9aea645c209"},"source":["#export\n","#Q3a: Execute the following statement. Read the error message. Explain what it means. \n","\n","# if we want to use the packages or python file, it should be in the sys.path directory or current directory\n","# but in those area, there is no exp because we don't register the the path where exp really exists\n","# That's why error comes up."],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-9d6058d113ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Q3a: Execute the following statement. Read the error message. Explain what it means.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_02\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# if we want to use the packages or python file, it should be in the sys.path directory or current directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# but in those area, there is no exp because we don't register the the path where exp really exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'exp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"KsprL6o5IRIe"},"source":["#Q3b. You can solve the problem by executing the following statement before the above statement. \n","# Explain why the following statements can solve this \"module not found\" problem. \n","# ----\n","#\n","# sys.path shows us paths where our python package exists and then we can add a the specific path where exp exists by sys.append\n","# Because we add the path where exp exists, python system can now find the exp in added path directory. \n","# That's why following statements solve the module not found."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0viyrQOz8TVm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091844409,"user_tz":-540,"elapsed":575,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"9fa5560d-757c-4cd0-9150-5b410ff4ad7d"},"source":["import sys\n","sys.path.append('/content/course-v3/nbs/dl2/')\n","sys.path"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/course-v3/nbs/dl2',\n"," '/content/course-v3/nbs/dl2',\n"," '/content/course-v3/nbs/dl2/']"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"jDlj09yaftUr"},"source":["import torch.nn.functional as F\n","from exp.nb_02 import *\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iywbFmpbxp73"},"source":["## Initial setup"]},{"cell_type":"markdown","metadata":{"id":"G5DBpkZWxp75"},"source":["### Data"]},{"cell_type":"markdown","metadata":{"id":"Q_7vJLX_xp75"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=1786)"]},{"cell_type":"code","metadata":{"id":"xg8JmuuDLwmt"},"source":["from IPython.display import Image\n","from six.moves import urllib\n","\n","opener = urllib.request.build_opener()\n","opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n","urllib.request.install_opener(opener)\n","\n","def get_data():\n","    import os\n","    import torchvision.datasets as datasets\n","    root = '../data'\n","    if not os.path.exists(root):\n","        os.mkdir(root)\n","\n","    train_set = datasets.MNIST(root=root, train=True, download=True)\n","    test_set = datasets.MNIST(root=root, train=False, download=True) #load validation set\n","    x_train, x_valid = train_set.data.split([50000, 10000])\n","    y_train, y_valid = train_set.targets.split([50000, 10000])\n","    return (x_train.view(50000, -1) / 256.0), y_train.float(), (x_valid.view(10000, -1))/ 256.0, y_valid.float()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRKV-q7Kxp76"},"source":["#mpl.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6um-ZEMxp76","colab":{"base_uri":"https://localhost:8080/","height":945,"referenced_widgets":["1f93fc4090e24535aabcfcf811edaf33","01962601ff474f048ca49f04bd83fcc5","da28b50e036b499d979098a6a96479e5","9bb9c431587f43049acd4477336c28b6","e92b775c679a44f28720225e0e541c7c","7067b8ceb2db4faaa56a38aae42bfbf0","5e616380c61546619e09ace36b13afa9","3ef95eb021bf4d159ac0c0a38fcc741d","ffc50301974f488982e45880fabb0cf3","9445eaaff7244e3ca7d49096eb130aa6","e73bdc13f9964680900fba0022984e18","ed1f35274e5a4ef89f6778964a364f0b","af1eccf3bbf44556af610afe9a794764","31ec0925b71f4fba89ae325b7b1618a0","b940c1c95f224bf7b43becd5bb6eaff7","6116562d097c4d759f3d9be9227d3023","91412f3eecc342dc976f72945e8a9e26","fb43606d83cb4c4f80fc123af952e829","70c7febedf4a48179b0c240063c7c511","e924ed657e554e1daeb903c6e080b106","9f6e41504fea45fdacf8f516f1a60edf","ea56384379714951bb7572c8c146fd6c","7bb5d40793d14cd586ac559cd9b0d33b","78d3fc6f57be4cef9134c1c37411dd78","99179c3c4c244e9cb78af5f7c312ee9c","a836a5c9b12c42bda03183cf61d7a327","bca5635e06384f01bab4260b7f24b167","e191e92ce85743da9fd763031a6247d1","0f7d0e11d8194fe181cfb1aa2a4534a9","264e0f3b207a41a79d2606c359760eb0","4eb260228813460daf4e052879eabea4","3ca430866570421cb08381e32154ca31"]},"executionInfo":{"status":"ok","timestamp":1619091058092,"user_tz":-540,"elapsed":3323,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"9fa12067-75e4-4bef-f878-afaf6643b858"},"source":["#Q4: when you execute the following statement, where is the downloaded data stored? Examine the left column of your colab page.\n","x_train,y_train,x_valid,y_valid = get_data()\n","##  if we download, they are stored in the path which we specified \"../data\"\n","## .. means parent-directory. we know that we are in /content now by using !pwd. \n","# if we use '..', python go up to parent directory which is '/' then make the data directory and finally save the datas in that directory\n","## so our data is saved in /data directory."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f93fc4090e24535aabcfcf811edaf33","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffc50301974f488982e45880fabb0cf3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91412f3eecc342dc976f72945e8a9e26","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99179c3c4c244e9cb78af5f7c312ee9c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oHcenWs_rZ_I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091058093,"user_tz":-540,"elapsed":1408,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"69fa0bd6-a5e0-47d6-e49b-ea24f20b562d"},"source":["#Q5a: Execute the following statement. What does the number displayed mean?\n","len(x_train)\n","# we can get the number of data samples which is first shape of x_train\n","# we have 50000 images"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"mxSwa7g5rcer","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091059363,"user_tz":-540,"elapsed":1703,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"21f8032a-5070-46cb-ab51-95cd1ec715cc"},"source":["#Q5b: Execute the following statement. What do the numbers displayed refer to?\n","x_train[0]\n","# those numbers refer to the pixel color brightness in gray scale.\n","# the image consists of 784 pixels,that is 28*28, and each pixel have color information."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0117,\n","        0.0703, 0.0703, 0.0703, 0.4922, 0.5312, 0.6836, 0.1016, 0.6484, 0.9961,\n","        0.9648, 0.4961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.1406, 0.3672, 0.6016,\n","        0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.8789, 0.6719, 0.9883,\n","        0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883,\n","        0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805, 0.3633, 0.3203,\n","        0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883,\n","        0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n","        0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.9414,\n","        0.8789, 0.6250, 0.4219, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.3164, 0.9375, 0.9883, 0.9883, 0.4648, 0.0977, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883, 0.5859, 0.1055, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844, 0.9883, 0.7305,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727, 0.9883,\n","        0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n","        0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883,\n","        0.9883, 0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883,\n","        0.9883, 0.7852, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0898, 0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883,\n","        0.7734, 0.3164, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0703, 0.6680, 0.8555, 0.9883, 0.9883, 0.9883, 0.9883, 0.7617,\n","        0.3125, 0.0352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.2148, 0.6719, 0.8828, 0.9883, 0.9883, 0.9883, 0.9883, 0.9531, 0.5195,\n","        0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281, 0.5273, 0.5156, 0.0625,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTrGABBgiPvi","executionInfo":{"status":"ok","timestamp":1619091063619,"user_tz":-540,"elapsed":1225,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"8c8063d7-c49e-4633-f623-344f84e60c9a"},"source":["#Q5c: Execute the following statement. What do the numbers displayed refer to?\n","y_train[0]\n","# we can get the first image's label which is the answer for the image.\n","# To know why it is 5, mnists dataset's answers consist of 0~9, digit number. And the first image represents 5.\n","# thats why it is 5."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzZC2MDkhtdJ","executionInfo":{"status":"ok","timestamp":1619091063838,"user_tz":-540,"elapsed":587,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"7b1ff4af-d8a2-4f34-e3fa-5bfe3468f45f"},"source":["#Q5d. Execute the following statement. What does the number displayed refer to? \n","x_train[0].shape\n","# it means, the number of features(or elements) of each image.\n","# for one image, there are 784 elements."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([784])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"pA-uJSgpD1Z0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091064416,"user_tz":-540,"elapsed":464,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"aaf59727-3b93-4653-99f4-a3775ecea0e5"},"source":["#Q5e. Execute the following statement. What do the numbers displayed refer to?\n","x_train.shape\n","# we have 50000 images and 784 pixel values for each image."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000, 784])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkNGxyrOibgh","executionInfo":{"status":"ok","timestamp":1619073272535,"user_tz":-540,"elapsed":571,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e2372f3e-926c-44e5-a8bc-697f1be7b236"},"source":["#Q5f. Execute the following statement. What do the numbers displayed refer to?\n","y_train.shape\n","# we have 50000 ground truths which are corresponding to 50000 image each."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"DESekbEYxp77","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1620108297166,"user_tz":-540,"elapsed":886,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"64f2f15b-431c-478c-ed92-7cdcc9c45bcb"},"source":["#Q6: Display the values of n,m, c, and nh. What are they? For what are they used in the following code? \n","n,m = x_train.shape\n","c = y_train.max()+1\n","nh = 50\n","# if we have neural net consisting of layer1,layer2\n","# n is the number of feature of each sample which is used for layer1 weight matrix multiplication which is 784 ( 50000*784 @ 784*50)\n","# m is the number of samples, and its dimension,50000, will kept throughout the whole neural net layers ( 50000*784 @ 784*50) -> (50000*50 @ 50*10)\n","# nh is the number of hidden nodes for each sample which is 50\n","# c is the number of the kind of labels,10, which we want to predict. It is the output number of layer2's node for each sample which is 10 (50000*10)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b9d0e300d1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Q6: Display the values of n,m, c, and nh. What are they? For what are they used in the following code?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# if we have neural net consisting of layer1,layer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_cCzC9iZxp77"},"source":["#The following defines Model class, which will be used to create a neural net.\n","#Q7a: nn.Module is the parent class of class Model. Why do you want to make Model a child class to its parent nn.Module, rather than making Model stand on its own?\n","#Q7b. See the definition of self.layers field. It contains nn.Linear(n_in,nh). What is the difference between nn.Linear class nn.Linear(n_in,nh)? \n","#Q7c. Would you think that the weight and bias parameters of the two linear layers are initialized when object nn.Linear(n_in,nh) is constructed? If so, guess why.\n","from torch import nn\n","class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x\n","#a\n","# nn.Module is the basic neural net module like a basket. After adding layers to that module, we can construct the neural net\n","# nn.Module encapsulate the parmeters, helping to use gpus,exporting,loading etc like a helper for making model class\n","# We often inherit this module when we make model class\n","# Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes\n","#b\n","# nn.Linear is class itself while nn.Linear(n_in,nh) means initializing the class to instance. It executes __init__ with parameters.\n","#c\n","# yes, when we should construct the weight and bias parameters, we need outer information with __init__ parameters\n","# for example weight initialization needs the numbers to make size of dimension. for weight, (n_in *nh) size\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJx4LIA1xp78"},"source":["#Q8. When the following statement is executed, what happens? Explain by referring to a method in class Model.\n","model = Model(m, nh, 10)\n","# If we initialize the class, __init__ function will be executed.\n","# In general, they declare the variables with parameters and make layers using parameters\n","# we can assign the three layers to class.layers with n_in,nh,n_out\n","# also we can set the basic things by __init__ inherited from nn.Module."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPFt346vxp78"},"source":["#Q9a. When the following statement is executed, what happens? Explain by referring to a method in class Model. \n","#Q9b. Draw a diagram or a graph by using arrows and boxes, which shows what computation is performed when model(x_train) is executed.\n","pred = model(x_train)\n","#a\n","# After we initialize the class, we can also execute the instance with parameters using __call__ function\n","# In nerual net model, we often predict the result with train data input using __call__ function\n","# Here, when we put the x_train data in __call__ function, x_train go through all layers and make prediction which is final output\n","#b\n","# 50000,784 -> linear(xW+b) -> 50000 * 50 -> activation(max(0,x)) function -> 50000 * 50 -> linear(xW_2+b_2) -> 50000 * 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXbLts9ZRMgd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091071941,"user_tz":-540,"elapsed":676,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"7837cbe3-020f-4d7e-8740-036d3d1dc953"},"source":["#Q10: Execute the following statement. What do the 10 displayed numbers refer to?\n","pred[0]\n","# pred[0] means the degree of being 10 label each for first data sample. In general, we select the prediction label with biggest number( 0.0901)\n","# Because we have 10 labels, there are also 10 numbers.\n","# for example, pred[0][0](-0.0841) means how much it is likely to be the label 0.\n","# remember it is not proability and just weight."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.1731,  0.0460, -0.0034, -0.0246,  0.2060, -0.1380,  0.0344,  0.0863,\n","         0.0132, -0.2229], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"-Ejrm01LRSxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091073198,"user_tz":-540,"elapsed":839,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"f63cfca1-d992-43ed-f387-887772a4332d"},"source":["#Q11. Execute the following statement. Is the resulting value near to 1? If not, what does it imply?\n","pred[0].sum()\n","# Of course not, it is just likely weight for each label, not the proability.\n","# So if we want to apply statstic proability concept for predicting label, we need to convert sum of elements to 1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.1761, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"qhbGAsGWxp79"},"source":["### Softmax function"]},{"cell_type":"markdown","metadata":{"id":"fFGQT0zpxp79"},"source":["First, we will need to compute the softmax of our activations. This is defined by:\n","\n","$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n","\n","or more concisely:\n","\n","$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n","\n","In practice, we will need the log of the softmax when we calculate the loss."]},{"cell_type":"code","metadata":{"id":"Q3ZRxbanxp7_"},"source":["def softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wo_n_z36r77v"},"source":["sm_pred = softmax(pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnURmMOHsA2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091076981,"user_tz":-540,"elapsed":581,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"4c3d010e-9521-4231-a73d-3142c2ed3e26"},"source":["#Q12. Execute the following statement. What do the 10 displayed numbers refer to?\n","sm_pred[0]\n","# each number means probability of each label being the answer.\n","# for exmaple, if sm_pred[0][0] is 0.3, the proability of being 0 for image is 30%\n","# Also their number arranges from 0 ~ 1 which is Probabilistic interval"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0850, 0.1058, 0.1007, 0.0986, 0.1241, 0.0880, 0.1046, 0.1101, 0.1024,\n","        0.0808], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"MvUVm4qpsQ80","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091078054,"user_tz":-540,"elapsed":686,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"d650b6eb-bebb-4a84-826f-4785555a1bbd"},"source":["#Q13. Execute the following statement. What does the displayed numbers mean?\n","sm_pred[0].sum()\n","# this should be 1 if it is subordinate to the probability. One of the probaility's property is that sum all values  is 1.\n","# this means each number gives us the information about how much probable each label would be the answer."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1., grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"sqq6KTqimJ-T"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"fWPxYHo1mNjm"},"source":["## Log softmax"]},{"cell_type":"code","metadata":{"id":"DNuWma6zr00h"},"source":["def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDeeKw-Sxp8A"},"source":["log_sm_pred = log_softmax(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJmGXhPfRYba","executionInfo":{"status":"ok","timestamp":1619091080173,"user_tz":-540,"elapsed":516,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"aa5a2621-993f-4237-8942-af541ab8ffdf"},"source":["log_sm_pred[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.4656, -2.2465, -2.2958, -2.3170, -2.0864, -2.4304, -2.2580, -2.2061,\n","        -2.2792, -2.5153], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"DfNuX2v1xp8B"},"source":["\n","\n","## Cross Entropy Loss Function.\n"," Read the following paragraph to understand what the cross entroy loss functioni is. \n","\n","The difference between  two probabilities: \n"," https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation\n","\n","The cross entropy formula takes in two distributions,  $p(x^{(s)})$, the true distribution (defined by the label data), and $\\hat{p}(x^{(s)})$, the estimated distribution (predicted by the neural net), defined over the discrete variable $x^{(s)}$  and is given by\n","\n","$H(p,\\hat{p})=−\\sum_{s \\in B} p(x^{(s)}) \\cdot log(\\hat{p}(x^{(s)}))$\n","\n","\n","In general, $ p(x^{(s)}) = [ p_{1} (x^{(s)}), ..., p_{n}(x^{(s)})]$ is a probability distribution over a set of categories. \n","But since our $ p(x^{(s)})$ are 1-hot encoded, that is, in the form of  $ p(x^{(s)}) =[0,0,..,0,1,0..]$, where the probability of only one category is one and those of the other categories are all zero, this can be rewritten as\n","\n"," \\begin{equation}\n"," H(p,\\hat{p})= -\\sum_{s \\in B} [ 0*\\log(\\hat{p}_{1} (x^{(s)}))  \n"," + 1*\\log(\\hat{p}_{i} (x^{(s)}) ) +..+\n"," 0* \\log( \\hat{p}_{n} (x^{(s)}) ) ] \\\\ \n","  = -\\sum_{s \\in B} \\log(\\hat{p}_{i(s)} ) (x^{(s)}) ) \n","  \\tag{crossEntroyEq}\n"," \\end{equation}\n"," \n"," Te softmax function plays the role of the probability distribution $\\hat{p}_{1} (x^{(s)})$.\n","\n","  Here $i(s)$ is the index of the one-hot  probability distribution $p(x^{(s)})$ where the probability is one. \n"," \n"," "]},{"cell_type":"markdown","metadata":{"id":"KBtZhgePxp8D"},"source":["# integer array indexing "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VehY657jB7IB","executionInfo":{"status":"ok","timestamp":1619091082128,"user_tz":-540,"elapsed":567,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"50b1acaa-37e2-4793-e564-87cc89bed9c0"},"source":["#Q14a.  Execute the following statement. What is the role of list [0,1,2] in the statement?  What do the displayed numbers refer to? \n","log_sm_pred[ [0,1,2]]\n","# The role of list [0,1,2] is to extract the 0th,1th,2th vector(sample) from whole output\n","# each element of those three vector represents the log_softmax values between -infinity and 0\n","# if there is highest value, model think that the label of highest value would be the answer.\n","# Becuase there are 10 kinds of answers, there are 10 elements for each vector for 0,1,2.\n","# It is calculated by softmax and log.\n","# First we calcuate probability of each label using softmax, then we apply the log \n","# why we use log is to get benefit from caculation in proability multiplication and at the same time\n","# it makes more penalty on big loss than softmax which makes the model training efficient."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-2.4656, -2.2465, -2.2958, -2.3170, -2.0864, -2.4304, -2.2580, -2.2061,\n","         -2.2792, -2.5153],\n","        [-2.5745, -2.2436, -2.3859, -2.3912, -2.0901, -2.3638, -2.3008, -2.1226,\n","         -2.2519, -2.3927],\n","        [-2.3427, -2.1785, -2.3696, -2.3438, -1.9762, -2.3294, -2.4005, -2.3111,\n","         -2.3806, -2.4893]], grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"uJM_9p0rxp8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091084680,"user_tz":-540,"elapsed":759,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"29d57f15-11f6-400e-95b1-7abf5a8283a0"},"source":["#Q14b.  Execute the following statement. What is the role of list [2,4,6] in the statement?  What do the displayed numbers mean? Explain compare the result of Q15b and the result of this statement.\n","log_sm_pred[[0,1,2], [2,4,6]]\n","# the role of list [2,4,6] is to extract log_softmax value of the ground truth index for each vector.\n","# log_sm_pred[0,2] means how much first sample data is likely to be 2 and log_smpred[1,4] means how much second sample data is likely to be 4 and so forth each.\n","# ideal number of each element is close to -inf except ground truth label, because log(0) = -inf\n","# If the number is not closest to 0, we need to train more until the ground truth index's prediction value becomes higher\n","#\n","# If we calcuate the loss with 14a, lots of computing resources are wasted. Because all probabilities, except the answer, become 0 in one-hot ground truth data, we don't\n","# need the other numbers. For example, if we calcuate the loss of first sample(log_sm_pred[0]) with y_train[0](suppose one-hot vector) using cross_entropy loss,\n","#we need only log_sm_red[0][5],where answer is 5, not the whole vector output of first sample. Since, any number multiplying with 0 becomes 0.\n","# So to be efficient loss calculation with cross_entropy, 14b is desirable."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.2958, -2.0901, -2.4005], grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"h1bTmfz5xp8J"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2081)"]},{"cell_type":"code","metadata":{"id":"xoc2YwU5wATE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091085746,"user_tz":-540,"elapsed":808,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c7404c63-37cd-4023-f439-ee8dbf6c6871"},"source":["range(y_train.long().shape[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["range(0, 50000)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"va2lulaep1_J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8GMBhmq9w4QG"},"source":["selectOneHot = log_sm_pred[ range(y_train.long().shape[0]), y_train.long() ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1c7b4HtqWLi","executionInfo":{"status":"ok","timestamp":1619091086912,"user_tz":-540,"elapsed":554,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"5531a634-33e4-4587-a4c4-f68341b26785"},"source":["selectOneHot.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xr89iih8WWT0","executionInfo":{"status":"ok","timestamp":1619091095257,"user_tz":-540,"elapsed":576,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"7b739105-d28a-45ce-9960-8351018143bd"},"source":["selectOneHot"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.4304, -2.5745, -1.9762,  ..., -2.3435, -2.0935, -2.3055],\n","       grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"1ZDSRenHqdcP"},"source":["#Q15.  What does selectOneHot refer to?\n","# it selects the log probability of ground truth's element for each sample from log_sm_pred\n","# for example, first image's answer is 5. so select the 5th number for first image(log_sm_pred[0,5]) and other images does same process.\n","# In conclusion, it contains the log probability values for each sample to get the right answer. we expect those values would be near 0."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZI0cXSLRxp8K"},"source":["def nll(input, target):\n","   \n","   return -input[range(target.shape[0]), target].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzmLXXWiPTSF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619091649089,"user_tz":-540,"elapsed":707,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"3b84b8cd-4256-4709-966e-7937443d83bf"},"source":["y_train.long().shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Bs5UU4fyxp8K"},"source":["#Q16. Read nill function and explain how this function computes the result. You need to refer to {CrossEntropyLoss}.\n","loss = nll(log_sm_pred, y_train.long())\n","# first we infer the log proability of each label for each image using log_softmax from output layer.\n","# then using crossentropy loss, we calculate the loss between each sample's prediction and true answers. Then we get the average of total datas' loss\n","# if we calculate crossentropy loss, we only use the logsoftmax value on answer label to save resources as I mentioned above.\n","# since,except the answer's index's value of answer-one hot encoding vector, they are all zero which are used to mulitply with output of model."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFFUIWliTCjA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkSkgQYQxp8L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c5cbca1-0047-4d62-b275-7445ec78e38e"},"source":["loss"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3240, grad_fn=<NegBackward>)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"-Xb-DbeRq2aa"},"source":["#Q16. Compare function nll() and the formula CrossEntropyEq. Explain that function nll() computes the cross entropy function between\n","# predicted probability distrubution of the input images and the ground truth (labeled) probability distribution of the input images. \n","test_near(F.cross_entropy(pred, y_train.long()), loss)\n","# if executing, we find that they are same.\n","# In PyTorch, log_softmax and nll_loss are combined in one optimized function, F.cross_entropy.\n","# as you see below, we convert the values into probability range 0 ~ 1 by using softmax\n","# softmax is exp(x_i) / sum of all exp(x_{})\n","# Then we calculate the loss between prediction and loss using cross-entropy loss\n","# In the case of cross-entropy loss for one data sample is p(y)*log(p(y_hat) where y is [0,0,0,...1,0,0] format and y_hat is [v1,v,2,v3,....,v10] format\n","# The only thing we need is 1 *log(v8). For calculation efficiency, we select answer index's value for y revealing 1 and same index for y_hat\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdFUzc94xp8V"},"source":["## Basic training loop"]},{"cell_type":"markdown","metadata":{"id":"495DuZ5Xxp8V"},"source":["Basically the training loop repeats over the following steps:\n","- get the output of the model on a batch of inputs\n","- compare the output to the labels we have and compute a loss\n","- calculate the gradients of the loss with respect to every parameter of the model\n","- update said parameters with those gradients to make them a little bit better"]},{"cell_type":"markdown","metadata":{"id":"6e6Y1AKyxp8W"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2542)"]},{"cell_type":"code","metadata":{"id":"fDHaEy3oxp8X"},"source":["loss_func = F.cross_entropy # This built-in Pytorch function  combines log_softmax and nll in a single function.\n","# In the following code, we will use this function, not the function nll defined above."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcuWlwDXxp8Y"},"source":["#export\n","def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozBbmfr7xp8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619075476271,"user_tz":-540,"elapsed":601,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e08f4f79-0956-4f03-e684-7d955c44da64"},"source":["bs=1000                  # batch size\n","\n","xb = x_train[0:bs]     # a mini-batch from x\n","preds = model(xb)      # predictions\n","preds[0], preds.shape\n","print(preds[0])\n","#Q17. What does preds[0] refer to?\n","# it means prediction for the first data sample. Each element means likey to belong to each index and the largest number will be the predicted as answer."],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 1.6429, -4.2640, -0.1881,  4.0876, -6.1015,  5.5995, -1.8717, -0.1294,\n","         1.9622, -0.9064], grad_fn=<SelectBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ue2zaDOLxp8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619073811957,"user_tz":-540,"elapsed":695,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"3e173a5e-ec30-4c0f-eef9-29fcc5c82157"},"source":["yb = y_train[0:bs]\n","loss = loss_func(preds, yb.long())\n","loss\n","#Q18. What does the value of loss refer to?\n","# it means how much the model makes errors using crossEntropy. Near 0 would be pursued"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3234, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"vYSsK_Rlxp8b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619073958248,"user_tz":-540,"elapsed":1301,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"b5d21992-e0de-421e-cd24-9db3c9f5d574"},"source":["accuracy(preds, yb)\n","#Q19. What does  the value of the above statement refer to? Examine the value. Is it supposed to be equal to one approximately?\n","# If it is not at the current moment, what is the reason for it?\n","\n","# It represents whether model prediction is same with the true answer or not, that is model accuracy.\n","# idealy, we want accuracy to be 1 but it is not 1 now. 7 % is too low to use.\n","# Because we didn't do enough training loop for finding best parameters called optimizing process to predict accurately,\n","# We need to try more training.\n","# So it implies that we need to make model do more training. And the model is not good enough to use yet."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0700)"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"9-hKTgAOFvu0"},"source":["# The mechanism for training the network: \n","\n","(1) computing the graidents of Loss with respect to tensors at each layer, (2) backpropgation: applying the chain rule to compute the gradient vector of the parameters, (3) updating the parameters.\n","\n","https://github.com/pytorch/pytorch/blob/35bd2b3c8b64d594d85fc740e94c30aa67892a34/torch/tensor.py\n","\n","https://github.com/pytorch/pytorch/blob/35bd2b3c8b64d594d85fc740e94c30aa67892a34/torch/tensor.py\n","\n","https://stackoverflow.com/questions/57248777/backward-function-in-pytorch\n"]},{"cell_type":"code","metadata":{"id":"dxLMr6u-xp8c"},"source":["lr = 0.5   # learning rate\n","epochs = 2 # how many epochs to train for"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0KH3m-Uxp8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619074071377,"user_tz":-540,"elapsed":3612,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"5abfb6a8-1acc-430b-ecc2-909b632ebb9d"},"source":["#Q20. What does the number  (n-1)//bs + 1 refer to?\n","# it refers how many we have the mini-batch. For example, if we set bs=32 and whole data is 100, we have 4 mini-batch.\n","# so if we have 10 batch sizes it become range(10), And why (n-1 and + 1) is ? It is to prevent batch size zero due to '//' calculation.\n","# even though the (n-1)//bs = 0, some number of data under the batch size still left and we should train those data.\n","# For example there are 15 data left and batch size is 32. Despite 15//32 == 0, we need to train those data that's why we set (n-1)//bs + 1\n","import pdb\n","for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1): # for each batch in the current epoch\n","#       pdb.set_trace()\n","        start_i = i*bs\n","        end_i = start_i+bs\n","\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","\n","\n","        ybHat = model(xb)\n","        #h = ybHat.register_hook( lambda grad: print (grad) )\n","\n","        loss = loss_func( ybHat, yb.long())        \n","\n","        #ybHat.grad\n","        loss.backward() # When you call loss.backward(), all it does is compute gradient of loss \n","                        # w.r.t all the parameters in loss that have requires_grad = True and store them in parameter.grad attribute for every parameter\n","\n","        #ybHat.grad                \n","        #The following performs what Pytorch function optimizer.step() does:\n","        # It updates all the weight and bias parameters based on the gradients of loss with respect to the weight and bias parameter \n","        with torch.no_grad():\n","            for l in model.layers:\n","                if hasattr(l, 'weight'):\n","                    l.weight -= l.weight.grad * lr\n","                    l.bias   -= l.bias.grad   * lr\n","\n","                    #\n","                    # loss.backward() computes the gradient of loss w.r.t. graph LEAVES.\n","                    #  This function accumulates gradients in the leaves - you might need to zero\n","                    # we don't care about gradients from the previous batch.\n","                    # Not zeroing grads would lead to gradient accumulation across batches\n","                    l.weight.grad.zero_()\n","                    l.bias.grad.zero_()\n","\n","       #Q21:  print the loss and the accuracy after training the net using the validation dataset.\n","       #      Explain how the loss and accuracy change as each batch is used  for training the network model.\n","       #      Try to convey detailed and specific information about the progress of training the neural net.\n","       #      Observe the printed data carefully\n","       #--\n","        #loss and accuracy should be changed because we are doing mini-batch training !\n","        # we update the parameters such as w,b with gradient descent for each batch loop.\n","        # the loss and accuracy might be fluctuated but in the whole aspect of training, loss Continuously go down and accuracy persistantly goes up.\n","        # Its process is a way of finding optimal parameters. At a certain moment, the number of loss and accuracy would be converged around certain number,\n","        #for example if 0.3004 for epoch1 and the this number doesn't change much in more training, we can know that model might be in the optimal status)\n","        # But, after many traning this value there is a chance to escape the local optima which we considered as real opimal, \n","        # so we should do lots of shoots to find real optimal status not stop at early time.\n","\n","\n","\n","       #Q22. Explain why you would use validation dataset to check the progress of learning of your network.\n","        # Because we don't want the model overfitting, we should test the model with new datas preventing too \"local\".\n","        # Overfitting means that the model is only fitable for the train data set, not the new data set.\n","        # In the real world there is lots of deviation or errors(alpha) for data. However if we use overfitted model, \n","        # this small errors make huge impact on the model only to make wrong prediction. That is, we need to make model generalize.\n","        # The way We can check the degree of overfitting is to use new data, which is validation data, persistantly for the training model\n","        # not to be overfitted for train dataset.\n","        \n","        yHatValid = model(x_valid)\n","        lossValid = loss_func( yHatValid, y_valid.long() )        \n","\n","        \n","        print('epoch={0}, batch ={1}:'.format(epoch, i) )  \n","        print('  lossValid=', lossValid )\n","        print('  accuracyValid = ', accuracy( yHatValid, y_valid) )\n","\n","    #Q23. Afer each epoch, print the loss and the accuracy of the network by using the training dataset. \n","    #     Explain the result. Be attentive to the result.   \n","    # it might be fluctuated in mini-batch training but\n","    # the loss of the network steadily down for training loop(0.49 -> 0.33), and the accuracy becomes higher than before.(0.84->0.9)\n","    # It means our model training do it with right way. Not wrong structured model.\n","#     epoch={0}: 0\n","#   lossTrain= tensor(0.4975, grad_fn=<NllLossBackward>)\n","#   accuracyTrain =  tensor(0.8412)\n","#     epoch={0}: 1\n","#   lossTrain= tensor(0.3390, grad_fn=<NllLossBackward>)\n","#   accuracyTrain =  tensor(0.9012)\n","\n","\n","    yHatTrain = model(x_train)\n","    lossTrain = loss_func( yHatTrain, y_train.long() )        \n","\n","    print('*******************************************************')    \n","    print('epoch={0}:', epoch )  \n","    print('  lossTrain=', lossTrain)\n","    print('  accuracyTrain = ', accuracy( yHatTrain, y_train) )\n","\n","    #Q24. Afer each epoch, print the loss and the accuracy of the network by using the validatio  dataset. \n","    #     Explain the result. Be attentive to the result.   \n","    # The validation's loss also steadily go down and the accuracy steadily goes up.\n","    # To sum up the information of validation and training result, the model training goes in reasonable way.\n","    # Since, if trainloss goes down and validation loss goes up, it signifies the chance of overfitting.\n","    # Or if trainloss goes up and validation loss goes up together, it signifies the chance of the training too much or we made wrong structure model.\n","#     epoch={0}: 0\n","#   lossValid  tensor(0.4490, grad_fn=<NllLossBackward>)\n","#   accuracyValid =  tensor(0.8627)\n","\n","#     epoch={0}: 1\n","#   lossValid  tensor(0.3103, grad_fn=<NllLossBackward>)\n","#   accuracyValid =  tensor(0.9123)\n","    yHatValid = model(x_valid)\n","    lossValid = loss_func( yHatValid, y_valid.long() )        \n","\n","    print('*******************************************************')    \n","    print('epoch={0}:', epoch )  \n","    print('  lossValid ', lossValid)\n","    print('  accuracyValid = ', accuracy( yHatValid, y_valid) )\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch=0, batch =0:\n","  lossValid= tensor(2.2533, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.1925)\n","epoch=0, batch =1:\n","  lossValid= tensor(2.1823, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.3560)\n","epoch=0, batch =2:\n","  lossValid= tensor(2.0875, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.5162)\n","epoch=0, batch =3:\n","  lossValid= tensor(1.9806, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.5763)\n","epoch=0, batch =4:\n","  lossValid= tensor(1.8442, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.5657)\n","epoch=0, batch =5:\n","  lossValid= tensor(1.7000, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6248)\n","epoch=0, batch =6:\n","  lossValid= tensor(1.5472, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6702)\n","epoch=0, batch =7:\n","  lossValid= tensor(1.4095, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6708)\n","epoch=0, batch =8:\n","  lossValid= tensor(1.3041, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6929)\n","epoch=0, batch =9:\n","  lossValid= tensor(1.1679, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7137)\n","epoch=0, batch =10:\n","  lossValid= tensor(1.1281, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6893)\n","epoch=0, batch =11:\n","  lossValid= tensor(1.0787, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6670)\n","epoch=0, batch =12:\n","  lossValid= tensor(1.1507, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.5991)\n","epoch=0, batch =13:\n","  lossValid= tensor(1.1407, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.6501)\n","epoch=0, batch =14:\n","  lossValid= tensor(0.9551, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7015)\n","epoch=0, batch =15:\n","  lossValid= tensor(0.8529, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7638)\n","epoch=0, batch =16:\n","  lossValid= tensor(0.9155, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7362)\n","epoch=0, batch =17:\n","  lossValid= tensor(0.7984, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7379)\n","epoch=0, batch =18:\n","  lossValid= tensor(0.8017, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7671)\n","epoch=0, batch =19:\n","  lossValid= tensor(0.7190, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8046)\n","epoch=0, batch =20:\n","  lossValid= tensor(0.7586, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7581)\n","epoch=0, batch =21:\n","  lossValid= tensor(0.7073, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7673)\n","epoch=0, batch =22:\n","  lossValid= tensor(0.6353, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8152)\n","epoch=0, batch =23:\n","  lossValid= tensor(0.6381, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7974)\n","epoch=0, batch =24:\n","  lossValid= tensor(0.6124, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8138)\n","epoch=0, batch =25:\n","  lossValid= tensor(0.6260, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7921)\n","epoch=0, batch =26:\n","  lossValid= tensor(0.5624, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8349)\n","epoch=0, batch =27:\n","  lossValid= tensor(0.5663, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8423)\n","epoch=0, batch =28:\n","  lossValid= tensor(0.5871, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8165)\n","epoch=0, batch =29:\n","  lossValid= tensor(0.6458, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7927)\n","epoch=0, batch =30:\n","  lossValid= tensor(0.5065, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8562)\n","epoch=0, batch =31:\n","  lossValid= tensor(0.4716, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8790)\n","epoch=0, batch =32:\n","  lossValid= tensor(0.4836, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8747)\n","epoch=0, batch =33:\n","  lossValid= tensor(0.5176, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8492)\n","epoch=0, batch =34:\n","  lossValid= tensor(0.5104, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8559)\n","epoch=0, batch =35:\n","  lossValid= tensor(0.5937, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8135)\n","epoch=0, batch =36:\n","  lossValid= tensor(0.7767, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7580)\n","epoch=0, batch =37:\n","  lossValid= tensor(0.8204, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.7388)\n","epoch=0, batch =38:\n","  lossValid= tensor(0.6209, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8079)\n","epoch=0, batch =39:\n","  lossValid= tensor(0.5225, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8297)\n","epoch=0, batch =40:\n","  lossValid= tensor(0.4632, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8710)\n","epoch=0, batch =41:\n","  lossValid= tensor(0.4861, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8446)\n","epoch=0, batch =42:\n","  lossValid= tensor(0.4328, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8837)\n","epoch=0, batch =43:\n","  lossValid= tensor(0.4211, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8817)\n","epoch=0, batch =44:\n","  lossValid= tensor(0.4089, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8908)\n","epoch=0, batch =45:\n","  lossValid= tensor(0.3923, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8904)\n","epoch=0, batch =46:\n","  lossValid= tensor(0.4008, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8872)\n","epoch=0, batch =47:\n","  lossValid= tensor(0.4118, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8857)\n","epoch=0, batch =48:\n","  lossValid= tensor(0.4371, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8672)\n","epoch=0, batch =49:\n","  lossValid= tensor(0.4490, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8627)\n","*******************************************************\n","epoch={0}: 0\n","  lossTrain= tensor(0.4975, grad_fn=<NllLossBackward>)\n","  accuracyTrain =  tensor(0.8412)\n","*******************************************************\n","epoch={0}: 0\n","  lossValid  tensor(0.4490, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8627)\n","epoch=1, batch =0:\n","  lossValid= tensor(0.4015, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8860)\n","epoch=1, batch =1:\n","  lossValid= tensor(0.4232, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8736)\n","epoch=1, batch =2:\n","  lossValid= tensor(0.4372, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8503)\n","epoch=1, batch =3:\n","  lossValid= tensor(0.4084, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8750)\n","epoch=1, batch =4:\n","  lossValid= tensor(0.4035, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8827)\n","epoch=1, batch =5:\n","  lossValid= tensor(0.4019, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8773)\n","epoch=1, batch =6:\n","  lossValid= tensor(0.3848, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8899)\n","epoch=1, batch =7:\n","  lossValid= tensor(0.3659, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8976)\n","epoch=1, batch =8:\n","  lossValid= tensor(0.3978, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8810)\n","epoch=1, batch =9:\n","  lossValid= tensor(0.3568, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8988)\n","epoch=1, batch =10:\n","  lossValid= tensor(0.3610, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8970)\n","epoch=1, batch =11:\n","  lossValid= tensor(0.3685, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8900)\n","epoch=1, batch =12:\n","  lossValid= tensor(0.3883, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8862)\n","epoch=1, batch =13:\n","  lossValid= tensor(0.3696, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8920)\n","epoch=1, batch =14:\n","  lossValid= tensor(0.3550, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8987)\n","epoch=1, batch =15:\n","  lossValid= tensor(0.3347, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9066)\n","epoch=1, batch =16:\n","  lossValid= tensor(0.3371, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9071)\n","epoch=1, batch =17:\n","  lossValid= tensor(0.3448, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8992)\n","epoch=1, batch =18:\n","  lossValid= tensor(0.3488, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9019)\n","epoch=1, batch =19:\n","  lossValid= tensor(0.3393, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9031)\n","epoch=1, batch =20:\n","  lossValid= tensor(0.3360, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9048)\n","epoch=1, batch =21:\n","  lossValid= tensor(0.3332, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9047)\n","epoch=1, batch =22:\n","  lossValid= tensor(0.3328, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9062)\n","epoch=1, batch =23:\n","  lossValid= tensor(0.3303, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9038)\n","epoch=1, batch =24:\n","  lossValid= tensor(0.3543, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8966)\n","epoch=1, batch =25:\n","  lossValid= tensor(0.3490, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8967)\n","epoch=1, batch =26:\n","  lossValid= tensor(0.3770, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8856)\n","epoch=1, batch =27:\n","  lossValid= tensor(0.4172, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8677)\n","epoch=1, batch =28:\n","  lossValid= tensor(0.3896, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8820)\n","epoch=1, batch =29:\n","  lossValid= tensor(0.3696, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8869)\n","epoch=1, batch =30:\n","  lossValid= tensor(0.3759, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8900)\n","epoch=1, batch =31:\n","  lossValid= tensor(0.3231, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9102)\n","epoch=1, batch =32:\n","  lossValid= tensor(0.3364, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9025)\n","epoch=1, batch =33:\n","  lossValid= tensor(0.3207, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9079)\n","epoch=1, batch =34:\n","  lossValid= tensor(0.3071, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9155)\n","epoch=1, batch =35:\n","  lossValid= tensor(0.3098, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9116)\n","epoch=1, batch =36:\n","  lossValid= tensor(0.3175, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9088)\n","epoch=1, batch =37:\n","  lossValid= tensor(0.3113, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9136)\n","epoch=1, batch =38:\n","  lossValid= tensor(0.3110, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9119)\n","epoch=1, batch =39:\n","  lossValid= tensor(0.3328, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9007)\n","epoch=1, batch =40:\n","  lossValid= tensor(0.3185, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9107)\n","epoch=1, batch =41:\n","  lossValid= tensor(0.3679, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8837)\n","epoch=1, batch =42:\n","  lossValid= tensor(0.3295, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9054)\n","epoch=1, batch =43:\n","  lossValid= tensor(0.3243, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9026)\n","epoch=1, batch =44:\n","  lossValid= tensor(0.3092, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9126)\n","epoch=1, batch =45:\n","  lossValid= tensor(0.3004, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9144)\n","epoch=1, batch =46:\n","  lossValid= tensor(0.3109, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9122)\n","epoch=1, batch =47:\n","  lossValid= tensor(0.3316, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9047)\n","epoch=1, batch =48:\n","  lossValid= tensor(0.3538, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.8945)\n","epoch=1, batch =49:\n","  lossValid= tensor(0.3103, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9123)\n","*******************************************************\n","epoch={0}: 1\n","  lossTrain= tensor(0.3390, grad_fn=<NllLossBackward>)\n","  accuracyTrain =  tensor(0.9012)\n","*******************************************************\n","epoch={0}: 1\n","  lossValid  tensor(0.3103, grad_fn=<NllLossBackward>)\n","  accuracyValid =  tensor(0.9123)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"98nX1aGLyXJP"},"source":[""]},{"cell_type":"code","metadata":{"id":"I0fC9kbvxp8g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cd813d0-b5cb-4df3-97c0-bf7ae1529cfb"},"source":["loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0979, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"n70culC-xp8h"},"source":["## Using parameters and optim"]},{"cell_type":"markdown","metadata":{"id":"Ztmln-E0xp8h"},"source":["### Parameters"]},{"cell_type":"markdown","metadata":{"id":"3JMWX2-fxp8i"},"source":["Use `nn.Module.__setattr__` and move relu to functional:"]},{"cell_type":"markdown","metadata":{"id":"2qu-R0Ljxp8j"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2818)"]},{"cell_type":"code","metadata":{"id":"lw5k0KJRxp8j"},"source":["class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.l1 = nn.Linear(n_in,nh)\n","        self.l2 = nn.Linear(nh,n_out)\n","        \n","    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lq8zwnBlxp8k"},"source":["model = Model(m, nh, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFsAG7KCxp8k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"56949b22-3eae-4ec0-8a23-e2f754e7fa66"},"source":["for name,l in model.named_children(): print(f\"{name}: {l}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["l1: Linear(in_features=784, out_features=50, bias=True)\n","l2: Linear(in_features=50, out_features=10, bias=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QQAcSLAVxp8l","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3607efe6-16e9-4fb8-e118-a6d7db48cb32"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (l1): Linear(in_features=784, out_features=50, bias=True)\n","  (l2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"O77Ds-LRxp8m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dd994eb-d95f-44f3-9adb-24ec6f4bb3d1"},"source":["model.l1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=784, out_features=50, bias=True)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"SUDx09pkxp8m"},"source":["def fit():\n","    for epoch in range(epochs):\n","        for i in range((n-1)//bs + 1):\n","            start_i = i*bs\n","            end_i = start_i+bs\n","            xb = x_train[start_i:end_i]\n","            yb = y_train[start_i:end_i]\n","            loss = loss_func(model(xb), yb.long())\n","\n","            loss.backward()\n","            with torch.no_grad():\n","                for p in model.parameters(): p -= p.grad * lr\n","                model.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkjGdktlxp8o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e98cf04b-c832-4d83-b86f-630125c2084a"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0097, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"-wQ0CkLmxp8p"},"source":["Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."]},{"cell_type":"code","metadata":{"id":"nfLTjkNJxp8p"},"source":["class DummyModule():\n","    def __init__(self, n_in, nh, n_out):\n","        self._modules = {}\n","        self.l1 = nn.Linear(n_in,nh)\n","        self.l2 = nn.Linear(nh,n_out)\n","        \n","    def __setattr__(self,k,v):\n","        if not k.startswith(\"_\"): self._modules[k] = v\n","        super().__setattr__(k,v)\n","        \n","    def __repr__(self): return f'{self._modules}'\n","    \n","    def parameters(self):\n","        for l in self._modules.values():\n","            for p in l.parameters(): yield p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxdtM1ilxp8q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"40d56eb6-6822-436c-c011-bd7ae14336d3"},"source":["mdl = DummyModule(m,nh,10)\n","mdl"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"asWZtQT7xp8r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba4de7ed-d661-41a1-c769-f9dbade23efa"},"source":["[o.shape for o in mdl.parameters()]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[torch.Size([50, 784]),\n"," torch.Size([50]),\n"," torch.Size([10, 50]),\n"," torch.Size([10])]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"PJzROcD-xp8r"},"source":["### Registering modules"]},{"cell_type":"markdown","metadata":{"id":"Gi6Bj3Egxp8s"},"source":["We can use the original `layers` approach, but we have to register the modules."]},{"cell_type":"markdown","metadata":{"id":"SCDxRdQexp8s"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2997)"]},{"cell_type":"code","metadata":{"id":"8ax355bwxp8t"},"source":["layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FVOwYH8xp8t"},"source":["class Model(nn.Module):\n","    def __init__(self, layers):\n","        super().__init__()\n","        self.layers = layers\n","        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5hq-X7Vxp8u"},"source":["model = Model(layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqYP2MXlxp8u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8148eec4-6e20-4ff3-8949-04c92b463be5"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n","  (layer_1): ReLU()\n","  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"diwEDyzJxp8v"},"source":["### nn.ModuleList"]},{"cell_type":"markdown","metadata":{"id":"yUVBUAqqxp8v"},"source":["`nn.ModuleList` does this for us."]},{"cell_type":"markdown","metadata":{"id":"w39ws0evxp8w"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3173)"]},{"cell_type":"code","metadata":{"id":"YNrrTzq6xp8w"},"source":["class SequentialModel(nn.Module):\n","    def __init__(self, layers):\n","        super().__init__()\n","        self.layers = nn.ModuleList(layers)\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m59ypeCdxp8x"},"source":["model = SequentialModel(layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVSNzdUmxp8x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd6d8b69-bbf4-40a8-c02e-54b28a92cd6e"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequentialModel(\n","  (layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=50, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=50, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"QheaMa-6xp8y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"394de92e-5802-4616-9845-97c940736af1"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0318, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"yk8EQfB6xp8y"},"source":["### nn.Sequential"]},{"cell_type":"markdown","metadata":{"id":"twIiTxJAxp8y"},"source":["`nn.Sequential` is a convenient class which does the same as the above:"]},{"cell_type":"markdown","metadata":{"id":"eWOOJnAXxp8y"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3199)"]},{"cell_type":"code","metadata":{"id":"YVIaRdsPxp8z"},"source":["model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_079mLyxp8z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3afc0d73-3d2e-4fb1-e601-c8dcbdff0bfb"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0522, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"fX3SUZc5xp8z"},"source":["nn.Sequential??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1EvUMznxp81","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc2353bd-57a8-4ba3-84fb-95360c0575ce"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=784, out_features=50, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"1o20CTI0xp82"},"source":["### optim"]},{"cell_type":"markdown","metadata":{"id":"_8edAASgxp82"},"source":["Let's replace our previous manually coded optimization step:\n","\n","```python\n","with torch.no_grad():\n","    for p in model.parameters(): p -= p.grad * lr\n","    model.zero_grad()\n","```\n","\n","and instead use just:\n","\n","```python\n","opt.step()\n","opt.zero_grad()\n","```"]},{"cell_type":"markdown","metadata":{"id":"lV7oQXBxxp83"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3278)"]},{"cell_type":"code","metadata":{"id":"rz8BYVQkxp84"},"source":["class Optimizer():\n","    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n","        \n","    def step(self):\n","        with torch.no_grad():\n","            for p in self.params: p -= p.grad * lr\n","\n","    def zero_grad(self):\n","        for p in self.params: p.grad.data.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr97GcPPxp84"},"source":["model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lh_YE0ogxp84"},"source":["opt = Optimizer(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOZPZ1rsxp85"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        start_i = i*bs\n","        end_i = start_i+bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1K8AuK0yxp85","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5486e4ff-e6d2-48d8-fcb7-0221c66b98fa"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0091, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"vtEnqOylxp86"},"source":["PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"]},{"cell_type":"code","metadata":{"id":"1EbDEWXsxp87"},"source":["#export\n","from torch import optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVc-OgiExp87"},"source":["optim.SGD.step??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJyAMVzbxp88"},"source":["def get_model():\n","    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n","    return model, optim.SGD(model.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fu_n5Z2zxp89","colab":{"base_uri":"https://localhost:8080/"},"outputId":"647cfcd2-e327-412e-f4f6-21373479b771"},"source":["model,opt = get_model()\n","loss_func(model(xb), yb.long())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3490, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"id":"XT0H_e6Vxp8-"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        start_i = i*bs\n","        end_i = start_i+bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Do5mQXdWxp8-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b67c1be-aac4-44b2-fc06-c3ca97a47774"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0048, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"UVaK2RCHxp8_"},"source":["Randomized tests can be very useful."]},{"cell_type":"markdown","metadata":{"id":"TSDu_zpXxp8_"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3442)"]},{"cell_type":"code","metadata":{"id":"3GQE3EvMxp9A"},"source":["assert acc>0.7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PWzGE8Rxp9A"},"source":["## Dataset and DataLoader"]},{"cell_type":"markdown","metadata":{"id":"nz8ts0aZxp9A"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"hhmhOE2xxp9B"},"source":["It's clunky to iterate through minibatches of x and y values separately:\n","\n","```python\n","    xb = x_train[start_i:end_i]\n","    yb = y_train[start_i:end_i]\n","```\n","\n","Instead, let's do these two steps together, by introducing a `Dataset` class:\n","\n","```python\n","    xb,yb = train_ds[i*bs : i*bs+bs]\n","```"]},{"cell_type":"markdown","metadata":{"id":"U-gLw07Fxp9B"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3578)"]},{"cell_type":"code","metadata":{"id":"QdCJfv5Zxp9B"},"source":["#export\n","class Dataset():\n","    def __init__(self, x, y): self.x,self.y = x,y\n","    def __len__(self): return len(self.x)\n","    def __getitem__(self, i): return self.x[i],self.y[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wzhoIAExp9C"},"source":["train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n","assert len(train_ds)==len(x_train)\n","assert len(valid_ds)==len(x_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxsllyZ8xp9C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7e66198-b65c-4bf5-d0bc-186c336644a3"},"source":["xb,yb = train_ds[0:5]\n","assert xb.shape==(5,28*28)\n","assert yb.shape==(5,)\n","xb,yb"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5., 0., 4., 1., 9.]))"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"PNvAKEUAxp9C"},"source":["model,opt = get_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6y3T2ONQxp9D"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        xb,yb = train_ds[i*bs : i*bs+bs]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiGSBU34xp9D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c824b5eb-dfbd-443e-aa68-8ff518caf9aa"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0098, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"0fgmSsJdxp9D"},"source":["### DataLoader"]},{"cell_type":"markdown","metadata":{"id":"QlMsRaK_xp9D"},"source":["Previously, our loop iterated over batches (xb, yb) like this:\n","\n","```python\n","for i in range((n-1)//bs + 1):\n","    xb,yb = train_ds[i*bs : i*bs+bs]\n","    ...\n","```\n","\n","Let's make our loop much cleaner, using a data loader:\n","\n","```python\n","for xb,yb in train_dl:\n","    ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"Ka7lGsJoxp9E"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3674)"]},{"cell_type":"code","metadata":{"id":"DcOXwOi-xp9F"},"source":["class DataLoader():\n","    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n","    def __iter__(self):\n","        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1FCkkcdxp9F"},"source":["train_dl = DataLoader(train_ds, bs)\n","valid_dl = DataLoader(valid_ds, bs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMXh88dvxp9G"},"source":["xb,yb = next(iter(valid_dl))\n","assert xb.shape==(bs,28*28)\n","assert yb.shape==(bs,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWnYMKDhxp9G","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"36a6bc37-1a9d-45e5-84ea-91ea3ca2f72c"},"source":["plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.)"]},"metadata":{"tags":[]},"execution_count":92},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"mekhrFC3xp9G"},"source":["model,opt = get_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIu_Wvwkxp9H"},"source":["def fit():\n","    for epoch in range(epochs):\n","        for xb,yb in train_dl:\n","            pred = model(xb)\n","            loss = loss_func(pred, yb.long())\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIFF7_-xxp9H"},"source":["fit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RH5KjHAaxp9H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be1bcbd7-fe53-4bff-f68f-484e17a56168"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0277, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"markdown","metadata":{"id":"-9uDi1fcxp9H"},"source":["### Random sampling"]},{"cell_type":"markdown","metadata":{"id":"JRARDaX9xp9P"},"source":["We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."]},{"cell_type":"markdown","metadata":{"id":"MUSSX0iNxp9P"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3942)"]},{"cell_type":"code","metadata":{"id":"mgUen1XGxp9Q"},"source":["class Sampler():\n","    def __init__(self, ds, bs, shuffle=False):\n","        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n","        \n","    def __iter__(self):\n","        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n","        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nV78WqQbxp9Q"},"source":["small_ds = Dataset(*train_ds[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-wrj8hrxp9Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16c68025-e171-44dd-e22f-c07442e89e3f"},"source":["s = Sampler(small_ds,3,False)\n","[o for o in s]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"cAyM-UXDxp9R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e16e8f20-47c9-4e77-877d-50a665aee1c5"},"source":["s = Sampler(small_ds,3,True)\n","[o for o in s]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([2, 5, 1]), tensor([3, 0, 6]), tensor([7, 8, 4]), tensor([9])]"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"code","metadata":{"id":"Xf0r66xbxp9R"},"source":["def collate(b):\n","    xs,ys = zip(*b)\n","    return torch.stack(xs),torch.stack(ys)\n","\n","class DataLoader():\n","    def __init__(self, ds, sampler, collate_fn=collate):\n","        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n","        \n","    def __iter__(self):\n","        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfhQOgsVxp9S"},"source":["train_samp = Sampler(train_ds, bs, shuffle=True)\n","valid_samp = Sampler(valid_ds, bs, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZpHj2oqxp9S"},"source":["train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n","valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qG_iithdxp9S","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"2ca0cbc3-b121-4a71-8ede-aa4b3a085f4c"},"source":["xb,yb = next(iter(valid_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.)"]},"metadata":{"tags":[]},"execution_count":104},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"wDtE1Hlgxp9S","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"1afcd8fd-2ba0-4474-bafd-2b4f7400b85e"},"source":["xb,yb = next(iter(train_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.)"]},"metadata":{"tags":[]},"execution_count":105},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"fkS0eizexp9T","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"779a6399-14ee-464f-e501-7b00dfcd6816"},"source":["xb,yb = next(iter(train_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9.)"]},"metadata":{"tags":[]},"execution_count":106},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"pwxmZdIBxp9U","colab":{"base_uri":"https://localhost:8080/","height":413},"outputId":"c186f148-4b56-4150-99a1-e2bc5b92efcf"},"source":["model,opt = get_model()\n","fit()\n","\n","loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-107-24090b23970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"]}]},{"cell_type":"markdown","metadata":{"id":"BkOcYVz5xp9U"},"source":["### PyTorch DataLoader"]},{"cell_type":"markdown","metadata":{"id":"Ek63kxvpxp9U"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4171)"]},{"cell_type":"code","metadata":{"id":"8heUT6Qaxp9V"},"source":["#export\n","from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc6PTXcIxp9V"},"source":["train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n","valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URCJObCIxp9V"},"source":["model,opt = get_model()\n","fit()\n","loss_func(model(xb), yb), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RV_tHff0xp9W"},"source":["PyTorch's defaults work fine for most things however:"]},{"cell_type":"code","metadata":{"id":"m1-XnjlYxp9W"},"source":["train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n","valid_dl = DataLoader(valid_ds, bs, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hldX-jfTxp9W"},"source":["model,opt = get_model()\n","fit()\n","\n","loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wHMsZhExp9W"},"source":["Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."]},{"cell_type":"markdown","metadata":{"id":"IZdHSkBbxp9X"},"source":["## Validation"]},{"cell_type":"markdown","metadata":{"id":"KzRnuK0dxp9X"},"source":["You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n","\n","We will calculate and print the validation loss at the end of each epoch.\n","\n","(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"]},{"cell_type":"markdown","metadata":{"id":"gG-PcbGPxp9X"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4260)"]},{"cell_type":"code","metadata":{"id":"gZpIe-Jqxp9X"},"source":["def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        # Handle batchnorm / dropout\n","        model.train()\n","#         print(model.training)\n","        for xb,yb in train_dl:\n","            loss = loss_func(model(xb), yb.long())\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","\n","        model.eval()\n","#         print(model.training)\n","        with torch.no_grad():\n","            tot_loss,tot_acc = 0.,0.\n","            for xb,yb in valid_dl:\n","                pred = model(xb)\n","                tot_loss += loss_func(pred, yb.long())\n","                tot_acc  += accuracy (pred,yb)\n","        nv = len(valid_dl)\n","        print(epoch, tot_loss/nv, tot_acc/nv)\n","    return tot_loss/nv, tot_acc/nv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU-f2jwTxp9Y"},"source":["*Question*: Are these validation results correct if batch size varies?"]},{"cell_type":"markdown","metadata":{"id":"kavB4SYyxp9Y"},"source":["`get_dls` returns dataloaders for the training and validation sets:"]},{"cell_type":"code","metadata":{"id":"Ze8BCcJrxp9Z"},"source":["#export\n","def get_dls(train_ds, valid_ds, bs, **kwargs):\n","    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n","            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLfgd85jxp9Z"},"source":["Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"]},{"cell_type":"code","metadata":{"id":"1xdz5p95xp9a"},"source":["train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n","model,opt = get_model()\n","loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TM7lifJHxp9a"},"source":["assert acc>0.9"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-fmPWSBxp9b"},"source":["## Export"]},{"cell_type":"code","metadata":{"id":"LWl4Vw_Txp9b"},"source":["!python notebook2script.py 03_minibatch_training.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxGTEBHtxp9c"},"source":[""],"execution_count":null,"outputs":[]}]}