{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"03_annotated_mini_batch.ipynb","provenance":[],"collapsed_sections":["I-fmPWSBxp9b"],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b6f66565484c47018b7cbf87c9180d39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_43ead27e18ef426c93dc5f83ebfb4fe3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4a3509c5c7ad421b874d28d42be750d9","IPY_MODEL_37c80576251e4879a0e819a7d19ed437"]}},"43ead27e18ef426c93dc5f83ebfb4fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a3509c5c7ad421b874d28d42be750d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_80688a7a6f7e4b84bd064635d322ffd9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5654ecc1271f401487e682aa3f64539c"}},"37c80576251e4879a0e819a7d19ed437":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_65099fb4ca5d4585a1e7e1770ce70fe0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:22&lt;00:00, 432225.49it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75625d33aeb843a38c458e11e078973d"}},"80688a7a6f7e4b84bd064635d322ffd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5654ecc1271f401487e682aa3f64539c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65099fb4ca5d4585a1e7e1770ce70fe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75625d33aeb843a38c458e11e078973d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"395169bebb3a4ce88dda40687b006b13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_10fb0e6a89474a31b33c1c16aec037cb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f56fe60559b488e8d660a81d0fe76ca","IPY_MODEL_3f9891b9d7f340058763b8b6f9e8c330"]}},"10fb0e6a89474a31b33c1c16aec037cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f56fe60559b488e8d660a81d0fe76ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7d736c2ec3e04b96bcccebddd1e3a204","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bca1bc0d30d248f4b366fc6c1f19a209"}},"3f9891b9d7f340058763b8b6f9e8c330":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_88fc432c08494ed79debae360ee19e40","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:02&lt;00:00, 12981.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2ddc0c76ea644998dbcd9d333a833b6"}},"7d736c2ec3e04b96bcccebddd1e3a204":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bca1bc0d30d248f4b366fc6c1f19a209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"88fc432c08494ed79debae360ee19e40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e2ddc0c76ea644998dbcd9d333a833b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a11e98355830485a882b2e2e5dcdbb81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_00a06c335bab4fe98c096e678d85f26e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6dccbe67bec74a399d6125c059d671df","IPY_MODEL_2fc8acb1fbd94098aee1e4fbfebf8aa0"]}},"00a06c335bab4fe98c096e678d85f26e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dccbe67bec74a399d6125c059d671df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a0ffab5d8174e72a768259ba9612a2c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_389cabe0e01a46dc9087c5e3fb41dc2e"}},"2fc8acb1fbd94098aee1e4fbfebf8aa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f014d171484c46ddb25af16b2a7a205f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:01&lt;00:00, 1317022.99it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07a118ded56a4313a75c01d785bd69a7"}},"6a0ffab5d8174e72a768259ba9612a2c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"389cabe0e01a46dc9087c5e3fb41dc2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f014d171484c46ddb25af16b2a7a205f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"07a118ded56a4313a75c01d785bd69a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"88ab7f55a3c946cb9ee26be5b465f8ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fdaa92d1bee948de88564f614639435c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d39d63f7587245de865d7c8b395ddfe5","IPY_MODEL_089eb701cfca4cd8b75666beaef45d24"]}},"fdaa92d1bee948de88564f614639435c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d39d63f7587245de865d7c8b395ddfe5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ce5ac39324b84d8c8404047abcfe5830","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0fa57f8bfc524314b6e91dd169697d90"}},"089eb701cfca4cd8b75666beaef45d24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4350d3e6b4b342618c3e84070ff1fc23","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:19&lt;00:00, 267.25it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d58ecf8100fe438bbacb08eee4c30980"}},"ce5ac39324b84d8c8404047abcfe5830":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0fa57f8bfc524314b6e91dd169697d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4350d3e6b4b342618c3e84070ff1fc23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d58ecf8100fe438bbacb08eee4c30980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"GJ-wL42Lxp7g","executionInfo":{"status":"ok","timestamp":1619069200297,"user_tz":-540,"elapsed":3110,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"AO3i1A7iL--k"},"source":["\n","# cross entropy summation function\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdH_c8nnqn-c","executionInfo":{"status":"ok","timestamp":1619069200298,"user_tz":-540,"elapsed":3100,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"88be8d0f-b353-4072-e406-42f3f8ae1c77"},"source":["#Q1. Execute the following statement. What is displayed? What does it mean?\n","!pwd\n","# we can get the directory where we are working on\n","# if run the code, we will get /content"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jhAroxykqtRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069217686,"user_tz":-540,"elapsed":16233,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"2673acc8-a464-4b86-ca0e-f065c11b9d08"},"source":["#Q2: Execute the following statement. What happens?  Examine the left column of your colab page to see what happens.\n","!git clone https://github.com/fastai/course-v3.git\n","# github주소에서 해당 유알엘에 있는 파일 복사해오기\n","# we can get the whole fine from github with url. Then new folder that is course-v3 appeared in our directory"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'course-v3'...\n","remote: Enumerating objects: 5890, done.\u001b[K\n","remote: Total 5890 (delta 0), reused 0 (delta 0), pack-reused 5890\u001b[K\n","Receiving objects: 100% (5890/5890), 263.03 MiB | 35.07 MiB/s, done.\n","Resolving deltas: 100% (3249/3249), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FOZb4AeKq6vQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069222057,"user_tz":-540,"elapsed":739,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"16297f5e-e671-430f-af2b-d666bc884ab9"},"source":["#Q3. Execute the following statement. What are displayed by the execution of sys.path? What do it mean?\n","import sys\n","sys.path.append('/content/course-v3/nbs/dl2')\n","sys.path\n","## we can add a import path. sys.path shows us paths where we can upload python package to use by python system.\n","# python system find the name which we import in those area. So if we add specific path, we can import files from it\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/course-v3/nbs/dl2']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"I86djNvcxp70","executionInfo":{"status":"ok","timestamp":1619069236121,"user_tz":-540,"elapsed":4831,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#export\n","#Q4: \"from exp.nb_01 import *\" imports classes, functions, etc from module nb_01 in package exp.\n","# Q4a.   how do you know that exp is a package?\n","# Q4b.   How is the Python system able to get access to package exp? That is, how can it search for it?\n","from exp.nb_02 import *\n","import torch.nn.functional as F\n","## if there is __init__.py in a folder, python recognize it as package.\n","## python search the directories based on our sys.path. If it finds the directory, it searches the sub-directory and so forth until finding our import file or class"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iywbFmpbxp73"},"source":["## Initial setup"]},{"cell_type":"markdown","metadata":{"id":"G5DBpkZWxp75"},"source":["### Data"]},{"cell_type":"markdown","metadata":{"id":"Q_7vJLX_xp75"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=1786)"]},{"cell_type":"code","metadata":{"id":"xg8JmuuDLwmt","executionInfo":{"status":"ok","timestamp":1619069238910,"user_tz":-540,"elapsed":621,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["from IPython.display import Image\n","from six.moves import urllib\n","\n","opener = urllib.request.build_opener()\n","opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n","urllib.request.install_opener(opener)\n","\n","def get_data():\n","    import os\n","    import torchvision.datasets as datasets\n","    root = '../data'\n","    if not os.path.exists(root):\n","        os.mkdir(root)\n","\n","    train_set = datasets.MNIST(root=root, train=True, download=True)\n","    test_set = datasets.MNIST(root=root, train=False, download=True) #load validation set\n","    x_train, x_valid = train_set.data.split([50000, 10000])\n","    y_train, y_valid = train_set.targets.split([50000, 10000])\n","    return (x_train.view(50000, -1) / 256.0), y_train.float(), (x_valid.view(10000, -1))/ 256.0, y_valid.float()\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRKV-q7Kxp76","executionInfo":{"status":"ok","timestamp":1619069241283,"user_tz":-540,"elapsed":880,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["mpl.rcParams['image.cmap'] = 'gray'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6um-ZEMxp76","colab":{"base_uri":"https://localhost:8080/","height":870,"referenced_widgets":["b6f66565484c47018b7cbf87c9180d39","43ead27e18ef426c93dc5f83ebfb4fe3","4a3509c5c7ad421b874d28d42be750d9","37c80576251e4879a0e819a7d19ed437","80688a7a6f7e4b84bd064635d322ffd9","5654ecc1271f401487e682aa3f64539c","65099fb4ca5d4585a1e7e1770ce70fe0","75625d33aeb843a38c458e11e078973d","395169bebb3a4ce88dda40687b006b13","10fb0e6a89474a31b33c1c16aec037cb","0f56fe60559b488e8d660a81d0fe76ca","3f9891b9d7f340058763b8b6f9e8c330","7d736c2ec3e04b96bcccebddd1e3a204","bca1bc0d30d248f4b366fc6c1f19a209","88fc432c08494ed79debae360ee19e40","e2ddc0c76ea644998dbcd9d333a833b6","a11e98355830485a882b2e2e5dcdbb81","00a06c335bab4fe98c096e678d85f26e","6dccbe67bec74a399d6125c059d671df","2fc8acb1fbd94098aee1e4fbfebf8aa0","6a0ffab5d8174e72a768259ba9612a2c","389cabe0e01a46dc9087c5e3fb41dc2e","f014d171484c46ddb25af16b2a7a205f","07a118ded56a4313a75c01d785bd69a7","88ab7f55a3c946cb9ee26be5b465f8ba","fdaa92d1bee948de88564f614639435c","d39d63f7587245de865d7c8b395ddfe5","089eb701cfca4cd8b75666beaef45d24","ce5ac39324b84d8c8404047abcfe5830","0fa57f8bfc524314b6e91dd169697d90","4350d3e6b4b342618c3e84070ff1fc23","d58ecf8100fe438bbacb08eee4c30980"]},"executionInfo":{"status":"ok","timestamp":1619069255341,"user_tz":-540,"elapsed":6347,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"1099747c-c485-4018-c7ff-183e077aaae6"},"source":["#Q5: when you execute the following statement, where is the downloaded data stored? Examine the left column of your colab page.\n","x_train,y_train,x_valid,y_valid = get_data()\n","##  if we download, they are stored in the root path which we specified \"../data\"\n","## .. means parent-directory. If we are in content then,finding from !pwd , python go up to parent directory which is '/' then make the data directory and finally save the datas in that directory\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6f66565484c47018b7cbf87c9180d39","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"395169bebb3a4ce88dda40687b006b13","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a11e98355830485a882b2e2e5dcdbb81","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 503: Service Unavailable\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88ab7f55a3c946cb9ee26be5b465f8ba","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HKu9tMbvBbfo"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwBgUYjX1Gbc","executionInfo":{"status":"ok","timestamp":1619069266620,"user_tz":-540,"elapsed":639,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c5aab40a-2f20-40af-c2bd-c33aaaef2671"},"source":["y_train.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"oHcenWs_rZ_I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069267997,"user_tz":-540,"elapsed":461,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"9c36f0fa-f712-472b-c9f4-32a05a06dfb8"},"source":["#Q6a: Execute the following statement. What does the number displayed mean?\n","len(x_train)\n","## 데이터샘플 개수\n","# we can get the number of data sampels which is first shape of x_train"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"mxSwa7g5rcer","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069270326,"user_tz":-540,"elapsed":616,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"3ec211e5-e36b-493d-9528-abcfa8a12fa6"},"source":["#Q6b: Execute the following statement. What does the numbers displayed mean?\n","y_train[0]\n","## 이미지 라벨\n","# we can get the first image's label which is the answer for the image.\n","# To know why it is 5, mnists dataset's answers consist of 0~9, digit number. And the first image represents 5.\n","# thats why it is 5."],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"DESekbEYxp77","executionInfo":{"status":"ok","timestamp":1619069272274,"user_tz":-540,"elapsed":770,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#Q7: Display the values of n,m, c, and nh. What are they? For what are they used in the following code? \n","n,m = x_train.shape\n","c = y_train.max()+1\n","nh = 50\n","# n피쳐데이터, m샘플데이터 - , c는 마지막에 사용\n","# if we have neural net consisting of layer1,layer2,\n","# n is the number of feature of each sample which is used for layer1 weight matrix multiplication\n","# m is the number of samples, and it's dimension,50000, will kept throughout the whole neural net layers\n","# nh is the number of hidden nodes for each sample, especially for layer 1's.\n","# c is the kinds of labels,10, which we want to predict. It is the output number of layer2's node for each sample"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cCzC9iZxp77","executionInfo":{"status":"ok","timestamp":1619069274157,"user_tz":-540,"elapsed":618,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#Q8: The following defines Model class, which will be used to create a neural net.\n","#Q8a: What is nn.Module?, What is nn.Linear? What is nn.ReLU? \n","#Q8b. What is the difference between  nn.Linear itself and nn.Linear(n_in,nh) ? \n","\n","class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x\n","#  \n","# nn.linear is linear layer which makes fully conntected layer applying linear transformation from input to output\n","# nn.Relu is the activation layer using Relu which works as max(0,x)\n","# Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n","# nn.Module is the basic neural net module like a basket. After adding layers to that module, we can construct the neural net\n","# nn.Module encapsulate the parmeters, helping to use gpus,exporting,loading etc like a helper for making model class\n","# We often inherit this module when we make model class\n","# nn.Linear is class itself while nn.Linear(n_in,nh) means initializing the class to instance. It executes __init__ with parameters."],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJx4LIA1xp78","executionInfo":{"status":"ok","timestamp":1619069295844,"user_tz":-540,"elapsed":639,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#Q9a. When Model(m,nh, 10) is executed, which method in Model class is executed?\n","model = Model(m, nh, 10)\n","# __init__\n","# If we initialize the class, __init__ function will be executed.\n","# In general, they declare the variables with parameters."],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPFt346vxp78","executionInfo":{"status":"ok","timestamp":1619069303585,"user_tz":-540,"elapsed":638,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#Q10a. Is model a function or an object? If it is an object, how is it used as if it is a function?\n","# When model(x_train) is executed, which method in Model is executed. What does this method create? \n","pred = model(x_train)\n","# After we initialize the class, we can also execute with parameters using __call__ function\n","# In nerual net model, we often predict the result with train data input using __call__ function"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXbLts9ZRMgd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069305287,"user_tz":-540,"elapsed":868,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"5f12dc51-2606-4559-b33c-a99358466e22"},"source":["#Q11: Execute the following statement. What does the displayed numbers mean?\n","pred[0]\n","# pred[0] means the degree of being 10 labels each for first data sample. In general, we select the prediction label with biggest number\n","# Because we have 10 labels, there are also 10 numbers.\n","# for example, pred[0][0] means how much likely to be the label 0."],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.0412, -0.0449, -0.2077,  0.0361,  0.2382,  0.2453, -0.1610, -0.1783,\n","         0.0115,  0.0708], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"-Ejrm01LRSxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069359071,"user_tz":-540,"elapsed":983,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"1f67ab75-f020-474f-8d92-5f9e5c2a8393"},"source":["#Q12. Execute the following statement. Is the resulting value near to 1? If not, what does it imply?\n","pred[0].sum()\n","# Of course not, it is just likely weight for each label not the proability.\n","# So if we want to apply statstic proability concept for predicting label, we need to convert those number to probability"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.0313, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"qhbGAsGWxp79"},"source":["### Cross entropy loss"]},{"cell_type":"markdown","metadata":{"id":"fFGQT0zpxp79"},"source":["First, we will need to compute the softmax of our activations. This is defined by:\n","\n","$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n","\n","or more concisely:\n","\n","$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n","\n","In practice, we will need the log of the softmax when we calculate the loss."]},{"cell_type":"code","metadata":{"id":"Q3ZRxbanxp7_","executionInfo":{"status":"ok","timestamp":1619069362071,"user_tz":-540,"elapsed":731,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True)))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wo_n_z36r77v","executionInfo":{"status":"ok","timestamp":1619069363151,"user_tz":-540,"elapsed":1016,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["sm_pred = softmax(pred)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnURmMOHsA2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069364097,"user_tz":-540,"elapsed":617,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c7a20b42-f05d-4885-c254-96a6f531b0a1"},"source":["#Q13. Execute the following statement. What does the displayed numbers mean?\n","sm_pred[0]\n","# each number means probability of each label being correct.\n","# for exmaple, if sm_pred[0][0] is 0.3, the proability of the image being 0 is 0.3\n","# Also their number arranges from 0 ~ 1 which is Probabilistic interval"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0952, 0.0948, 0.0806, 0.1028, 0.1258, 0.1267, 0.0844, 0.0830, 0.1003,\n","        0.1064], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"MvUVm4qpsQ80","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069371098,"user_tz":-540,"elapsed":668,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"92919ee2-bdfd-4ccf-bb0c-63cc9539d2b4"},"source":["#Q14. Execute the following statement. What does the displayed numbers mean?\n","sm_pred[0].sum()\n","# this should be 1 if it is subordinate with probability.\n","# this means each number gives us the information about how much probable each label would be the answer."],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0000, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"DNuWma6zr00h","executionInfo":{"status":"ok","timestamp":1619069373380,"user_tz":-540,"elapsed":633,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDeeKw-Sxp8A","executionInfo":{"status":"ok","timestamp":1619069375638,"user_tz":-540,"elapsed":625,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["log_sm_pred = log_softmax(pred)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJmGXhPfRYba","executionInfo":{"status":"ok","timestamp":1619069376942,"user_tz":-540,"elapsed":788,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"1d6e42f3-6789-4ee6-ac9d-45a60a1b41fe"},"source":["log_sm_pred[0]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.3522, -2.3559, -2.5187, -2.2749, -2.0728, -2.0658, -2.4721, -2.4894,\n","        -2.2996, -2.2402], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"JRmNSz1vRb0N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069379986,"user_tz":-540,"elapsed":617,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e8540f1d-de77-489d-bcd8-b29398a398a3"},"source":["log_sm_pred[0].sum()"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-23.1415, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"DfNuX2v1xp8B"},"source":["\n","\n","## Q15. Read the following paragraph to understand what the cross entroy loss functioni is. \n","\n","The difference between probabilities interpretation: https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation\n","\n","The cross entropy formula takes in two distributions,  $p(x^{(s)})$, the true distribution (defined by the label data), and $\\hat{p}(x^{(s)})$, the estimated distribution (predicted by the neural net), defined over the discrete variable x and is given by\n","\n","$H(p,\\hat{p})=−\\sum_{s \\in B} p(x^{(s)}) \\cdot log(\\hat{p}(x^{(s)}))$\n","\n","\n","In general, $ p(x^{(s)}) = [ p_{1} (x^{(s)}), ..., p_{n}(x^{(s)})]$ is a probability distribution over a set of categories. \n","But since our $ p(x^{(s)})$ are 1-hot encoded, that is, in the form of  $ p(x^{(s)}) =[0,0,..,0,1,0..]$, where the probability of only one category is one and those of the other categories are all zero, this can be rewritten as\n","\n"," $-\\sum_{s \\in B} [ 0*\\log(\\hat{p}_{1} (x^{(s)}))\n"," + 1*\\log(\\hat{p}_{i} (x^{(s)}) ) +..+0*\\log(\\hat{p}_{n} (x^{(s)}) ) ] \\\\\n","  = -\\sum_{s \\in B} \\log(\\hat{p}_{i(s)} ) (x^{(s)}) ) \n","  \\tag{crossEntroyEq}$ \n","\n","#  In this notebook, the softmax function plays the role of the probability distribution $\\hat{p}_{1} (x^{(s)})$.\n","\n","  Here $i(s)$ is the index of the one-hot  probability distribution $p(x^{(s)})$ where the probability is one. \n"," \n"," "]},{"cell_type":"markdown","metadata":{"id":"KBtZhgePxp8D"},"source":["This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."]},{"cell_type":"code","metadata":{"id":"3R_y0cP_xp8D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069406669,"user_tz":-540,"elapsed":628,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"f03e5a75-b79e-43a0-a423-b798dac4c54b"},"source":["#Q15a. Execute the following statement. What do the displayed numbers mean? \n","y_train[:3]\n","# This number means that for 3 sample datas the images represents 5,0,4 each\n","# it does not coverted into one-hot vector yet. For eaxample, [0,0,0,0,0,1,0,0,0,0] for 5\n","# meaning probability of being numbers 0 through 9 except 5 is 0%, and prability of being 5 is 100%"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5., 0., 4.])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VehY657jB7IB","executionInfo":{"status":"ok","timestamp":1619069408776,"user_tz":-540,"elapsed":729,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"bcbc14fe-b67d-4589-e693-2dc462ed3975"},"source":["#Q15b.  Execute the following statement. What do the displayed numbers mean? \n","log_sm_pred[ [0,1,2]]\n","# They represent the log_softmax numbers of being the answer 0 to 9 each for 3 samples.\n","# Becuase there are 10 kinds of answers, there are 10 elements for each vector.\n","# It is calculated by softmax and log.\n","# First we calcuate probability of each label using softmax, then we apply the log \n","# why we use log is to get benefit from caculation in proability and\n","# it makes more penalty on big loss than softmax which makes the model training efficient.\n"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-2.3522, -2.3559, -2.5187, -2.2749, -2.0728, -2.0658, -2.4721, -2.4894,\n","         -2.2996, -2.2402],\n","        [-2.2425, -2.3675, -2.5629, -2.2829, -2.0346, -2.0615, -2.4655, -2.4647,\n","         -2.3155, -2.3611],\n","        [-2.2945, -2.3563, -2.4126, -2.2710, -2.1739, -2.1068, -2.3705, -2.4384,\n","         -2.3605, -2.2911]], grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"uJM_9p0rxp8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069411111,"user_tz":-540,"elapsed":431,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"59a8c603-7320-4c5a-b074-6fc453a93c72"},"source":["#Q15c.  Execute the following statement. What do the displayed numbers mean? Explain compare the result of Q15b and the result of this statement.\n","log_sm_pred[[0,1,2], [5,0,4]]\n","# The numbers means first sample data is log_softmax values likely to be 5 and second sample data is likely to be 0 and so forth each.\n","# We know the answer for each sample, so ideal number for each is 0, because log(1) = 0\n","# If the number is not closest to 0, we need to train more until certain point\n","# If we calcuate the loss with 15b, lots of computing resources are wasted. Because all probability except the answer becomes 0 in one-hot label data, we don't\n","# need the other numbers. For example, if we calcuate the loss of first sample, we need only log_sm_red[0][5], where answer is 5, to mulitply when we uses cross-entropy loss.\n","# not the whole vector output of first sample which is log_sm_red[0]\n","# So to be efficient loss calculation with cross_entropy, 15c is desirable."],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.0658, -2.2425, -2.1739], grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"oSK1T-mgxp8I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069413183,"user_tz":-540,"elapsed":629,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"291b0359-1ed4-40ba-b7ff-2e322f101ca0"},"source":["y_train.shape[0]"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"-ogBL3jZxPPN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069414447,"user_tz":-540,"elapsed":539,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c3ffdee7-8939-4a94-af83-3d586ecbddc8"},"source":["y_train"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5., 0., 4.,  ..., 8., 4., 8.])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"h1bTmfz5xp8J"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2081)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPJ9QjlLQQc_","executionInfo":{"status":"ok","timestamp":1619069417224,"user_tz":-540,"elapsed":610,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"cc99b211-21ae-4423-f4a7-d12cb0257cc2"},"source":["log_sm_pred[0]"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.3522, -2.3559, -2.5187, -2.2749, -2.0728, -2.0658, -2.4721, -2.4894,\n","        -2.2996, -2.2402], grad_fn=<SelectBackward>)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoc2YwU5wATE","executionInfo":{"status":"ok","timestamp":1619069421320,"user_tz":-540,"elapsed":750,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"adeafe86-56eb-4f17-95f0-9850775b030c"},"source":["range(y_train.long()[0:10].shape[0])"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["range(0, 10)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8GMBhmq9w4QG","executionInfo":{"status":"ok","timestamp":1619069431441,"user_tz":-540,"elapsed":693,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"2e3f9ecd-f29c-4714-80a6-4fb8f0bf938a"},"source":["log_sm_pred[ range(y_train.long()[0:10].shape[0]), y_train.long()[0:10] ]"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.0658, -2.2425, -2.1739, -2.3419, -2.3175, -2.4383, -2.4129, -2.2841,\n","        -2.3639, -2.2075], grad_fn=<IndexBackward>)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"ZI0cXSLRxp8K","executionInfo":{"status":"ok","timestamp":1619069435826,"user_tz":-540,"elapsed":662,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def nll(input, target):\n","   \n","   return -input[range(target.shape[0]), target].mean()"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzmLXXWiPTSF","executionInfo":{"status":"ok","timestamp":1619069438509,"user_tz":-540,"elapsed":787,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"42a2d3da-16ba-4b86-fd44-69901f6d89a0"},"source":["y_train.long().shape[0]"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"_qRbZvGePbHV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069439671,"user_tz":-540,"elapsed":629,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e18e8b21-5303-43a1-87f5-3691e00152fc"},"source":["x_train.shape"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000, 784])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Bs5UU4fyxp8K","executionInfo":{"status":"ok","timestamp":1619069442241,"user_tz":-540,"elapsed":743,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#Q16. Read nill function and explain how this function computes the result. You need to refer to {CrossEntropyLoss}.\n","loss = nll(log_sm_pred, y_train.long())\n","# first we infer the log proability of each label for each image using log_softmax.\n","# then using crossentropy loss, we calculate the loss of each sample and get the average of total samples' loss\n","# if we calculate crossentropy loss, we only use the logsoftmax value on answer index from prediction of model to save resources.\n","# since,except the answer index value of answer-one hot encoding, there are all zero which are used to mulitply with output of model on each value"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrPFVa91P0EO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069443617,"user_tz":-540,"elapsed":604,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e927b0d8-b2d4-4659-f13f-265f7d2a5561"},"source":["y_train"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5., 0., 4.,  ..., 8., 4., 8.])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"6uzxkFQaPv-W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069445607,"user_tz":-540,"elapsed":815,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e30450ea-f2cb-4bd8-9a5c-286401962a96"},"source":["y_train.long()"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 0, 4,  ..., 8, 4, 8])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"zkSkgQYQxp8L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069447700,"user_tz":-540,"elapsed":676,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"010fcd3a-f885-4e60-93a9-93f41572a130"},"source":["loss"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3193, grad_fn=<NegBackward>)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"rHyuP5t7xp8M"},"source":["Note that the formula \n","\n","$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n","\n","gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"]},{"cell_type":"code","metadata":{"id":"veHLG6DXxp8M","executionInfo":{"status":"ok","timestamp":1619069509141,"user_tz":-540,"elapsed":1250,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"aelGhkV5xp8N","executionInfo":{"status":"ok","timestamp":1619069510070,"user_tz":-540,"elapsed":630,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["test_near(nll(log_softmax(pred), y_train.long()), loss)"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVPZMaTcxp8O"},"source":["Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n","\n","$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n","\n","where a is the maximum of the $x_{j}$."]},{"cell_type":"code","metadata":{"id":"No5iHYvCxp8P","executionInfo":{"status":"ok","timestamp":1619069697473,"user_tz":-540,"elapsed":624,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def logsumexp(x):\n","    print(x[0])\n","    print(x[0].max(-1))\n","    m = x.max(-1)[0]\n","    print(m[0])\n","    return m + (x-m[:,None]).exp().sum(-1).log()"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpzazYXDxp8Q"},"source":["This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "]},{"cell_type":"code","metadata":{"id":"dQ_VQU0Sxp8Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619069699847,"user_tz":-540,"elapsed":613,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"41c40a76-01c7-4b63-92e7-12f9bbd62d3d"},"source":["test_near(logsumexp(pred), pred.logsumexp(-1))"],"execution_count":53,"outputs":[{"output_type":"stream","text":["tensor([-0.0412, -0.0449, -0.2077,  0.0361,  0.2382,  0.2453, -0.1610, -0.1783,\n","         0.0115,  0.0708], grad_fn=<SelectBackward>)\n","torch.return_types.max(\n","values=tensor(0.2453, grad_fn=<MaxBackward0>),\n","indices=tensor(5))\n","tensor(0.2453, grad_fn=<SelectBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wkGCt3oDxp8R"},"source":["So we can use it for our `log_softmax` function."]},{"cell_type":"code","metadata":{"id":"q7nsaEFtxp8R","executionInfo":{"status":"ok","timestamp":1619069719094,"user_tz":-540,"elapsed":603,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt1PreQAxp8S","executionInfo":{"status":"ok","timestamp":1619069720622,"user_tz":-540,"elapsed":946,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["test_near(nll(log_softmax(pred), y_train.long()), loss)"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcdHBuPOxp8S"},"source":["Then use PyTorch's implementation."]},{"cell_type":"code","metadata":{"id":"U609ws15xp8T","executionInfo":{"status":"ok","timestamp":1619069735753,"user_tz":-540,"elapsed":603,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["test_near(F.nll_loss(F.log_softmax(pred, -1), y_train.long()), loss)"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yOo22f7Exp8U"},"source":["In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."]},{"cell_type":"code","metadata":{"id":"lU7uCHBwxp8U","executionInfo":{"status":"ok","timestamp":1619070006845,"user_tz":-540,"elapsed":652,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["test_near(F.cross_entropy(pred, y_train.long()), loss)"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdFUzc94xp8V"},"source":["## Basic training loop"]},{"cell_type":"markdown","metadata":{"id":"495DuZ5Xxp8V"},"source":["Basically the training loop repeats over the following steps:\n","- get the output of the model on a batch of inputs\n","- compare the output to the labels we have and compute a loss\n","- calculate the gradients of the loss with respect to every parameter of the model\n","- update said parameters with those gradients to make them a little bit better"]},{"cell_type":"markdown","metadata":{"id":"6e6Y1AKyxp8W"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2542)"]},{"cell_type":"code","metadata":{"id":"fDHaEy3oxp8X","executionInfo":{"status":"ok","timestamp":1619070008516,"user_tz":-540,"elapsed":456,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["loss_func = F.cross_entropy # This criterion combines log_softmax and nll_loss in a single function."],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcuWlwDXxp8Y","executionInfo":{"status":"ok","timestamp":1619070073382,"user_tz":-540,"elapsed":586,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["#export\n","def accuracy(out, yb): \n","    print(out[0])\n","    print(torch.argmax(out, dim=1))\n","    return (torch.argmax(out, dim=1)==yb).float().mean()"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozBbmfr7xp8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070087145,"user_tz":-540,"elapsed":598,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"91f40a33-4072-4c57-dc94-15f375dcb8e1"},"source":["bs=64                  # batch size\n","\n","xb = x_train[0:bs]     # a mini-batch from x\n","preds = model(xb)      # predictions\n","preds[0], preds.shape"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([-0.0412, -0.0449, -0.2077,  0.0361,  0.2382,  0.2453, -0.1610, -0.1783,\n","          0.0115,  0.0708], grad_fn=<SelectBackward>), torch.Size([64, 10]))"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"Ue2zaDOLxp8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070087997,"user_tz":-540,"elapsed":687,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"a128ce93-6a6c-4bea-d2eb-e058c761555d"},"source":["yb = y_train[0:bs]\n","loss_func(preds, yb.long())"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3114, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"vYSsK_Rlxp8b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070088907,"user_tz":-540,"elapsed":625,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"65a8967b-68d2-4a8c-c537-cc8f82dbacd7"},"source":["accuracy(preds, yb)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["tensor([-0.0412, -0.0449, -0.2077,  0.0361,  0.2382,  0.2453, -0.1610, -0.1783,\n","         0.0115,  0.0708], grad_fn=<SelectBackward>)\n","tensor([5, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 3, 4, 5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 4,\n","        9, 4, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 4, 5, 4, 5, 4, 4, 3, 5, 4, 4, 4, 4,\n","        5, 4, 4, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 4])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor(0.0312)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"dxLMr6u-xp8c","executionInfo":{"status":"ok","timestamp":1619070093041,"user_tz":-540,"elapsed":613,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["lr = 0.5   # learning rate\n","epochs = 1 # how many epochs to train for"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9-hKTgAOFvu0"},"source":["# The mechanism for training the network: (1) computing the graidents of Loss with respect to tensors at each layer, (2) backpropgation: applying the chain rule to compute the gradient vector of the parameters, (3) updating the parameters.\n","\n","https://github.com/pytorch/pytorch/blob/35bd2b3c8b64d594d85fc740e94c30aa67892a34/torch/tensor.py\n","\n","https://github.com/pytorch/pytorch/blob/35bd2b3c8b64d594d85fc740e94c30aa67892a34/torch/tensor.py\n","\n","https://stackoverflow.com/questions/57248777/backward-function-in-pytorch\n",": Pytorch does not support this non-scalar function derivatives. Instead, pytorch assumes out is only an intermediate tensor and somewhere \"upstream\" there is a scalar loss function, that through chain rule provides d loss/ d out[i,j]. This \"upstream\" gradient is of size 2-by-3 and this is actually the argument you provide backward in this case: out.backward(g) where g_ij = d loss/ d out_ij.\n","\n","The gradients are then calculated by chain rule d loss / d a[i,j] = (d loss/d out[i,j]) * (d out[i,j] / d a[i,j]); In fact,  So it's not just (d loss/d out[i,j]) * (d out[i,j] / d a[i,j]), but in fact sum_{k,l} (d loss/d out[k,l]) * (d out[k,l] / d a[i,j]).\n","\n","# The \"gradient accumulation\"\n","\n","https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/9\n","\n","In the following example, y.backward() is called 5 times, so the final value of x.grad will be 5*cos(0)=5.\n","\n","import torch\n","from torch.autograd import Variable\n","\n","x = Variable(torch.Tensor([[0]]), requires_grad=True)\n","\n","for t in range(5):\n","    y = x.sin() \n","    y.backward()\n","    \n","print(x.grad) # shows 5\n","Calling x.grad.data.zero_() before y.backward() can make sure x.grad is exactly the same as current y’(x), not a sum of y’(x) in all previous iterations.\n","\n","x = Variable(torch.Tensor([[0]]), requires_grad=True) \n","\n","for t in range(5):\n","    if x.grad is not None:\n","        x.grad.data.zero_()\n","    y = x.sin() \n","    y.backward()\n","\n","print(x.grad) # shows 1\n","I also got confused by this “zeroing gradient” when first learning pytorch. The doc of torch.autograd.backward does mention that\n","\n","This function accumulates gradients in the leaves - you might need to zero them before calling it.\n","\n","But this is quite hard to find and pretty confusing for (say) tensorflow users.\n","\n","Official tutorials like 60 Minute Blitz 45 or PyTorch with Examples 69 both say nothing about why one needs to call grad.data.zero_() during training. I think it would be useful to explain this a little more in beginner-level tutorials. RNN is a good example for why accumulating gradient (instead of refreshing) is useful, but I guess new users wouldn’t even know that backward() is accumulating gradient\n","\n","\n","# A more rigorous reference to autograd:\n","\n","https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n","\n","torch.no_grad():\n","\n","When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later.\n","\n","The gradient of  \n","b\n","=\n","w\n","1\n","∗\n","a\n"," w.r.t it's inputs \n","w\n","1\n"," and \n","a\n"," is \n","a\n"," and \n","w\n","1\n"," respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network.\n","\n","While, we are performing inference, we don't compute gradients, and thus, don't need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.\n","\n","PyTorch offers a context manager, called torch.no_grad for this purpose.\n","\n","\n","with torch.no_grad:\n","\n","\tinference code goes here \n","\n","No graph is defined for operations executed under this context manager.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-fBlf5P4US6","executionInfo":{"status":"ok","timestamp":1619070140846,"user_tz":-540,"elapsed":1215,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e47d46ea-55c0-4ae8-f1c9-7abd8f4daab9"},"source":["from torch.autograd import Variable\n","import torch\n","x = Variable(torch.Tensor([[0]]), requires_grad=True)\n","\n","for t in range(5): \n","    y = x.sin()\n","    y.backward()\n","\n","    print(x.grad)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["tensor([[1.]])\n","tensor([[2.]])\n","tensor([[3.]])\n","tensor([[4.]])\n","tensor([[5.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hOxLxsk304tO","executionInfo":{"status":"ok","timestamp":1619070150936,"user_tz":-540,"elapsed":684,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n","bs=64                  # batch size\n","\n","xb = x_train[0:bs]     # a mini-batch from x\n","cost_function = F.cross_entropy"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2VMS0nr0_BK","executionInfo":{"status":"ok","timestamp":1619070153633,"user_tz":-540,"elapsed":567,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["lr = 0.5   # learning rate\n","epochs = 2 # how many epochs to train for"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0KH3m-Uxp8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070549132,"user_tz":-540,"elapsed":707,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"8df6e199-037f-442c-db16-fc1048054a69"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1): # for each batch in the current epoch\n","#         set_trace()\n","        start_i = i*bs\n","        end_i = start_i+bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","\n","        ybHat = model(xb)\n","        h = ybHat.register_hook( lambda grad: print (grad) )\n","\n","        loss = loss_func( ybHat, yb.long())\n","\n","        # loss.backward() computes the gradient of loss w.r.t. graph LEAVES.\n","        #  This function accumulates gradients in the leaves - you might need to zero\n","        # ``.grad`` attributes or set them to ``None`` before calling it.\n","        #  (This makes it easier for rnn, because each module will be back propogated through several times.)\n","       \n","       # Attribute grad of tensor ybHat (ybHat.grad = dloss/dybHat) is ``None`` by default and becomes a Tensor the first time a call to\n","        #func `backward` computes gradients for ybHat.\n","        #The attribute will then contain the gradients computed and future calls to\n","        #:func:`backward` will accumulate (add) gradients into it.\n","\n","        #Since the backward() function accumulates gradients, and you don’t want to mix up gradients between minibatches, \n","        # you have to zero them out at the start of a new minibatch.\n","       \n","\n","\n","        ybHat.grad\n","        loss.backward() # When you call loss.backward(), all it does is compute gradient of loss \n","                        # w.r.t all the parameters in loss that have requires_grad = True and store them in parameter.grad attribute for every parameter\n","\n","        ybHat.grad                \n","        #optimizer.step() updates all the parameters based on parameter.grad \n","        with torch.no_grad():\n","            for l in model.layers:\n","                if hasattr(l, 'weight'):\n","                    l.weight -= l.weight.grad * lr\n","                    l.bias   -= l.bias.grad   * lr\n","\n","                    #we don't care about gradients from the previous batch. Not zeroing grads would lead to gradient accumulation across batches\n","                    l.weight.grad.zero_()\n","                    l.bias  .grad.zero_()"],"execution_count":76,"outputs":[{"output_type":"stream","text":["tensor([[ 2.8639e-11,  5.0197e-12,  7.6480e-13,  4.6426e-06,  7.8007e-18,\n","         -4.6715e-06,  2.3612e-13,  1.5880e-13,  1.7212e-14,  3.0065e-08],\n","        [-4.8429e-08,  1.1404e-12,  3.4232e-08,  5.8735e-12,  8.2275e-11,\n","          8.4376e-13,  2.9224e-09,  6.7393e-09,  2.1853e-11,  3.6548e-09],\n","        [ 5.8160e-08,  1.9815e-07,  3.4483e-05,  1.7391e-06, -5.6699e-05,\n","          6.6204e-08,  8.8445e-06,  7.2992e-06,  9.1404e-07,  3.0959e-06],\n","        [ 1.1090e-09, -3.0613e-06,  3.7656e-07,  6.4584e-09,  1.1344e-08,\n","          7.8872e-09,  7.3895e-09,  2.1007e-06,  5.5162e-07,  7.5181e-11],\n","        [ 8.1127e-09,  4.7647e-09,  6.2809e-09,  5.9257e-08,  1.0071e-04,\n","          2.7890e-07,  6.1293e-09,  1.2049e-05,  1.8679e-05, -1.3180e-04],\n","        [ 9.3747e-07,  1.7583e-07, -5.5453e-05,  2.1503e-06,  2.0898e-06,\n","          1.6474e-06,  1.4952e-05,  6.5125e-07,  2.9420e-05,  3.4269e-06],\n","        [ 1.4793e-11, -3.7039e-06,  2.7603e-08,  1.0904e-06,  4.3990e-07,\n","          2.7844e-09,  9.1467e-08,  9.0458e-07,  1.0742e-06,  7.3029e-08],\n","        [ 3.6657e-12,  1.0355e-13,  6.1647e-11, -3.7253e-08,  3.1275e-17,\n","          1.2174e-09,  4.3007e-17,  5.5823e-12,  9.3097e-11,  3.5160e-08],\n","        [ 4.6335e-10, -1.5317e-05,  6.1187e-08,  1.9282e-06,  4.1076e-07,\n","          9.4652e-08,  1.4297e-07,  6.7775e-06,  3.8876e-06,  2.0132e-06],\n","        [ 2.3599e-11,  9.6002e-12,  1.2248e-08,  4.5621e-11, -2.4773e-07,\n","          3.4790e-10,  2.2536e-07,  1.5094e-09,  7.2851e-09,  4.8524e-10],\n","        [ 7.1628e-12,  5.2755e-08,  2.4895e-10, -1.6764e-07,  6.3132e-12,\n","          2.5345e-08,  6.7936e-15,  4.1651e-12,  6.2978e-11,  8.9152e-08],\n","        [ 1.8937e-07,  1.7215e-05,  1.4623e-06,  3.1405e-07,  2.9634e-07,\n","         -4.1560e-05,  9.1998e-06,  1.8477e-07,  1.2701e-05,  4.6779e-11],\n","        [ 3.0285e-15,  1.4858e-09,  2.8847e-12, -1.8626e-09,  2.7353e-15,\n","          3.6459e-11,  8.7230e-19,  5.8697e-15,  1.2092e-13,  5.0028e-11],\n","        [ 3.0393e-09,  1.3982e-09,  3.9644e-09,  5.5778e-10,  2.0247e-10,\n","          1.3710e-06, -1.5646e-06,  9.8715e-10,  1.8215e-07,  7.6663e-13],\n","        [ 1.7427e-10, -6.9058e-06,  3.2708e-08,  7.4170e-07,  5.5670e-08,\n","          1.1405e-07,  8.5357e-08,  1.4802e-06,  4.2608e-06,  1.3396e-07],\n","        [ 1.3714e-10,  2.3392e-12,  8.3262e-09,  1.3499e-10,  4.7736e-08,\n","          5.1247e-09,  5.3978e-12, -1.4738e-05,  9.3633e-08,  1.4582e-05],\n","        [ 7.4172e-06,  2.2497e-07, -1.4568e-04,  2.0393e-06,  5.7018e-06,\n","          5.0194e-09,  2.4228e-05,  9.2178e-05,  8.0906e-06,  5.7963e-06],\n","        [ 4.1560e-08,  4.9974e-07,  5.8412e-09,  6.9651e-09,  1.1702e-07,\n","          6.6318e-09,  6.2311e-09,  6.4404e-08, -8.3819e-07,  8.9546e-08],\n","        [ 3.3761e-05,  1.2848e-05,  1.4135e-06,  3.9545e-06,  3.1082e-06,\n","          7.6919e-04, -9.5788e-04,  7.9112e-07,  1.3277e-04,  3.9394e-08],\n","        [ 1.5056e-08,  8.0501e-07,  1.2139e-09,  5.9489e-05,  7.5326e-07,\n","          1.6763e-05,  2.5002e-11,  3.0890e-06,  4.4640e-06, -8.5380e-05],\n","        [ 5.1524e-12,  1.2139e-11,  3.5346e-07,  7.1585e-10, -3.5763e-07,\n","          1.6370e-11,  3.3830e-10,  2.6653e-11,  1.2061e-10,  3.0287e-09],\n","        [-4.4331e-07,  1.1950e-11,  2.2696e-09,  4.8283e-09,  1.5289e-09,\n","          3.2541e-09,  1.7952e-08,  2.6097e-07,  3.8405e-11,  1.5130e-07],\n","        [ 3.0963e-08,  9.9043e-07,  3.9882e-08,  3.5268e-07,  8.9974e-05,\n","          4.6095e-07,  2.0611e-08,  1.7695e-05,  2.4677e-06, -1.1203e-04],\n","        [ 8.7706e-10, -1.8887e-06,  1.9720e-07,  8.4245e-09,  2.3823e-08,\n","          3.0797e-09,  3.3207e-09,  1.3625e-06,  2.8599e-07,  1.5557e-10],\n","        [ 5.6936e-08, -3.0143e-03,  1.7833e-05,  2.8700e-03,  9.5373e-05,\n","          7.4822e-07,  8.6252e-07,  1.8665e-05,  4.0458e-06,  6.7325e-06],\n","        [ 1.7177e-10,  2.6382e-08, -1.4715e-06,  1.4405e-06,  1.0979e-18,\n","          1.3041e-13,  2.0733e-14,  3.7162e-12,  5.6206e-09,  2.6082e-14],\n","        [ 4.0253e-10,  2.1050e-07,  3.4190e-08,  3.1667e-09, -1.4622e-06,\n","          8.3296e-10,  7.5123e-08,  2.3431e-07,  1.4144e-07,  7.6240e-07],\n","        [ 3.4765e-15,  1.0808e-14,  1.2058e-13, -3.7253e-09,  4.9595e-21,\n","          2.9235e-09,  2.6167e-21,  4.5377e-17,  1.9229e-13,  4.3127e-10],\n","        [ 2.7687e-09,  7.7119e-08, -1.3397e-04,  1.9975e-08,  1.4714e-14,\n","          7.4896e-13,  6.1486e-13,  1.3387e-04,  4.1565e-09,  1.1768e-11],\n","        [ 2.8153e-06,  5.6329e-04,  5.8291e-05,  6.6489e-05,  3.1691e-05,\n","          8.6178e-05,  3.7684e-06, -1.2884e-03,  4.5472e-04,  2.1205e-05],\n","        [ 1.1851e-09,  2.3302e-05,  7.4815e-07, -3.3543e-05,  2.1950e-08,\n","          4.8082e-08,  1.2277e-09,  6.7238e-06,  2.1654e-06,  5.2959e-07],\n","        [ 4.1347e-07,  8.5600e-07,  1.4125e-08,  2.5668e-07,  1.1351e-10,\n","          5.4894e-09,  6.8723e-09,  5.1735e-07, -2.5555e-06,  4.8354e-07],\n","        [ 6.7440e-08,  2.5781e-07,  5.6961e-08,  4.0206e-09,  6.2607e-09,\n","          3.4108e-06, -4.3092e-06,  5.5307e-08,  4.4968e-07,  2.5429e-10],\n","        [ 2.3581e-08,  3.7095e-07,  8.4514e-09,  1.7294e-06,  2.6473e-07,\n","          4.7783e-08,  8.5547e-12,  5.4655e-04,  2.9108e-07, -5.4929e-04],\n","        [-4.1602e-06,  1.4008e-11,  1.3410e-06,  1.0136e-08,  2.8925e-10,\n","          1.6227e-08,  8.5654e-07,  7.0967e-10,  1.9370e-06,  3.7611e-10],\n","        [ 2.1362e-09,  2.6106e-08,  3.0614e-11,  1.6270e-08,  2.0144e-09,\n","         -2.4140e-06,  1.1929e-07,  3.2494e-10,  2.1278e-06,  1.2006e-07],\n","        [ 9.9298e-11,  9.6829e-08,  8.8028e-09,  3.1959e-10,  4.7733e-09,\n","          2.6247e-08, -1.3970e-07,  2.4492e-10,  1.7609e-09,  5.5338e-13],\n","        [-2.5891e-07,  2.8018e-12,  2.3847e-07,  2.4426e-09,  2.7809e-12,\n","          1.7955e-11,  8.5403e-09,  1.8875e-09,  6.8059e-09,  1.1869e-10],\n","        [ 4.1565e-08,  2.6924e-07,  1.2586e-04,  8.0224e-04,  1.7230e-07,\n","          1.3340e-07,  1.1144e-07, -9.5532e-04,  2.1071e-07,  2.6283e-05],\n","        [ 5.9996e-07,  2.1114e-07,  5.9200e-08,  1.1199e-08,  1.5027e-06,\n","          2.9836e-06, -5.8468e-06,  1.3592e-07,  3.3004e-07,  1.2840e-08],\n","        [ 7.8225e-11, -3.7933e-06,  5.1264e-09,  2.4674e-07,  3.2065e-07,\n","          4.0167e-08,  1.6149e-07,  1.1586e-06,  1.7611e-06,  9.6974e-08],\n","        [ 5.6075e-07,  3.4093e-08,  1.3344e-06,  5.7499e-07,  1.4411e-09,\n","          1.7815e-08,  4.7860e-09,  9.6620e-09, -2.6599e-06,  1.1860e-07],\n","        [ 1.0206e-06,  1.8492e-05,  3.9214e-05,  3.1965e-05,  1.0091e-07,\n","          9.7288e-05,  8.7679e-07, -4.4974e-04,  2.5203e-05,  2.3558e-04],\n","        [ 6.9200e-09,  1.6379e-07,  2.2004e-10,  1.1275e-06,  7.7312e-09,\n","          1.9825e-06,  3.2988e-10,  8.6051e-07,  4.4655e-07, -4.5979e-06],\n","        [ 1.2675e-08,  4.9776e-08,  1.1357e-06, -1.3652e-05,  3.3203e-11,\n","          7.8498e-07,  9.1390e-11,  5.3801e-06,  6.0121e-06,  2.7606e-07],\n","        [ 1.0827e-09,  4.7901e-13,  3.0277e-10,  7.7472e-08,  2.2679e-08,\n","          1.7607e-06,  1.3852e-11,  2.0694e-07,  9.4245e-08, -2.1625e-06],\n","        [ 8.4432e-06,  4.2414e-10,  2.1851e-05,  5.8816e-06,  7.1308e-11,\n","          2.5760e-06,  3.5759e-09,  4.6814e-08, -4.1794e-05,  2.9923e-06],\n","        [ 1.5466e-10,  1.1897e-11,  1.6435e-12,  4.7234e-10,  1.7349e-08,\n","         -2.9802e-08,  4.7411e-09,  4.9770e-14,  6.3405e-09,  4.0134e-10],\n","        [ 2.2139e-07,  9.2738e-05,  3.0224e-07,  2.1910e-04,  9.6619e-06,\n","          4.2585e-04,  2.2489e-07,  4.5440e-07,  1.1632e-04, -8.6487e-04],\n","        [ 8.1841e-10,  3.1041e-11,  3.2191e-11, -3.6388e-05,  1.2773e-15,\n","          3.3410e-05,  1.2940e-15,  1.0240e-13,  3.5875e-10,  2.9759e-06],\n","        [ 2.3296e-11,  2.6959e-08,  1.7608e-10, -5.4389e-06,  4.5939e-11,\n","          5.3613e-06,  9.4059e-13,  3.8384e-11,  1.1139e-10,  5.0949e-08],\n","        [ 0.0000e+00,  6.3720e-18,  8.4404e-12,  7.1448e-13,  3.9729e-15,\n","          2.4596e-14,  3.0156e-12,  2.4825e-11,  1.6225e-13,  1.6611e-10],\n","        [ 1.1849e-10,  6.1075e-15,  5.7043e-10,  1.6880e-10,  3.2074e-13,\n","          6.8657e-09,  3.7715e-13, -1.0058e-07,  5.2843e-10,  9.1516e-08],\n","        [ 7.3061e-08,  1.5886e-05,  1.1146e-07,  7.4580e-05, -4.5207e-04,\n","          1.8188e-05,  1.9128e-07,  2.1257e-04,  2.9208e-06,  1.2756e-04],\n","        [ 1.8813e-10,  2.1602e-11,  9.8444e-10,  1.8981e-08,  3.3301e-04,\n","          8.9681e-09,  2.7716e-09,  1.2715e-04,  1.1567e-05, -4.7177e-04],\n","        [ 1.5201e-09,  8.6720e-10,  4.8370e-08,  3.8578e-06,  6.5364e-11,\n","          2.8015e-09,  5.4572e-12,  1.4928e-07, -7.3761e-06,  3.3146e-06],\n","        [ 0.0000e+00,  3.2331e-18,  2.9822e-14,  1.1857e-15,  2.0420e-11,\n","          2.1702e-12,  6.0742e-10,  5.9233e-11,  5.5474e-13,  3.2513e-13],\n","        [ 8.1617e-11,  5.8473e-11,  8.4753e-10,  5.1651e-08,  5.7089e-06,\n","          6.5103e-08,  6.9661e-12,  2.0392e-06,  2.0508e-08, -7.8846e-06],\n","        [ 1.9401e-13,  9.2828e-13,  1.9091e-11,  9.2388e-12, -4.6566e-08,\n","          8.3674e-11,  2.7045e-10,  4.4004e-12,  4.4504e-08,  2.7175e-09],\n","        [ 7.5596e-09, -3.4058e-06,  1.6462e-06,  1.5918e-08,  3.6414e-09,\n","          1.5812e-08,  5.2163e-08,  6.7224e-07,  9.9194e-07,  4.8332e-11],\n","        [ 1.4091e-07,  2.0931e-12,  2.9271e-07,  1.1038e-09, -1.0114e-06,\n","          1.7651e-10,  5.6984e-08,  6.7864e-08,  3.1382e-09,  4.4915e-07],\n","        [ 3.2315e-08,  3.3020e-06,  2.8385e-09,  1.1986e-07, -1.3337e-05,\n","          6.7099e-06,  1.2309e-07,  1.0582e-06,  1.5817e-06,  4.0656e-07],\n","        [ 1.6731e-07,  1.4307e-08,  4.6717e-10,  5.5614e-10,  1.7698e-10,\n","          6.6233e-06, -7.1013e-06,  3.0956e-10,  2.9506e-07,  5.8446e-12],\n","        [-8.7841e-05,  2.1374e-11,  7.9827e-08,  3.3235e-06,  9.2961e-13,\n","          6.9884e-07,  5.0107e-09,  1.4524e-06,  8.1687e-05,  5.9702e-07]])\n","tensor([[ 2.9648e-11,  5.8609e-12,  8.0965e-13,  4.2940e-06,  7.9533e-18,\n","         -4.3260e-06,  2.7728e-13,  1.7642e-13,  1.8717e-14,  3.1903e-08],\n","        [-5.0291e-08,  1.1220e-12,  3.4995e-08,  5.1258e-12,  8.0281e-11,\n","          7.3348e-13,  2.9416e-09,  6.8534e-09,  2.1799e-11,  3.3598e-09],\n","        [ 5.5656e-08,  1.8804e-07,  3.3081e-05,  1.6460e-06, -5.4890e-05,\n","          6.3302e-08,  8.5188e-06,  7.3134e-06,  8.7669e-07,  3.1476e-06],\n","        [ 1.0618e-09, -3.1376e-06,  3.6385e-07,  5.9768e-09,  1.1344e-08,\n","          7.2257e-09,  6.9749e-09,  2.2031e-06,  5.3922e-07,  7.8598e-11],\n","        [ 6.3357e-09,  3.7006e-09,  4.7841e-09,  4.5764e-08,  8.6512e-05,\n","          2.1705e-07,  4.6862e-09,  1.0480e-05,  1.5162e-05, -1.1244e-04],\n","        [ 9.3733e-07,  1.9156e-07, -5.9369e-05,  1.8340e-06,  2.1127e-06,\n","          1.5929e-06,  1.6735e-05,  7.3210e-07,  3.1544e-05,  3.6883e-06],\n","        [ 1.2299e-11, -3.2065e-06,  2.3736e-08,  8.0094e-07,  3.9474e-07,\n","          2.2087e-09,  8.5173e-08,  8.5246e-07,  9.8115e-07,  6.5246e-08],\n","        [ 4.0983e-12,  1.1468e-13,  7.0735e-11, -4.4703e-08,  3.7207e-17,\n","          1.2039e-09,  4.8680e-17,  6.7422e-12,  1.0725e-10,  4.3233e-08],\n","        [ 4.1375e-10, -1.4408e-05,  5.5466e-08,  1.5944e-06,  3.7804e-07,\n","          8.1187e-08,  1.3627e-07,  6.6036e-06,  3.6260e-06,  1.9296e-06],\n","        [ 2.1696e-11,  8.6403e-12,  1.1261e-08,  4.1355e-11, -2.2724e-07,\n","          3.0567e-10,  2.0682e-07,  1.5522e-09,  6.8254e-09,  5.4419e-10],\n","        [ 9.0640e-12,  8.2900e-08,  3.1754e-10, -2.4028e-07,  8.8719e-12,\n","          3.0153e-08,  1.0098e-14,  5.8521e-12,  9.0210e-11,  1.2656e-07],\n","        [ 1.9688e-07,  1.8077e-05,  1.5474e-06,  3.2015e-07,  3.0927e-07,\n","         -4.3696e-05,  9.6911e-06,  2.0422e-07,  1.3350e-05,  4.9635e-11],\n","        [ 3.5339e-15,  2.0771e-09,  3.5939e-12, -1.8626e-09,  3.4099e-15,\n","          3.7618e-11,  1.0731e-18,  7.4359e-15,  1.4744e-13,  5.7965e-11],\n","        [ 2.5824e-09,  1.3156e-09,  3.3489e-09,  4.4060e-10,  1.8818e-10,\n","          1.2432e-06, -1.4212e-06,  9.4242e-10,  1.6871e-07,  7.8177e-13],\n","        [ 1.5944e-10, -6.5332e-06,  3.0368e-08,  6.4364e-07,  5.2463e-08,\n","          1.0000e-07,  8.1925e-08,  1.4710e-06,  4.0219e-06,  1.3189e-07],\n","        [ 1.1990e-10,  2.0684e-12,  7.4518e-09,  1.1312e-10,  4.7401e-08,\n","          4.3889e-09,  4.7119e-12, -1.6019e-05,  8.6065e-08,  1.5873e-05],\n","        [ 7.2570e-06,  2.1783e-07, -1.4633e-04,  1.8825e-06,  5.6612e-06,\n","          4.7866e-09,  2.4038e-05,  9.3169e-05,  7.9946e-06,  6.1088e-06],\n","        [ 4.1715e-08,  5.0596e-07,  5.7596e-09,  6.9349e-09,  1.1917e-07,\n","          6.5161e-09,  6.0477e-09,  6.7534e-08, -8.6240e-07,  1.0491e-07],\n","        [ 3.0221e-05,  1.2433e-05,  1.3018e-06,  3.2325e-06,  2.8511e-06,\n","          6.3870e-04, -8.1164e-04,  7.4686e-07,  1.2213e-04,  3.4120e-08],\n","        [ 1.2682e-08,  7.3756e-07,  1.0513e-09,  4.9070e-05,  7.2751e-07,\n","          1.3185e-05,  2.1520e-11,  2.9088e-06,  3.9560e-06, -7.0600e-05],\n","        [ 4.7091e-12,  1.1213e-11,  3.3200e-07,  6.5219e-10, -3.3528e-07,\n","          1.5117e-11,  3.1527e-10,  2.6658e-11,  1.1445e-10,  3.2081e-09],\n","        [-4.3213e-07,  1.1804e-11,  2.3179e-09,  4.1344e-09,  1.4943e-09,\n","          2.7861e-09,  1.8062e-08,  2.6664e-07,  3.8134e-11,  1.3815e-07],\n","        [ 2.4991e-08,  8.8195e-07,  3.2444e-08,  2.8593e-07,  8.0501e-05,\n","          4.0886e-07,  1.8129e-08,  1.6257e-05,  2.1133e-06, -1.0053e-04],\n","        [ 8.6578e-10, -1.9819e-06,  1.9316e-07,  7.8869e-09,  2.4491e-08,\n","          2.8711e-09,  3.1810e-09,  1.4618e-06,  2.8632e-07,  1.7025e-10],\n","        [ 4.3712e-08, -1.8255e-03,  1.4743e-05,  1.6990e-03,  8.2615e-05,\n","          5.4100e-07,  8.6668e-07,  1.8213e-05,  3.9009e-06,  5.6390e-06],\n","        [ 1.6391e-10,  2.5910e-08, -1.2368e-06,  1.2043e-06,  1.0267e-18,\n","          1.0881e-13,  2.0166e-14,  3.7019e-12,  5.5009e-09,  2.3473e-14],\n","        [ 3.7192e-10,  1.9718e-07,  3.1650e-08,  2.8986e-09, -1.4901e-06,\n","          7.6914e-10,  6.9789e-08,  2.3351e-07,  1.3500e-07,  8.2062e-07],\n","        [ 4.0279e-15,  1.2683e-14,  1.4328e-13, -3.7253e-09,  6.0590e-21,\n","          3.0097e-09,  2.9692e-21,  5.6015e-17,  2.2580e-13,  5.3453e-10],\n","        [ 2.6752e-09,  7.5529e-08, -1.3653e-04,  1.7579e-08,  1.4487e-14,\n","          6.9622e-13,  6.0770e-13,  1.3643e-04,  4.0827e-09,  1.2248e-11],\n","        [ 2.2634e-06,  4.5681e-04,  4.9054e-05,  5.4608e-05,  2.8240e-05,\n","          6.8376e-05,  2.9243e-06, -1.0490e-03,  3.6628e-04,  2.0468e-05],\n","        [ 1.2773e-09,  2.9823e-05,  8.6121e-07, -4.2165e-05,  2.5380e-08,\n","          5.1987e-08,  1.4830e-09,  8.2799e-06,  2.5180e-06,  6.0167e-07],\n","        [ 4.0889e-07,  8.5831e-07,  1.3647e-08,  2.4197e-07,  1.1748e-10,\n","          5.3061e-09,  6.6680e-09,  5.2712e-07, -2.5928e-06,  5.2879e-07],\n","        [ 5.9361e-08,  2.5057e-07,  5.1188e-08,  3.4804e-09,  5.8121e-09,\n","          3.0988e-06, -3.9292e-06,  5.2385e-08,  4.0773e-07,  2.3764e-10],\n","        [ 1.9135e-08,  3.1194e-07,  6.7788e-09,  1.2566e-06,  2.5433e-07,\n","          3.5607e-08,  7.0206e-12,  4.9709e-04,  2.5320e-07, -4.9923e-04],\n","        [-4.1733e-06,  1.3198e-11,  1.3505e-06,  8.4921e-09,  2.8507e-10,\n","          1.3609e-08,  8.6458e-07,  7.3125e-10,  1.9360e-06,  3.6158e-10],\n","        [ 2.2134e-09,  3.0050e-08,  3.1504e-11,  1.5205e-08,  2.1646e-09,\n","         -2.6915e-06,  1.3536e-07,  3.6436e-10,  2.3679e-06,  1.3684e-07],\n","        [ 8.0871e-11,  1.0166e-07,  7.4821e-09,  2.6177e-10,  4.1385e-09,\n","          2.4463e-08, -1.3970e-07,  2.2367e-10,  1.5818e-09,  5.0002e-13],\n","        [-2.5705e-07,  2.7375e-12,  2.3649e-07,  2.3607e-09,  2.8589e-12,\n","          1.6735e-11,  8.4630e-09,  1.8813e-09,  6.5413e-09,  1.2156e-10],\n","        [ 3.4642e-08,  2.3482e-07,  1.1048e-04,  6.6335e-04,  1.5842e-07,\n","          1.1193e-07,  9.5728e-08, -8.0096e-04,  1.8026e-07,  2.6309e-05],\n","        [ 4.9099e-07,  1.9830e-07,  4.9035e-08,  8.8286e-09,  1.3488e-06,\n","          2.6037e-06, -5.1241e-06,  1.2533e-07,  2.8818e-07,  1.2038e-08],\n","        [ 6.6143e-11, -3.4710e-06,  4.4863e-09,  1.8462e-07,  2.8871e-07,\n","          3.2529e-08,  1.5131e-07,  1.1024e-06,  1.6246e-06,  8.6494e-08],\n","        [ 5.0725e-07,  3.3641e-08,  1.2315e-06,  4.3827e-07,  1.3108e-09,\n","          1.4971e-08,  4.7706e-09,  9.7735e-09, -2.3637e-06,  1.2299e-07],\n","        [ 8.5988e-07,  1.7365e-05,  3.4978e-05,  2.4788e-05,  9.0610e-08,\n","          8.1002e-05,  8.1958e-07, -3.9964e-04,  2.2837e-05,  2.1690e-04],\n","        [ 5.7002e-09,  1.5407e-07,  1.8225e-10,  8.4641e-07,  7.0750e-09,\n","          1.6192e-06,  3.0643e-10,  8.2047e-07,  4.0031e-07, -3.8566e-06],\n","        [ 1.2328e-08,  5.1029e-08,  1.1364e-06, -1.3742e-05,  3.3684e-11,\n","          7.7463e-07,  8.9470e-11,  5.5210e-06,  5.9616e-06,  2.8379e-07],\n","        [ 8.3783e-10,  3.5787e-13,  2.2515e-10,  5.9651e-08,  1.8521e-08,\n","          1.3281e-06,  1.0310e-11,  1.7715e-07,  7.4262e-08, -1.6578e-06],\n","        [ 8.2228e-06,  4.1181e-10,  2.1359e-05,  5.5360e-06,  6.9490e-11,\n","          2.3719e-06,  3.3474e-09,  4.7030e-08, -4.0819e-05,  3.2779e-06],\n","        [ 1.5958e-10,  1.3211e-11,  1.7206e-12,  4.4618e-10,  1.8594e-08,\n","         -3.1665e-08,  5.5012e-09,  5.5389e-14,  6.9077e-09,  4.5852e-10],\n","        [ 1.8998e-07,  8.6205e-05,  2.5891e-07,  1.6626e-04,  8.9529e-06,\n","          3.2112e-04,  2.0976e-07,  4.3356e-07,  1.0546e-04, -6.8908e-04],\n","        [ 9.3861e-10,  3.7944e-11,  3.8094e-11, -3.6818e-05,  1.4772e-15,\n","          3.3477e-05,  1.5106e-15,  1.2170e-13,  4.2341e-10,  3.3404e-06],\n","        [ 2.7180e-11,  3.6271e-08,  2.0831e-10, -6.0843e-06,  5.6044e-11,\n","          5.9887e-06,  1.2253e-12,  4.7009e-11,  1.3670e-10,  6.0776e-08],\n","        [ 0.0000e+00,  5.8087e-18,  8.1176e-12,  6.6386e-13,  3.7530e-15,\n","          2.3322e-14,  2.8538e-12,  2.3662e-11,  1.5603e-13,  1.6684e-10],\n","        [ 1.0683e-10,  5.4274e-15,  5.3025e-10,  1.4674e-10,  2.9287e-13,\n","          6.1002e-09,  3.3757e-13, -1.0058e-07,  4.7688e-10,  9.2678e-08],\n","        [ 5.9017e-08,  1.4427e-05,  9.4111e-08,  5.6954e-05, -4.0400e-04,\n","          1.3705e-05,  1.6619e-07,  1.9777e-04,  2.5246e-06,  1.1830e-04],\n","        [ 1.4942e-10,  1.6203e-11,  7.6367e-10,  1.5380e-08,  2.5968e-04,\n","          7.0258e-09,  2.0729e-09,  1.0730e-04,  9.1202e-06, -3.7613e-04],\n","        [ 1.4762e-09,  8.3445e-10,  4.6874e-08,  3.6270e-06,  6.2014e-11,\n","          2.6145e-09,  5.0393e-12,  1.5606e-07, -7.6815e-06,  3.8470e-06],\n","        [ 0.0000e+00,  2.9853e-18,  2.8941e-14,  1.1297e-15,  1.9333e-11,\n","          2.0622e-12,  5.7943e-10,  5.7165e-11,  5.3528e-13,  3.3694e-13],\n","        [ 6.4874e-11,  4.6390e-11,  6.5861e-10,  3.9817e-08,  5.0981e-06,\n","          5.1801e-08,  5.5113e-12,  1.7770e-06,  1.6780e-08, -6.9821e-06],\n","        [ 1.7383e-13,  8.5402e-13,  1.7482e-11,  8.3399e-12, -4.4703e-08,\n","          7.5959e-11,  2.4874e-10,  4.3839e-12,  4.1888e-08,  2.9976e-09],\n","        [ 7.2449e-09, -3.3947e-06,  1.6112e-06,  1.3981e-08,  3.6422e-09,\n","          1.3657e-08,  4.9934e-08,  7.1769e-07,  9.7815e-07,  4.8575e-11],\n","        [ 1.3359e-07,  1.9793e-12,  2.8076e-07,  1.0220e-09, -1.0170e-06,\n","          1.6840e-10,  5.4537e-08,  6.9910e-08,  3.0020e-09,  4.7450e-07],\n","        [ 2.9560e-08,  3.0362e-06,  2.6080e-09,  1.1139e-07, -1.2363e-05,\n","          6.1122e-06,  1.0953e-07,  1.0548e-06,  1.4636e-06,  4.4379e-07],\n","        [ 1.4003e-07,  1.4210e-08,  4.0582e-10,  4.5871e-10,  1.5451e-10,\n","          6.0174e-06, -6.4308e-06,  2.8041e-10,  2.5772e-07,  5.1165e-12],\n","        [-8.3652e-05,  2.0415e-11,  7.6570e-08,  3.1229e-06,  9.2891e-13,\n","          6.3451e-07,  4.7681e-09,  1.4148e-06,  7.7795e-05,  6.0222e-07]])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"I0fC9kbvxp8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070673087,"user_tz":-540,"elapsed":694,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"e502eae2-afc4-4c7c-da31-3fb4a39098a0"},"source":["loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0069, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"n70culC-xp8h"},"source":["## Using parameters and optim"]},{"cell_type":"markdown","metadata":{"id":"Ztmln-E0xp8h"},"source":["### Parameters"]},{"cell_type":"markdown","metadata":{"id":"3JMWX2-fxp8i"},"source":["Use `nn.Module.__setattr__` and move relu to functional:"]},{"cell_type":"markdown","metadata":{"id":"2qu-R0Ljxp8j"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2818)"]},{"cell_type":"code","metadata":{"id":"lw5k0KJRxp8j","executionInfo":{"status":"ok","timestamp":1619070678605,"user_tz":-540,"elapsed":719,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.l1 = nn.Linear(n_in,nh)\n","        self.l2 = nn.Linear(nh,n_out)\n","        \n","    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"lq8zwnBlxp8k","executionInfo":{"status":"ok","timestamp":1619070680072,"user_tz":-540,"elapsed":754,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["model = Model(m, nh, 10)"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFsAG7KCxp8k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070681578,"user_tz":-540,"elapsed":633,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"04e61ee7-4895-4e78-d0df-8ade037ae775"},"source":["for name,l in model.named_children(): print(f\"{name}: {l}\")"],"execution_count":80,"outputs":[{"output_type":"stream","text":["l1: Linear(in_features=784, out_features=50, bias=True)\n","l2: Linear(in_features=50, out_features=10, bias=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QQAcSLAVxp8l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070683249,"user_tz":-540,"elapsed":443,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"27e78c83-448b-44eb-fc9e-5a867e6169b3"},"source":["model"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (l1): Linear(in_features=784, out_features=50, bias=True)\n","  (l2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"O77Ds-LRxp8m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070685014,"user_tz":-540,"elapsed":846,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"3af0e232-9bda-4dd2-f454-fea41a3bacca"},"source":["model.l1"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=784, out_features=50, bias=True)"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"SUDx09pkxp8m","executionInfo":{"status":"ok","timestamp":1619070685259,"user_tz":-540,"elapsed":538,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["def fit():\n","    for epoch in range(epochs):\n","        for i in range((n-1)//bs + 1):\n","            start_i = i*bs\n","            end_i = start_i+bs\n","            xb = x_train[start_i:end_i]\n","            yb = y_train[start_i:end_i]\n","            loss = loss_func(model(xb), yb.long())\n","\n","            loss.backward()\n","            with torch.no_grad():\n","                for p in model.parameters(): p -= p.grad * lr\n","                model.zero_grad()"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkjGdktlxp8o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2db913c-a4b9-4a7d-9003-28aadf55efff"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.2048, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"-wQ0CkLmxp8p"},"source":["Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."]},{"cell_type":"code","metadata":{"id":"nfLTjkNJxp8p","executionInfo":{"status":"ok","timestamp":1619070712435,"user_tz":-540,"elapsed":620,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["class DummyModule():\n","    def __init__(self, n_in, nh, n_out):\n","        self._modules = {}\n","        self.l1 = nn.Linear(n_in,nh)\n","        self.l2 = nn.Linear(nh,n_out)\n","        \n","    def __setattr__(self,k,v):\n","        if not k.startswith(\"_\"): self._modules[k] = v\n","        super().__setattr__(k,v)\n","        \n","    def __repr__(self): return f'{self._modules}'\n","    \n","    def parameters(self):\n","        for l in self._modules.values():\n","            for p in l.parameters(): yield p"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxdtM1ilxp8q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070714519,"user_tz":-540,"elapsed":733,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"9ae8f2e2-4daa-494c-b076-ba0f941c0b1b"},"source":["mdl = DummyModule(m,nh,10)\n","mdl"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"asWZtQT7xp8r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070715776,"user_tz":-540,"elapsed":522,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"266973a7-2580-40f7-98d8-ae7400d4ad4a"},"source":["[o.shape for o in mdl.parameters()]"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[torch.Size([50, 784]),\n"," torch.Size([50]),\n"," torch.Size([10, 50]),\n"," torch.Size([10])]"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"PJzROcD-xp8r"},"source":["### Registering modules"]},{"cell_type":"markdown","metadata":{"id":"Gi6Bj3Egxp8s"},"source":["We can use the original `layers` approach, but we have to register the modules."]},{"cell_type":"markdown","metadata":{"id":"SCDxRdQexp8s"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2997)"]},{"cell_type":"code","metadata":{"id":"8ax355bwxp8t","executionInfo":{"status":"ok","timestamp":1619070721226,"user_tz":-540,"elapsed":875,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FVOwYH8xp8t","executionInfo":{"status":"ok","timestamp":1619070722378,"user_tz":-540,"elapsed":722,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["# add_module 상속\n","class Model(nn.Module):\n","    def __init__(self, layers):\n","        super().__init__()\n","        self.layers = layers\n","        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5hq-X7Vxp8u","executionInfo":{"status":"ok","timestamp":1619070742825,"user_tz":-540,"elapsed":711,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}}},"source":["model = Model(layers)"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqYP2MXlxp8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619070743607,"user_tz":-540,"elapsed":627,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"c2c5bf45-c669-4ce1-a9eb-97366de2b65d"},"source":["model"],"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n","  (layer_1): ReLU()\n","  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"diwEDyzJxp8v"},"source":["### nn.ModuleList"]},{"cell_type":"markdown","metadata":{"id":"yUVBUAqqxp8v"},"source":["`nn.ModuleList` does this for us."]},{"cell_type":"markdown","metadata":{"id":"w39ws0evxp8w"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3173)"]},{"cell_type":"code","metadata":{"id":"YNrrTzq6xp8w"},"source":["class SequentialModel(nn.Module):\n","    def __init__(self, layers):\n","        super().__init__()\n","        self.layers = nn.ModuleList(layers)\n","        \n","    def __call__(self, x):\n","        for l in self.layers: x = l(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m59ypeCdxp8x"},"source":["model = SequentialModel(layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVSNzdUmxp8x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4be69e8d-f133-4c01-8617-44e8451cae20"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequentialModel(\n","  (layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=50, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=50, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"QheaMa-6xp8y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b02d5cb4-cbd1-4f08-ae29-1be7c5332c91"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0107, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"yk8EQfB6xp8y"},"source":["### nn.Sequential"]},{"cell_type":"markdown","metadata":{"id":"twIiTxJAxp8y"},"source":["`nn.Sequential` is a convenient class which does the same as the above:"]},{"cell_type":"markdown","metadata":{"id":"eWOOJnAXxp8y"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3199)"]},{"cell_type":"code","metadata":{"id":"YVIaRdsPxp8z"},"source":["model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_079mLyxp8z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3994c698-7ae4-47ee-f9cf-291c194a3ce8"},"source":["fit()\n","loss_func(model(xb), yb.long()), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1236, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"fX3SUZc5xp8z"},"source":["from torch import nn\n","nn.Sequential??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1EvUMznxp81","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5637639-9c72-4336-ff2d-686a0bcb4816"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=784, out_features=50, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=50, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"id":"1o20CTI0xp82"},"source":["### optim"]},{"cell_type":"markdown","metadata":{"id":"_8edAASgxp82"},"source":["Let's replace our previous manually coded optimization step:\n","\n","```python\n","with torch.no_grad():\n","    for p in model.parameters(): p -= p.grad * lr\n","    model.zero_grad()\n","```\n","\n","and instead use just:\n","\n","```python\n","opt.step()\n","opt.zero_grad()\n","```"]},{"cell_type":"markdown","metadata":{"id":"lV7oQXBxxp83"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3278)"]},{"cell_type":"code","metadata":{"id":"rz8BYVQkxp84"},"source":["class Optimizer():\n","    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n","        \n","    def step(self):\n","        with torch.no_grad():\n","            for p in self.params: p -= p.grad * lr\n","\n","    def zero_grad(self):\n","        for p in self.params: p.grad.data.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr97GcPPxp84"},"source":["model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lh_YE0ogxp84"},"source":["opt = Optimizer(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOZPZ1rsxp85"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        start_i = i*bs\n","        end_i = start_i+bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1K8AuK0yxp85","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96477a22-437a-4d1e-cedc-81ccfdc7ad38"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1114, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"vtEnqOylxp86"},"source":["PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"]},{"cell_type":"code","metadata":{"id":"1EbDEWXsxp87"},"source":["#export\n","from torch import optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVc-OgiExp87"},"source":["\n","optim.SGD.step??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJyAMVzbxp88"},"source":["def get_model():\n","    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n","    return model, optim.SGD(model.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fu_n5Z2zxp89","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8de39113-a531-428e-f89e-e70459000741"},"source":["model,opt = get_model()\n","loss_func(model(xb), yb.long())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3170, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"XT0H_e6Vxp8-"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        start_i = i*bs\n","        end_i = start_i+bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step() # model parameter 불러와서 gradient update\n","        opt.zero_grad() # "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Do5mQXdWxp8-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4289d3b3-4da9-498d-b2b9-96e5e69cc7dc"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.2243, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"markdown","metadata":{"id":"UVaK2RCHxp8_"},"source":["Randomized tests can be very useful."]},{"cell_type":"markdown","metadata":{"id":"TSDu_zpXxp8_"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3442)"]},{"cell_type":"code","metadata":{"id":"3GQE3EvMxp9A"},"source":["assert acc>0.7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PWzGE8Rxp9A"},"source":["## Dataset and DataLoader"]},{"cell_type":"markdown","metadata":{"id":"nz8ts0aZxp9A"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"hhmhOE2xxp9B"},"source":["It's clunky to iterate through minibatches of x and y values separately:\n","\n","```python\n","    xb = x_train[start_i:end_i]\n","    yb = y_train[start_i:end_i]\n","```\n","\n","Instead, let's do these two steps together, by introducing a `Dataset` class:\n","\n","```python\n","    xb,yb = train_ds[i*bs : i*bs+bs]\n","```"]},{"cell_type":"markdown","metadata":{"id":"U-gLw07Fxp9B"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3578)"]},{"cell_type":"code","metadata":{"id":"QdCJfv5Zxp9B"},"source":["#export\n","class Dataset():\n","    def __init__(self, x, y): self.x,self.y = x,y\n","    def __len__(self): return len(self.x)\n","    def __getitem__(self, i): return self.x[i],self.y[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wzhoIAExp9C"},"source":["train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n","assert len(train_ds)==len(x_train)\n","assert len(valid_ds)==len(x_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxsllyZ8xp9C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a240c6a-3e35-40d2-8956-49fe539d3d28"},"source":["xb,yb = train_ds[0:5]\n","assert xb.shape==(5,28*28)\n","assert yb.shape==(5,)\n","xb,yb"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5., 0., 4., 1., 9.]))"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"PNvAKEUAxp9C"},"source":["model,opt = get_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6y3T2ONQxp9D"},"source":["for epoch in range(epochs):\n","    for i in range((n-1)//bs + 1):\n","        xb,yb = train_ds[i*bs : i*bs+bs]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb.long())\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiGSBU34xp9D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16d2e73b-d668-4c50-8025-5a3ae2ea8682"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.3409, grad_fn=<NllLossBackward>), tensor(0.9375))"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"0fgmSsJdxp9D"},"source":["### DataLoader"]},{"cell_type":"markdown","metadata":{"id":"QlMsRaK_xp9D"},"source":["Previously, our loop iterated over batches (xb, yb) like this:\n","\n","```python\n","for i in range((n-1)//bs + 1):\n","    xb,yb = train_ds[i*bs : i*bs+bs]\n","    ...\n","```\n","\n","Let's make our loop much cleaner, using a data loader:\n","\n","```python\n","for xb,yb in train_dl:\n","    ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"Ka7lGsJoxp9E"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3674)"]},{"cell_type":"code","metadata":{"id":"DcOXwOi-xp9F"},"source":["class DataLoader():\n","    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n","    def __iter__(self):\n","        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1FCkkcdxp9F"},"source":["train_dl = DataLoader(train_ds, bs)\n","valid_dl = DataLoader(valid_ds, bs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMXh88dvxp9G"},"source":["xb,yb = next(iter(valid_dl))\n","assert xb.shape==(bs,28*28)\n","assert yb.shape==(bs,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWnYMKDhxp9G","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"12b6abb1-0f8c-4dac-ba8c-0ffbcee1bd21"},"source":["plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.)"]},"metadata":{"tags":[]},"execution_count":106},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"mekhrFC3xp9G"},"source":["model,opt = get_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIu_Wvwkxp9H"},"source":["def fit():\n","    for epoch in range(epochs):\n","        for xb,yb in train_dl:\n","            pred = model(xb)\n","            loss = loss_func(pred, yb.long())\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIFF7_-xxp9H"},"source":["fit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RH5KjHAaxp9H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b7502d5-445e-465e-a214-710cb6fa098d"},"source":["loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0518, grad_fn=<NllLossBackward>), tensor(1.))"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"markdown","metadata":{"id":"-9uDi1fcxp9H"},"source":["### Random sampling"]},{"cell_type":"markdown","metadata":{"id":"JRARDaX9xp9P"},"source":["We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."]},{"cell_type":"markdown","metadata":{"id":"MUSSX0iNxp9P"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3942)"]},{"cell_type":"code","metadata":{"id":"mgUen1XGxp9Q"},"source":["class Sampler():\n","    def __init__(self, ds, bs, shuffle=False):\n","        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n","        \n","    def __iter__(self):\n","        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n","        for i in range(0, self.n, self.bs): \n","            yield self.idxs[i:i+self.bs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nV78WqQbxp9Q"},"source":["small_ds = Dataset(*train_ds[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-wrj8hrxp9Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"423bda15-8805-4037-e7b6-65a7575a7933"},"source":["s = Sampler(small_ds,3,False)\n","[o for o in s]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"cAyM-UXDxp9R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f96ed10-0c06-4f7a-ced1-08591b7ad9ff"},"source":["s = Sampler(small_ds,3,True)\n","[o for o in s]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([9, 6, 7]), tensor([3, 2, 0]), tensor([5, 4, 8]), tensor([1])]"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"Xf0r66xbxp9R"},"source":["def collate(b):\n","    xs,ys = zip(*b)\n","    return torch.stack(xs),torch.stack(ys)\n","\n","class DataLoader():\n","    def __init__(self, ds, sampler, collate_fn=collate):\n","        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n","        \n","    def __iter__(self):\n","        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfhQOgsVxp9S"},"source":["train_samp = Sampler(train_ds, bs, shuffle=True)\n","valid_samp = Sampler(valid_ds, bs, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZpHj2oqxp9S"},"source":["train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n","valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qG_iithdxp9S","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"6367fbae-db63-4292-8db2-21ecef0bbbe4"},"source":["xb,yb = next(iter(valid_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.)"]},"metadata":{"tags":[]},"execution_count":118},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"wDtE1Hlgxp9S","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"f64d8585-9b27-4833-915c-3569b6e0bc0c"},"source":["xb,yb = next(iter(train_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.)"]},"metadata":{"tags":[]},"execution_count":119},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANTklEQVR4nO3df6jVdZ7H8dcrG/9xhGyjiziyzU5F1EYZJgvK0pIz/fjH7I8YjcVlbZUYYYw1NlpogmVDYmeW7Z+JOyWjizkMpKRDNDomWwYNXcNNzR1tQxnlqltC0/wRZr73j/t1uNU9n3M953vO91zfzwdczjnf9/me8+aLL78/PuecjyNCAC5/VzTdAID+IOxAEoQdSIKwA0kQdiCJK/v5Zra59A/0WER4ouVd7dlt32v7d7Y/sP1EN68FoLfc6Ti77WmSjkj6rqQTkt6RtCwi3i+sw54d6LFe7NkXSPogIj6MiHOSfiFpSRevB6CHugn7HEm/H/f4RLXsS2yvsj1ie6SL9wLQpZ5foIuIYUnDEofxQJO62bOflDR33ONvVcsADKBuwv6OpBtsf9v2dEnfl7S9nrYA1K3jw/iIOG97jaRfS5omaUNEHKqtMwC16njoraM345wd6LmefKgGwNRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIdT9kMDLqFCxe2rO3YsaO47vXXX1+snz17tqOemtRV2G0fk/SppC8knY+I+XU0BaB+dezZ/yYiPqrhdQD0EOfsQBLdhj0k7bS9z/aqiZ5ge5XtEdsjXb4XgC50exi/KCJO2r5W0i7b/xMRb4x/QkQMSxqWJNvR5fsB6FBXe/aIOFndnpG0TdKCOpoCUL+Ow257hu2ZF+9L+p6kg3U1BqBe3RzGD0naZvvi67wUEa/V0hVQg+XLl7eszZw5s7juggXlg9TXXpt6/9Q7DntEfCjpthp7AdBDDL0BSRB2IAnCDiRB2IEkCDuQhCP696E2PkHXmWeffbZYP3ToUMvaxo0b625nyrhw4ULL2r59+4rr3nnnnXW30zcR4YmWs2cHkiDsQBKEHUiCsANJEHYgCcIOJEHYgST4KekB8PDDDxfr69atK9Zff/31lrXLeZy93XYrWb9+fY2dTA3s2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZB8D06dO7Wv/WW2+tqZOp5cEHH+x43YMH801xwJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0PbrutPNntM888U6xX02K3tHLlykvuaSq46aabivWlS5cW62+99VbL2qlTpzrqaSpru2e3vcH2GdsHxy272vYu20er21m9bRNAtyZzGP9zSfd+ZdkTknZHxA2SdlePAQywtmGPiDcknf3K4iWSLv7e0UZJD9TcF4CadXrOPhQRo9X9U5KGWj3R9ipJqzp8HwA16foCXUREacLGiBiWNCwxsSPQpE6H3k7bni1J1e2Z+loC0Audhn27pBXV/RWSXqmnHQC90vYw3vYWSXdJusb2CUk/krRe0i9tr5R0XNJDvWxyqlu2bFmxfu211xbre/fuLdZ37tx5yT1NBe22W0T5rPCxxx5rWfvkk0866mkqaxv2iGi1xe+uuRcAPcTHZYEkCDuQBGEHkiDsQBKEHUiCr7j2weLFi7taf8OGDcX6uXPnunr9QfXoo48W68ePHy/WR0ZG6mxnymPPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM5eg6eeeqpYb/dT0u1s2bKlq/UH1X333Vesz5gxo1hft25dne1c9tizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLPXYN68ecX6FVd093/qrl27ivUjR460rN14443FdV966aVi/cCBA8V6u5+5LrnllluK9WPHjhXrmzZt6vi9M2LPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM5eA9td1dtZtGhRsb5w4cKOX3vOnDnF+p49e4r1q666qli/+eabW9bWr19fXHfz5s3F+gsvvFCsP/LII8V6Nm337LY32D5j++C4ZU/bPml7f/V3f2/bBNCtyRzG/1zSvRMs//eIuL36e7XetgDUrW3YI+INSWf70AuAHurmAt0a2+9Vh/mzWj3J9irbI7aZeAtoUKdh/6mk70i6XdKopB+3emJEDEfE/IiY3+F7AahBR2GPiNMR8UVEXJD0M0kL6m0LQN06Crvt2eMeLpV0sNVzAQyGtuPstrdIukvSNbZPSPqRpLts3y4pJB2TtLqHPQ68bdu2FeuHDx/u6vWHhoaK9bvvvrtlrd331Z977rlifXR0tFhv5/PPP29ZO3r0aHHd5cuXF+ufffZZsc44+5e1DXtELJtg8Ys96AVAD/FxWSAJwg4kQdiBJAg7kARhB5JwRPTvzez+vRkGwo4dO1rW7rjjjuK658+fL9aXLFlSrO/fv79Yv1xFxITfqWbPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ8FPS6MqaNWuK9Xvuuadlbdq0acV1165dW6xnHUfvFHt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXZ05aGHHirWS2Ppb775ZnHd559/vqOeMDH27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBL8bj6J200WfOnWqWD937lzLWmmqaUnau3dvsY6Jdfy78bbn2t5j+33bh2z/sFp+te1dto9Wt7PqbhpAfSZzGH9e0j9GxM2S/krSD2zfLOkJSbsj4gZJu6vHAAZU27BHxGhEvFvd/1TSYUlzJC2RtLF62kZJD/SqSQDdu6TPxtu+TtI8Sb+VNBQRo1XplKQJT+5sr5K0qvMWAdRh0lfjbX9T0suS1kbEH8bXYuwq34QX3yJiOCLmR8T8rjoF0JVJhd32NzQW9M0RsbVafNr27Ko+W9KZ3rQIoA5tD+NtW9KLkg5HxE/GlbZLWiFpfXX7Sk86RE9deWX5n8CmTZuK9XZDt6WvsTK01l+TOWdfKOlvJR2wffGHup/UWMh/aXulpOOSyl9sBtCotmGPiL2SJhykl1T+VASAgcHHZYEkCDuQBGEHkiDsQBKEHUiCn5JObvXq1cX64sWLi/WPP/64WG83To/+Yc8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzp7c3Llzu1r/7bffLtYZZx8c7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2ZN79dVXi/XHH3+8WN+6dWuxjsHBnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHknC7+bVtz5W0SdKQpJA0HBH/YftpSf8g6f+qpz4ZEcVBW9vlNwPQtYiYcNblyYR9tqTZEfGu7ZmS9kl6QGPzsf8xIv5tsk0QdqD3WoV9MvOzj0oare5/avuwpDn1tgeg1y7pnN32dZLmSfpttWiN7fdsb7A9q8U6q2yP2B7pqlMAXWl7GP+nJ9rflPRfkv41IrbaHpL0kcbO4/9FY4f6f9/mNTiMB3qs43N2SbL9DUm/kvTriPjJBPXrJP0qIv6yzesQdqDHWoW97WG8bUt6UdLh8UGvLtxdtFTSwW6bBNA7k7kav0jSm5IOSLpQLX5S0jJJt2vsMP6YpNXVxbzSa7FnB3qsq8P4uhB2oPc6PowHcHkg7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNHvKZs/knR83ONrqmWDaFB7G9S+JHrrVJ29/XmrQl+/z/61N7dHImJ+Yw0UDGpvg9qXRG+d6ldvHMYDSRB2IImmwz7c8PuXDGpvg9qXRG+d6ktvjZ6zA+ifpvfsAPqEsANJNBJ22/fa/p3tD2w/0UQPrdg+ZvuA7f1Nz09XzaF3xvbBccuutr3L9tHqdsI59hrq7WnbJ6ttt9/2/Q31Ntf2Htvv2z5k+4fV8ka3XaGvvmy3vp+z254m6Yik70o6IekdScsi4v2+NtKC7WOS5kdE4x/AsP3Xkv4oadPFqbVsPyvpbESsr/6jnBUR/zQgvT2tS5zGu0e9tZpm/O/U4Larc/rzTjSxZ18g6YOI+DAizkn6haQlDfQx8CLiDUlnv7J4iaSN1f2NGvvH0nctehsIETEaEe9W9z+VdHGa8Ua3XaGvvmgi7HMk/X7c4xMarPneQ9JO2/tsr2q6mQkMjZtm65SkoSabmUDbabz76SvTjA/Mtutk+vNucYHu6xZFxB2S7pP0g+pwdSDF2DnYII2d/lTSdzQ2B+CopB832Uw1zfjLktZGxB/G15rcdhP01Zft1kTYT0qaO+7xt6plAyEiTla3ZyRt09hpxyA5fXEG3er2TMP9/ElEnI6ILyLigqSfqcFtV00z/rKkzRGxtVrc+LabqK9+bbcmwv6OpBtsf9v2dEnfl7S9gT6+xvaM6sKJbM+Q9D0N3lTU2yWtqO6vkPRKg718yaBM491qmnE1vO0an/48Ivr+J+l+jV2R/19J/9xEDy36+gtJ/139HWq6N0lbNHZY97nGrm2slPRnknZLOirpN5KuHqDe/lNjU3u/p7FgzW6ot0UaO0R/T9L+6u/+prddoa++bDc+LgskwQU6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji/wFtKBheWJs5hgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"fkS0eizexp9T","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"51ad1749-fbb5-42da-adb4-2f649e7adecc"},"source":["xb,yb = next(iter(train_dl))\n","plt.imshow(xb[0].view(28,28))\n","yb[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9.)"]},"metadata":{"tags":[]},"execution_count":120},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"pwxmZdIBxp9U","colab":{"base_uri":"https://localhost:8080/","height":449},"outputId":"975b6c12-4885-496a-9318-90d8529be2e9"},"source":["model,opt = get_model()\n","fit()\n","\n","loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-121-24090b23970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"]}]},{"cell_type":"markdown","metadata":{"id":"BkOcYVz5xp9U"},"source":["### PyTorch DataLoader"]},{"cell_type":"markdown","metadata":{"id":"Ek63kxvpxp9U"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4171)"]},{"cell_type":"code","metadata":{"id":"8heUT6Qaxp9V"},"source":["#export\n","from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc6PTXcIxp9V"},"source":["train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n","valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URCJObCIxp9V","colab":{"base_uri":"https://localhost:8080/","height":413},"outputId":"b316b685-e677-440a-a87a-179efba49375"},"source":["model,opt = get_model()\n","fit()\n","loss_func(model(xb), yb), accuracy(model(xb), yb)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-124-7c5441fd45b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"]}]},{"cell_type":"markdown","metadata":{"id":"RV_tHff0xp9W"},"source":["PyTorch's defaults work fine for most things however:"]},{"cell_type":"code","metadata":{"id":"m1-XnjlYxp9W"},"source":["train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n","valid_dl = DataLoader(valid_ds, bs, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hldX-jfTxp9W","colab":{"base_uri":"https://localhost:8080/","height":449},"outputId":"1e61d521-5d11-4296-f054-d4b6da994933"},"source":["model,opt = get_model()\n","fit()\n","\n","loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n","assert acc>0.7\n","loss,acc"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-126-24090b23970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"]}]},{"cell_type":"markdown","metadata":{"id":"4wHMsZhExp9W"},"source":["Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."]},{"cell_type":"markdown","metadata":{"id":"IZdHSkBbxp9X"},"source":["## Validation"]},{"cell_type":"markdown","metadata":{"id":"KzRnuK0dxp9X"},"source":["You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n","\n","We will calculate and print the validation loss at the end of each epoch.\n","\n","(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"]},{"cell_type":"markdown","metadata":{"id":"gG-PcbGPxp9X"},"source":["[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4260)"]},{"cell_type":"code","metadata":{"id":"gZpIe-Jqxp9X"},"source":["def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        # Handle batchnorm / dropout\n","        model.train()\n","#         print(model.training)\n","        for xb,yb in train_dl:\n","            loss = loss_func(model(xb), yb.long())\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","\n","        model.eval()\n","#         print(model.training)\n","        with torch.no_grad():\n","            tot_loss,tot_acc = 0.,0.\n","            for xb,yb in valid_dl:\n","                pred = model(xb)\n","                tot_loss += loss_func(pred, yb.long())\n","                tot_acc  += accuracy (pred,yb)\n","        nv = len(valid_dl)\n","        print(epoch, tot_loss/nv, tot_acc/nv)\n","    return tot_loss/nv, tot_acc/nv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU-f2jwTxp9Y"},"source":["*Question*: Are these validation results correct if batch size varies?"]},{"cell_type":"markdown","metadata":{"id":"kavB4SYyxp9Y"},"source":["`get_dls` returns dataloaders for the training and validation sets:"]},{"cell_type":"code","metadata":{"id":"Ze8BCcJrxp9Z"},"source":["#export\n","def get_dls(train_ds, valid_ds, bs, **kwargs):\n","    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n","            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLfgd85jxp9Z"},"source":["Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"]},{"cell_type":"code","metadata":{"id":"1xdz5p95xp9a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b4b7c1d-dcf1-4070-f5a1-8c9325b72364"},"source":["train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n","model,opt = get_model()\n","loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 tensor(0.2109) tensor(0.9345)\n","1 tensor(0.1198) tensor(0.9660)\n","2 tensor(0.1526) tensor(0.9538)\n","3 tensor(0.1194) tensor(0.9668)\n","4 tensor(0.1065) tensor(0.9705)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TM7lifJHxp9a"},"source":["assert acc>0.9"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-fmPWSBxp9b"},"source":["## Export"]},{"cell_type":"code","metadata":{"id":"LWl4Vw_Txp9b","outputId":"21bd0810-c37a-4d12-9157-d733a96c0e26"},"source":["!python notebook2script.py 03_minibatch_training.ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Converted 03_minibatch_training.ipynb to nb_03.py\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxGTEBHtxp9c"},"source":[""],"execution_count":null,"outputs":[]}]}